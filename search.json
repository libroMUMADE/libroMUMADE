[
  {
    "objectID": "01-intro.html#origen-del-arte-o-ciencia-de-la-predicción",
    "href": "01-intro.html#origen-del-arte-o-ciencia-de-la-predicción",
    "title": "\n1  Introducción\n",
    "section": "\n1.1 Origen del arte o ciencia de la predicción",
    "text": "1.1 Origen del arte o ciencia de la predicción\nPara conocer el origen de la predicción es necesario remontarse a la antigua Grecia, al oráculo de Delfos. Cuenta la mitología que el dios Zeus soltó dos águilas, una desde el este y otro desde el oeste, y el punto donde se encontraran sería el centro del universo. Las águilas se encontraron en Delfos, y en el punto exacto donde se encontraron colocó una piedra, el omphalos (ómbligo). Gaia (también llamada Gea) daba a conocer sus profecías a través de las sibilas. Además, el lugar estaba protegido por la hija de Gaia, la temible serpiente Pitón.\nAl igual que su padre, el dios Apolo, tenía una vida interesante a la par que complicada. No obstante, el primer logro por el que se dio a conocer, fue dar muerte a la serpiente Pitón. Dado que la serpiente Pitón era la hija de la diosa Gaia, habría que reparar tal desagravio. Por ello, Apolo tuvo que trabajar cuidando vacas durante 8 años para purificarse. Una vez pagó su deuda, arrebató el oráculo a Gaia. Desde entonces, fue conocido como Apolo pitio, el dios de la profecía y Delfos fue su principal templo.\nEso es con respecto a la mitología. Excavaciones arqueológicas han mostrado que desde 1500 hasta 1100 A.C., el sitio estaba ocupado por pequeños asentamientos micénicos al final de la edad de Bronce que rezaban a la diosa Gaia. El nuevo dios Apolo llegó, quizás mediante la invasión de los Dorios, y comenzó a dominar. El resultado fue el mayor negocio de predicción más exitoso en la historia. Por casi mil años, el oráculo de Delfos era consultado para temas relacionados con negocios, política, religión y guerra. Con el auge del cristianismo y en el tercer siglo D.C., el oráculo perdería toda su fama (y negocio).\nEl poeta Iamblichus cuenta una historia sobre el oráculo cuando todavía estaba en su máximo apogeo. Un gravador de gemas llamado Mnsarchus visitó el oráculo para preguntarle si un viaje que estaba pensando realizar sería beneficioso. El oráculo le respondió que lo sería, además, le dijo que su mujer estaba embarazada (algo que él no sabía en aquel momento) y que daría a luz a un hijo que destacaría en belleza y sabiduría.\nAsí fue y Mnesarchus se dio cuenta que el hijo fue enviado por lo dioses, y lo llamó Pitágoras (que significa en griego, predicho por la pitia). Matemático, filósofo e incluso entrenador olímpico, Pitágoras desarrolló un nuevo sistema de predicción basado en el poder de los números en lugar del oráculo. Los detalles de cómo este sistema de predicción numérica funcionaba permanece desconocido, dado que no quedan trabajos escritos por Pitágoras. Lo poco que se conoce sobre el legado de Pitágoras se debe a filósofos posteriores como Platón y Aristóteles.\nHasta ahora hemos introducido la historia acerca del arte de la predicción. No obstante, es interesante destacar que aunque en el idioma español no distiguimos entre las predicciones que se realizan de forma científica y las que se realizan mediante técnicas de adivinación (bola de cristal, tarot, etc..). En inglés si existe dicha palabra y es forecasting, cuyo origen creo es digno de mencionar.\nUna de las aplicaciones más famosas del arte de la predicción es la meteorología y de hecho, la meteorología está muy relacionada con la palabra inglesa forecast. Si nos remontamos a Reino Unido en 1854, el Almirante Robert FitzRoy, que había capitaneado el Beagle, barco en el que viajó Darwin alrededor del mundo, Figura 1.1, estableció la primera oficina meteorológica. Este ex-marinero sabía cómo las predicciones meteorológicas tenían potencial para salvar vidas de los marineros advirtiendo las tormentas.\n\n\nFigura 1.1: Acuarela con la llegada del Beagle a Wualia. Autor: Conrad Martens.\n\nLamentablemente, los esfuerzos de FitzRoy no fueron bien recibidos por el público ni por la comunidad científica en aquella época, donde las predicciones meteorológicas provenían principalmente de los astrólogos. Para evitar que compararán sus predicciones con la astrología evitó utilizar palabras como prediction, y en su lugar, inventó una nueva forecast, que la definió de la siguiente manera: The term forecast is strictly applicable to such an opinion as is the result of a scientific combination and calculation"
  },
  {
    "objectID": "01-intro.html#clasificación-de-métodos-de-predicción",
    "href": "01-intro.html#clasificación-de-métodos-de-predicción",
    "title": "\n1  Introducción\n",
    "section": "\n1.2 Clasificación de métodos de predicción",
    "text": "1.2 Clasificación de métodos de predicción\nDado que existe una amplia amalgama de técnicas de predicción, hacer una clasificación exhaustiva de dichas técnicas sería algo complicado. No obstante, una primera aproximación a una clasificación podría ser la siguiente:\n\n\nMétodos Cualitativos: Este tipo de métodos no utilizan ningún tipo de algoritmo matemático, sino que sus predicciones se basan en experiencias, opiniones, consejo de expertos, intuiciones, rumores. A este tipo de información también se le conoce como soft information. Entre los métodos cualitativos más conocidos se encuentran:\n\nMétodo Delphi: Pronóstico desarrollado mediante un grupo de expertos que responden preguntas en rondas sucesivas. Las respuestas anónimas del grupo retroalimentan en cada ronda a todos los participantes. Se pueden usar entre tres y seis rondas para lograr un consenso sobre el pronóstico. El método Delphi fue desarrollado en 1948 al inicio de la guerra fría por la corporación RAND para el departamento de defensa de los EEUU para investigar qué ocurriría en una guerra nuclear.\nEstudios de mercado: Grupos, cuestionarios, pruebas de mercado o estudios que se usan para obtener datos sobre las condiciones del mercado, generalmente sobre intención de compra de un determinado producto\nJuicio de expertos: Pronóstico que puede hacer un grupo o un individuo basándose en sus experiencias, intuición o hechos relacionados con la situación. No se usa método riguroso\n\n\n\nMétodos Cuantitativos: Son métodos que implican algún tipo de ecuación matemática basados en información cuantitativa como ventas anteriores, datos de inventarios, promociones, etc. A este tipo de información también se le conoce como hard information. A la hora de realizar predicciones mediante métodos cuantitativos, los métodos más conocidos se pueden clasificar en función del número de variables que se incluyan en el modelo.\n\nSeries temporales o modelos extrapolativos: Una serie temporal es simplemente el conjunto de valores de una determinada variable ordenados con respecto al tiempo. La Figura 1.2 muestra algunos ejemplos de series temporales que podrán ser analizadas con los métodos que se proponen en este libro. Las series que se muestran en la figura desde izquierda a derecha y desde arriba hacia abajo son las cotizaciones medias diarias del IBEX-35, las entradas mensuales de pasajeros internacionales por avión en España, el PIB per capita trimestral en términos reales y poder de paridad de compra de España, y el caudal anual del río Nilo a su paso por Assuan. Dado que sólo se analiza una variable también se les conoce como modelos univariantes. El análisis de series temporales comprende una gran variedad de métodos que abarcan desde modelos muy básicos hasta modelos muy complejos como podrían ser modelos no lineales basados en redes neuronales. Para el objeto de este libro, nos centraremos en métodos simples que suelen ser los más utilizados en la industria, como las técnicas: i) Ingenuas (Naïve), ii) Medias móviles y iii) Suavizado exponencial.\nMétodos causales o Modelos explicativos: Los métodos causales utilizan más de una variable por ello también se les conoce como multivariantes. En este caso la variable que queremos predecir su valor se expresa en función de otras variables explicativas. Por ejemplo, se podría explicar el consumo de energía eléctrica en función de la temperatura, o las ventas de un producto en función del gasto publicitario. Este tipo de modelos se suelen expresar en forma de regresión, con la única peculiaridad que las variables incluidas en la regresión deben estar ordenadas con respecto al tiempo. Por este motivo, este tipo de regresiones se les denomina regresiones dinámicas.\n\n\n\n\n\nFigura 1.2: Ejemplos de series temporales."
  },
  {
    "objectID": "01-intro.html#series-temporales",
    "href": "01-intro.html#series-temporales",
    "title": "\n1  Introducción\n",
    "section": "\n1.3 Series temporales",
    "text": "1.3 Series temporales\nLa materia prima con la que vamos a contar en este libro son datos procedentes de series temporales extraídas de la realidad, y como tales, con propiedades muy variadas. Como se ha dicho antes, una serie temporal es una sucesión de valores de una variables ordenados en el tiempo. La predicción consistirá en obtener los valores futuros de dicha variable. Dicho de este modo, estaríamos tratando la variable como un conjunto de valores dados o deterministas, como si no pudiera haber sido de otro modo. Esto es cierto en casos muy raros, porque la realidad es que las cosas pueden suceder de muchas formas posibles o, al menos, así lo suponemos. Esta concepción se generalizará más tarde cuando tratemos las series temporales como realizaciones de procesos estocásticos.\nPara ser más explícitos, la Figura 1.3 muestra una posible descomposición de una porción de la serie de pasajeros (Figura 1.2) en tendencia, estacionalidad y ruido (a veces también llamado componente irregular o residuo).\n\n\nFigura 1.3: Componentes de la serie de pasajeros.\n\nEl análisis de series temporales consiste en identificar el patrón de esos datos, es decir, la forma que tienen esos datos a lo largo del tiempo para poder proyectarla en el futuro y utilizar esas proyecciones como predicciones.\nPara identificar el patrón generador de datos, la serie temporal se puede dividir en componentes como nivel promedio, tendencia, estacionalidad y error. La Figura Figura 1.3 muestra la representación gráfica de dichos componentes. De este modo, la serie temporal se puede expresar como la combinación de dichos componentes. Si la forma en que combinamos dichos componentes es aditiva, la serie temporal \\(y(t)\\) se podría expresar de la siguiente forma:\n\\[\n    y(t)=N+T+S+e\n\\tag{1.1}\\] donde \\(N\\) es el nivel, es decir el valor promedio de la serie distinto de cero, \\(T\\) es la tendencia que representa las variaciones lentas de la serie temporal, por eso también se le conoce como componente de baja frecuencia, \\(S\\) es la estacionalidad y finalmente \\(e\\) es el error aleatorio. Hay que señalar que aunque los principales componentes de una serie temporal vienen definidos en Ecuación 1.1, pueden existir otros componentes como variables exógenas y una componente de ciclo, cuyo periodo es diferente de los reflejados en la componente \\(S\\), como por ejemplo el ciclo económico.\nEntre los componentes de una serie temporal, quizás el que parece menos común es la componente de estacionalidad, la cuál se podría definir como la variación cíclica o periódica alrededor de la tendencia. Un ejemplo de serie temporal con estacionalidad podría ser las ventas de juguetes o de mazapanes. Dichas ventas son mayores durante el mes de diciembre y bajas durante el resto de los meses.\nOtra forma de introducir los componentes es mediante la siguiente expresión multiplicativa, tal que:\n\\[\ny(t)=N\\cdot T \\cdot S \\cdot e\n\\tag{1.2}\\]\nO una expresión mixta: \\[\n    y(t)=(N + T) \\cdot S + e\n\\tag{1.3}\\] donde las componentes \\(N\\) y \\(T\\) se combinan de forma aditiva, la estacionalidad \\(S\\) de forma multiplicativa y el error \\(e\\) de forma aditiva.\nEn este libro nos centraremos en el análisis de la familia de suavizado exponencial para realizar las previsiones. Se eligieron estos procedimientos porque suelen estar disponibles en el software comercial y cumplen los criterios de bajo coste y baja participación de la gerencia.\nA continuación, se expondrán las diferentes versiones de algoritmos pertenecientes a la familia del suavizado exponencial. Cada versión u algoritmo cambia dependiendo de los componentes que contenga la serie temporal a analizar."
  },
  {
    "objectID": "02-etsSSOE.html#suavizado-exponencial-simple.-componentes-de-nivel-y-error.",
    "href": "02-etsSSOE.html#suavizado-exponencial-simple.-componentes-de-nivel-y-error.",
    "title": "\n3  Suavizado Exponencial\n",
    "section": "\n3.1 Suavizado exponencial simple. Componentes de nivel y error.",
    "text": "3.1 Suavizado exponencial simple. Componentes de nivel y error.\nEn primer lugar, se asume que el patrón subyacente en la demanda es constante (al menos para los próximos días o semanas) con algunas fluctuaciones aleatorias alrededor del promedio, es decir, se supone que la serie temporal está formada por la combinación de los componentes de nivel y error aleatorio (Ecuación 3.1). Por tanto, las técnicas de predicción que se verán en esta sección asumen el siguiente proceso generador de datos: \\[\n    y(t)= N + e\n\\tag{3.1}\\]\n\n3.1.1 Media móvil\nSuaviza la demanda histórica promediando un número selecto de periodos pasados de datos y utiliza ese promedio como pronóstico para el siguiente periodo. El origen del modelo no es constante, sino que cada vez que se dispone de una nueva observación el origen del modelo se mueve con el fin de utilizar la historia más reciente para realizar las predicciones.\n\n\nFigura 3.1: Diferencia entre observaciones y pronósticos.\n\nEn la Figura Figura 3.1 se muestra cómo se utilizan las observaciones (\\(Y_1, Y_2, \\ldots, Y_t\\)) hasta el origen (\\(t\\)) para realizar los pronósticos para tiempos posteriores (\\(F_{t+1}, F_{t+2},\\ldots,\\)). De forma que, los pronósticos mediante la media móvil, se pueden calcular mediante la siguiente expresión: \\[\n    F_{t+1}=\\sum_{t-n+1}^t \\frac{Y_i}{n}\n\\tag{3.2}\\] donde \\(i\\) es el número del periodo, \\(t\\) es el periodo actual (el periodo para el cual la demanda real más reciente es conocida) y \\(n\\) representa el número de periodos considerados en el promedio móvil. También se le conoce como orden de la media móvil. Otra expresión equivalente para calcular la predicción de la demanda para el periodo \\(t+1\\) mediante una media móvil de orden \\(n\\) es: \\[\n    F_{t+1}=F_t+ \\frac{Y_t-Y_{t-n}}{n}\n\\tag{3.3}\\] Si comparamos la expresión Ecuación 3.2 con Ecuación 3.3 se puede observar que la última ecuación sólo necesita almacenar la última predicción (\\(F_t\\)), la última observación (\\(Y_t\\)) y la observación determinada por el orden de la media móvil (\\(Y_{t-n}\\)). En cambio la primera ecuación tiene que almacenar todos los valores desde \\(Y_{t-n+1}\\) hasta \\(Y_t\\). Además, en Ecuación 3.3 el número de operaciones que tiene que realizar para proporcionar una predicción es menor, lo que implica que es una expresión más eficiente desde el punto de vista computacional.\n\n\nResuelve\nSolución\n\n\n\nRealiza una función en R que tenga como argumentos de entrada la serie temporal y el orden de la media móvil y, como argumento de salida, la predicción de la media móvil un paso hacia delante. Para comprobar que funciona correctamente, simula una normal aleatoria de tamaño 100 y un orden de media móvil de n=3. Dibuja la normal junto con las predicciones de la media móvil.\n\n\n\nrm(list = ls())\nsuppressPackageStartupMessages(library(\"UComp\"))\nsuppressPackageStartupMessages(library(\"ggplot2\"))\nlibrary(UComp)\nlibrary(ggplot2)\nmediamovil &lt;- function(y, n=1){\n    # dimension de la serie temporal\n    dim&lt;-length(y)\n    f&lt;- rep(0,dim+1)\n    #Inicializo\n    f[n+1]&lt;-mean(y[1:n])\n    #Calculo la media móvil recursiva\n    for (t in (n+1): dim){\n    f[t+1]&lt;-f[t]+(y[t]-y[t-n])/n\n    }\n    return(f)\n}\n#Simulo una normal\ndata &lt;- as.ts(rnorm(100))\n#Calculo su media movil de orden 3\npredMM&lt;-as.ts(mediamovil(data,3))\n#Lo dibujo\nautoplot(predMM)+autolayer(data)\n\n\n\n\n\n\n\nAl estar al final del periodo \\(t\\), se conoce la serie temporal en el periodo \\(t\\) y los pronósticos se hacen para \\(t+1\\), o \\(t+X\\) periodos en el futuro. No se hacen pronósticos para el periodo \\(t\\) ya que la serie temporal de éste es conocida (ver Figura 3.1 ). El método ingenuo (Naïve) es una media móvil para el caso particular de \\(n=1\\), el cual utiliza la última observación como predicción para el siguiente periodo. Este método ingenuo se utiliza como modelo de referencia cuando se propone una técnica de predicción, es decir, si alguien propone una técnica de predicción sofisticada para ciertos datos, estará justificada siempre y cuando mejore las predicciones proporcionadas por el método ingenuo.\n\n\nResuelve\nSolución Excel\nSolución R\n\n\n\nLas ventas de un disolvente químico son las mostradas en la tercera columna de la Tabla 3.1, se pide calcular la predicción para el último mes mediante una media móvil de tercer orden y de quinto orden\n\n\nTabla 3.1: Ventas mensuales disolvente\n\nMes\nPeriodo\nVentas\n\n\n\nEnero\n1\n200\n\n\nFebrero\n2\n135\n\n\nMarzo\n3\n195\n\n\nAbril\n4\n197,5\n\n\nMayo\n5\n310\n\n\nJunio\n6\n175\n\n\nJulio\n7\n155\n\n\nAgosto\n8\n130\n\n\nSeptiembre\n9\n220\n\n\nOctubre\n10\n277,5\n\n\nNoviembre\n11\n235\n\n\nDiciembre\n12\n?\n\n\n\n\n\n\nSi aplicamos la ecuación Ecuación 3.2 para \\(n=3\\) y \\(n=5\\) podemos obtener las predicciones solicitadas para el mes de diciembre, ver Tabla 3.2 . La cuarta y quinta columna en la Tabla muestran las predicciones obtenidas para una media móvil de orden 3 y 5, respectivamente.\n\n\nTabla 3.2: Ventas mensuales disolvente y predicciones\n\n\n\n\n\n\n\n\nMes\nPeriodo\nVentas\nMedia Móvil (n=3)\nMedia Móvil (n=5)\n\n\n\nEnero\n1\n200\n\n\n\n\nFebrero\n2\n135\n\n\n\n\nMarzo\n3\n195\n\n\n\n\nAbril\n4\n197,5\n176,7\n\n\n\nMayo\n5\n310\n175,8\n\n\n\nJunio\n6\n175\n234,2\n207,5\n\n\nJulio\n7\n155\n227,5\n202,5\n\n\nAgosto\n8\n130\n213,3\n206,5\n\n\nSeptiembre\n9\n220\n153,3\n193,5\n\n\nOctubre\n10\n277,5\n168,3\n198\n\n\nNoviembre\n11\n235\n209,2\n191,5\n\n\nDiciembre\n12\n?\n244,2\n203,5\n\n\n\n\n\n\nEn construcción\n\n# Borramos memoria\nrm(list = ls())\nsuppressPackageStartupMessages(library(\"ggplot2\"))\nsuppressPackageStartupMessages(library(\"UComp\"))\n# Cargamos librerías\nlibrary(UComp)\nlibrary(ggplot2)\n# Recuperamos la función media móvil\nmediamovil &lt;- function(y, n=1){\n    # dimension de la serie temporal\n    dim&lt;-length(y)\n    f&lt;- rep(0,dim+1)\n    #Inicializo\n    f[n+1]&lt;-mean(y[1:n])\n    #Calculo la media móvil recursiva\n    for (t in (n+1): dim){\n    f[t+1]&lt;-f[t]+(y[t]-y[t-n])/n\n    }\n    return(f)\n}\n#Cargamos los datos\ny&lt;-(ts(c(200,135,195,197.5,310,175,155,130,220,277.5,235)))\n\nmm3&lt;-as.ts(mediamovil(y,3))\nmm5&lt;-as.ts(mediamovil(y,5))\n\nautoplot(y)+ autolayer(mm3) + autolayer(mm5) +labs(title = \"Ventas mensuales\")\n\n\n\n\n\n\n\nEn el ejemplo anterior, se han realizado unas predicciones basadas en las medias móviles de diferente orden. No obstante, uno podría preguntarse, ¿cómo se define el orden de la media móvil? o lo que es lo mismo ¿qué valor de n elijo para realizar mis predicciones? Para determinar el valor de n, se tendrían que calcular los errores de predicción y seleccionar aquel valor de n que los minimice. Como todavía no se han definido los errores de predicción, dejaremos para más adelante en este capítulo como llevar a cabo dicha optimización. De momento, aunque no sepamos cómo elegir n de forma óptima si es recomendable que se entienda el efecto del parámetro de diseño n. Para ello pondremos el siguiente ejemplo. En la Figura 3.2 se ha representado una serie temporal y(t) la cuál queremos predecir. Con ese fin, se han calculado medias móviles con los siguientes valores de n, n=10, 50. ¿Podrías explicar cuál ha sido el efecto de aumentar el valor de n? En principio hay dos efectos muy claros. El primero es que cuanto mayor es el valor de n, más observaciones se requieren para poder empezar a realizar predicciones. El segundo efecto, es que cuanto mayor es el valor de n mayor es el suavizado de nuestras predicciones. Estas mismas conclusiones se pueden observar en el primer ejemplo en la Figura 3.2. De acuerdo a estas conclusiones iniciales uno podría pensar que la mejor decisión será elegir un orden lo mayor posible para conseguir un mayor grado de suavizado. No obstante esta decisión también tiene sus inconvenientes. Si repetimos el mismo experimento anterior, pero en este caso sobre la serie temporal mostrada en la Figura 3.3, se puede observar que la media móvil de orden (\\(n=50\\)) es incapaz de adaptarse al salto brusco que ha ocurrido en los datos, mientras que la media móvil de menor orden (\\(n=10\\)) se adapta mucho más rápido. Por tanto, cuanto mayor sea el orden de la media móvil, mayor será su suavizado pero menor será su capacidad de adaptación a cambios bruscos en la serie temporal.\n\n\nFigura 3.2: Efecto de suavizado del parámetro n.\n\n\n\nFigura 3.3: Efecto del parámetro n sobre la capacidad de adaptación a cambios bruscos.\n\n\n3.1.2 Suavizado exponencial\nEl algoritmo de suavizado exponencial fue inicialmente propuesto por Robert Brown sobre 1944, cuando estuvo trabajando como analista de investigación de operaciones en la marina de los EEUU, donde implementó esta idea en un aparato mecánico con el fin de rastrear la velocidad y ángulo utilizados para derribar submarinos. Otra de sus primeras aplicaciones fue pronosticar la necesidad de piezas de repuesto en el sistema de inventarios de la marina de los EEUU. Posteriormente, sobre 1950 extendió sus resultados para incluir componentes de tendencia y estacionalidad. Es interesante señalar que, independientemente, Charles Holt que trabajaba en la oficina de investigación naval desarrolló también un suavizado exponencial incluyendo las componentes de tendencia y estacionalidad.\nAl igual que la media móvil, el suavizado exponencial se utiliza cuando las componentes principales en la serie temporal son el nivel y el error aleatorio. Si analizamos el ejemplo 1 de la sección anterior, se puede comprobar que la media móvil suaviza los datos históricos ponderando con un mismo peso cada observación del histórico de datos. De forma que si volvemos a calcular la media móvil de orden 3 del ejemplo anterior, teníamos que: \\[\n\\begin{equation}\n    F_{12}=\\frac{220 + 277.5 + 235}{3}=244.2\n\\end{equation}\n\\] donde el peso de cada observación sería \\(1/3\\). No obstante, uno podría pensar que tendría más sentido que lo que ha pasado ayer afecte más a lo que pase mañana, que lo que ocurrió hace 2 semanas, o dicho en otras palabras, que los pesos más cercanos al presente sean mayores que los pesos más alejados del presente.\nEn este apartado se describe el suavizado exponencial que, siguiendo esa lógica, coloca los pesos en las observaciones de manera que decrecen exponencialmente según las observaciones se alejan del presente. El pronóstico por suavizado exponencial se calcula como: \\[\n\\begin{eqnarray}\n    F_{t+1}&=&F_t + \\alpha(Y_t - F_t)  \\\\\n    F_{t+1}&=&\\alpha(Y_t) + (1-\\alpha)F_t    \n\\end{eqnarray}\n\\tag{3.4}\\] donde \\(\\alpha\\) es una constante positiva entre 0 y 1. Se puede observar que la nueva predicción es simplemente la predicción anterior más un ajuste de los errores de predicción que ocurrieron en el periodo anterior. Cuando \\(\\alpha\\) tiene un valor cercano a 1, el nuevo pronóstico incluirá un ajuste sustancial debido al error en el pronóstico anterior. Sin embargo, si tiene una valor cercano a 0, el nuevo pronóstico no se ajustará mucho. Las implicaciones del suavizado exponencial se pueden ver mejor si expandimos la ecuación anterior: \\[\n\\begin{eqnarray}\nF_{t+1}&=&\\alpha Y_t + \\alpha (1-\\alpha)Y_{t-1} + \\alpha(1-\\alpha)^2Y_{t-2}+ \\alpha(1-\\alpha)^3Y_{t-3} \\nonumber \\\\\n&&+\\alpha(1-\\alpha)^4Y_{t-4}+\\ldots+(1-\\alpha)^{t}F_1   \n\\end{eqnarray}\n\\tag{3.5}\\] La Ecuación 3.5 muestra como se distribuyen los pesos en las observaciones pasadas, de modo que si \\(\\alpha\\) es menor que 1, dicho peso decrece exponencialmente. Es interesante señalar el último término de dicha expresión \\((1-\\alpha)^{t}F_1\\), donde \\(F_1\\) es la predicción inicial, la cuál desconocemos y tenemos que estimar. La estimación de dicho valor se le conoce como inicialización del algoritmo.\n\n\nResuelve\nSolución\n\n\n\nSupongamos que \\(\\alpha=0,3\\) y \\(t=3\\), ¿Cuál es el peso que tendrá la predicción inicial \\(F_1\\)?\n\n\n\\[\nF_4=0.3Y_3+0.21Y_2+0.147Y_1+0.343F_1\n\\] Se puede observar que el peso de la predicción inicial \\(F_1\\) es igual a 0.343 que es incluso superior al peso de la observación más reciente \\(Y_3\\) que es 0.3. Por tanto, hay que ser conscientes que si disponemos de pocas observaciones el peso de la predicción inicial es muy significativo y, por tanto, es crucial estimar adecuadamente \\(F_1\\).\n\n\n\n\n\nResuelve\nSolución\n\n\n\nPara el mismo valor de \\(\\alpha\\) del ejercicio anterior, ¿cuál sería el peso de \\(F_1\\) si t=20? Para resolverlo, programa una función que calcule los pesos en función del valor de \\(\\alpha\\) y \\(t\\). Dibuja la evolución de dichos pesos frente al tiempo.\n\n\n\n# Borramos memoria\nrm(list = ls())\nsuppressPackageStartupMessages(library(\"ggplot2\"))\nsuppressPackageStartupMessages(library(\"UComp\"))\n# Cargamos librerías\nlibrary(UComp)\nlibrary(ggplot2)\n# Recuperamos la función media móvil\nsespesos &lt;- function(t=10, a=0.1){\n    peso&lt;-rep(0,t+1)\n    for (i in 1:(t) ) {\n    #Calculo la media móvil recursiva\n    peso[i]&lt;-a*(1-a)^(i-1)\n    }\n    #Le añado el peso de la condición inicial\n    peso[t+1]&lt;-(1-a)^t\n    return(peso)\n}\npesos&lt;-sespesos(20,0.3)\nd1&lt;-data.frame(tiempo=c(1:21),pesos)\n\nggplot(data = d1,mapping = aes(x = tiempo, y=pesos)) + \n    geom_line() +\n    geom_point() +\n    scale_x_continuous(breaks=c(1:21),labels=c(paste(\"Y\",20:1,sep=\"\"), paste(\"F\",1,sep=\"\")))\n\n\n\n\nEn este caso, el peso sería aproximadamente cero, es decir, cuando disponemos de más observaciones las predicciones iniciales tienen muy poco peso.\n\n\n\n\n\nResuelve\nSolución\n\n\n\nPara el mismo valor de \\(t=20\\) del ejercicio anterior, ¿qué ocurriría para diferentes valores de \\(\\alpha=0.2, 0.3, 0.7\\)? Dibuja la evolución de dichos pesos frente al tiempo.\n\n\n\n# Borramos memoria\nrm(list = ls())\nsuppressPackageStartupMessages(library(\"ggplot2\"))\nsuppressPackageStartupMessages(library(\"UComp\"))\nsuppressPackageStartupMessages(library(\"latex2exp\"))\n# Cargamos librerías\nlibrary(UComp)\nlibrary(ggplot2)\nlibrary(latex2exp)\n\n# Recuperamos la función media móvil\nsespesos &lt;- function(t=10, a=0.1){\n    peso&lt;-rep(0,t+1)\n    for (i in 1:(t) ) {\n    #Calculo la media móvil recursiva\n    peso[i]&lt;-a*(1-a)^(i-1)\n    }\n    #Le añado el peso de la condición inicial\n    peso[t+1]&lt;-(1-a)^t\n    return(peso)\n}\npesos1&lt;-sespesos(20,0.2)\npesos2&lt;-sespesos(20,0.3)\npesos3&lt;-sespesos(20,0.7)\nd1&lt;-data.frame(tiempo=rep(c(1:21),3),pesos=c(pesos1,pesos2,pesos3),alpha=c(rep(0.2,21),rep(0.3,21),rep(0.7,21)))\n\n\nggplot(data = d1,aes(x = tiempo, y=pesos, group=alpha, color=as.factor(alpha))) + \n    geom_point() +\n    geom_line() +\n    scale_x_continuous(breaks=rep(c(1:21),3),labels=rep(c(paste(\"Y\",20:1,sep=\"\"), paste(\"F\",1,sep=\"\")),3)) +\n    labs(x=\"Observaciones\", y=\"Pesos\", color=TeX(\"$\\\\alpha$\"))\n\n\n\nFigura 3.4: Efecto del parámetro alpha sobre los pesos del suavizado exponencial.\n\n\n\nLa Figura 3.4 muestra la influencia del parámetro \\(\\alpha\\) sobre el peso de las observaciones. En dicha figura se puede observar que cuanto más cercano a 1 se encuentra el parámetro \\(\\alpha\\), más peso se le dan a las observaciones más recientes y viceversa.\n\n\n\n\n\nResuelve\nSolución\n\n\n\nUna bodega en la Mancha ha exportado grandes cantidades de vino de calidad suprema durante los ocho últimos trimestres que se muestran en la segunda columna de la Tabla 3.3. El director de operaciones quiere analizar la utilización de la técnica de suavizado exponencial en la previsión de las cajas de botellas exportadas. Para ello, supone que la previsión de vino exportado en el primer trimestre fue de 1.750 cajas. Se plantea utilizar los siguientes valores \\(\\alpha=0,1\\) y \\(0,5\\). ¿Cuál será la previsión para el trimestre 9?\n\n\nTabla 3.3: Ventas trimestrales de la bodega\n\nTrimestre\nVentas\n\n\n\n1\n1.800\n\n\n2\n1.700\n\n\n3\n1.590\n\n\n4\n1.600\n\n\n5\n1.800\n\n\n6\n2.000\n\n\n7\n1.850\n\n\n8\n1.820\n\n\n9\n?\n\n\n\n\n\n\n\n# Borramos memoria\nrm(list = ls())\nsuppressPackageStartupMessages(library(\"ggplot2\"))\nsuppressPackageStartupMessages(library(\"UComp\"))\n\n# Cargamos librerías\nlibrary(UComp)\nlibrary(ggplot2)\n\n#Programo una función que calcule el suavizado exponencial simple\nses &lt;- function(y, a=0.03){\n    t&lt;-length(y)\n    f&lt;-rep(0,t+1)\n    #Inicializo\n    f[1]&lt;-y[1]\n    # Recorremos el vector y\n    for (i in 1:(t) ) {\n    #Calculo la media móvil recursiva\n    f[i+1]&lt;-f[i]+a*(y[i]-f[i])\n    }\n    return(f)\n}\n\nventas&lt;-c(1800,1700,1590,1600,1800,2000,1850,1820)\n\npred1&lt;-ses(ventas,0.1)\npred2&lt;-ses(ventas,0.5)\n\n#Dibujamos los resultados\ndv&lt;-data.frame(tiempo=c(1:length(ventas)),ventas)\nd1&lt;-data.frame(tiempo=c(rep(c(1:9),2),1:8),pred=c(pred1,pred2,ventas),alpha=c(rep(\"alpha=0.1\",9),rep(\"alpha=0.5\",9),rep('Ventas',8)))\n\nggplot() + \n    geom_point(data = d1,aes(x = tiempo, y=pred, group=alpha, color=as.factor(alpha))) +\n    geom_line(data = d1,aes(x = tiempo, y=pred, group=alpha, color=as.factor(alpha))) +\n    labs(x=\"Observaciones\", y=\"Predicciones\", color=\"Leyenda\")\n\n\n\n\nEn la figura se muestran las cajas descargadas junto con las previsiones utilizando un suavizado exponencial simple para los dos valores de \\(\\alpha\\) considerados. Se puede observar que un valor del \\(\\alpha\\) más cercano a 0 proporciona un mayor suavizado en las predicciones.\n\n\n\nHasta ahora hemos supuesto que el valor de la predicción inicial \\(F_1\\) era un dato conocido. No obstante, en la realidad dicha predicción inicial no es conocida sino que hay que estimarla. Una solución heurística común es usar la primera observación como pronóstico inicial (\\(F_1=Y_1\\)). Otra posibilidad más robusta es promediar los tres, cuatro primeros valores y usar ese promedio como pronóstico inicial, tal que:\n\\[\n    F_1=\\frac{Y_1+Y_2+Y_3}{3}\n\\]"
  },
  {
    "objectID": "02-etsSSOE.html#evaluación-de-las-predicciones",
    "href": "02-etsSSOE.html#evaluación-de-las-predicciones",
    "title": "\n3  Suavizado Exponencial\n",
    "section": "\n3.2 Evaluación de las predicciones",
    "text": "3.2 Evaluación de las predicciones\nEn la Figura 3.5 (1) se muestra una pintura de Caravaggio localizada en el museo del Louvre de París: La buenaventura, 1595. El cuadro muestra a un joven vestido con ropas elegantes al que una chica pobre lee la palma de la mano. El chico parece encantado al mirarle a la cara (no se da cuenta de que ella está quitándole poco a poco el anillo) a esta mirada masculina ella responde con su propia mirada astuta y silenciosa. La atención del espectador se centra precisamente en esas miradas que permiten adivinar lo que cada uno de los personajes piensan, en lugar de lo que ocurre en las dos manos de los personajes.\nEste cuadro se menciona en el libro del cisne negro de Nassim Taleb para mostrarnos cuán ingenuos podemos llegar a ser frente a aquellos que nos cuentan el futuro. Para no ser tomados como ingenuos, necesitaremos ser críticos con las predicciones a las que nos tengamos que enfrentar. Para ello una herramienta muy útil es conocer el error de predicción. De forma que cuando alguien nos presente una estimación para no ser tomados por ingenuos también deberíamos exigir cuál es el error de dicha estimación. Por ejemplo, no es lo mismo decir nuestra previsión de ventas para el mes siguiente es de 1000 unidades más/menos 10 que tener una previsión de 1000 unidades más/menos 500.\nOtro cuadro que muestra el interés de conocer el futuro se muestra en Figura 3.5 (2). Dicho cuadro de Julio Romero (1920) se muestra en el museo Thyssen de Málaga (https://www.carmenthyssenmalaga.org/obra/la-buenaventura)\n\n\n\n\n\n(1) La buenaventura de Caravaggio.\n\n\n\n\n\n(2) La buenaventura de Julio Romero de Torres.\n\n\n\nFigura 3.5: Cuadros representando el interés de los humanos por conocer el futuro.\n\n\nEn resumen, el ejercicio de previsión no finaliza una vez realizadas las predicciones sino que tenemos que comprobar que dichas previsiones son adecuadas. Para ello es importante estimar los errores de predicción. Además, el análisis de los errores nos permite:\n\nDetectar indicadores de demandas erráticas que deben evaluarse con cuidado y quizás eliminar de los datos.\nEstimar los parámetros de los algoritmos mediante la minimización de los errores de predicción.\nDeterminar cuando el método de pronóstico no representa ya la demanda actual y es necesario volver a proponer diferentes modelos.\n\n\n3.2.1 Medidas de error de predicción\nIndependientemente de si se utilizan métodos o modelos formales para realizar la predicción de cualquier variable, algo que nos va a interesar siempre es comprobar la calidad de las predicciones, su nivel de precisión e incluso comparar la precisión de distintos métodos para las mismas series, con el fin de elegir el mejor.\nDenotamos por \\(\\hat{y}_{T+l|T}\\) a la predicción de la variable \\(y_t\\), \\(l\\) periodos hacia adelante con origen de predicción \\(T\\). El error de predicción \\(l\\) periodos hacia adelante con origen \\(T\\) se define como la diferencia entre la predicción y el valor real, tal que:\n\\[\\hat{e}_{T+l|T}=\\hat{y}_{T+l|T}-y_{T+l}\\]\nA la hora de definir las métricas hay que distinguir dos conceptos: a) el sesgo del error; y b) la precisión del error.\nEl sesgo nos indica el error sistemático de las predicciones, es decir, si las predicciones son superiores o inferiores a los valores reales. Evidentemente, para predicciones individuales el error nunca será exactamente cero, pero lo ideal es que los errores positivos se compensen con errores negativos. Por tanto, las medidas de sesgo tienen asociado un signo positivo o negativo. La métrica más conocida es el ME (ver Tabla 3.4)\n\n\nTabla 3.4: Métricas de sesgo.\n\n\n\n\n\n\n\nMétrica\nFórmula\n\n\n\nSin escala\nMean Error\n\\(ME=\\frac{1}{n}\\sum_{l=1}^{n}\\hat{e}_{T+l|T}\\)\n\n\nPorcentual\nMean Percentage Error\n\\(MPE=\\frac{100}{n}\\sum_{l=1}^{n}\\frac{\\hat{e}_{T+l|t}}{y_t}\\)\n\n\n\n\nLas métricas de sesgo nos indican el signo del error pero no el tamaño del mismo, el cuál se define como la precisión del error. Dicho de otra forma, si utilizamos el ME, los errores positivos se compensan con los negativos y no nos da una indicación de cómo de grandes son los errores. Para completar las métricas de sesgo hay que incluir otra métricas que nos indiquen la precisión del error, es decir, su tamaño. Estas métricas transformarán los errores para evitar el signo de forma que los errores siempre sumen. Las formas más comunes para evitar el signo de los errores es usar valores absolutos o elevar los errores al cuadrado. De esta forma, se obtiene el Mean Absolute Error (MAE), el Mean Squared Error (MSE) y el Root Mean Squared Error (RMSE=\\(\\sqrt{MSE}\\)). En la Tabla 3.5 se muestra un resumen de las métricas de precisión. Hay que señalar que tanto el MAE como el RMSE tienen las mismas unidades que la variable de estudio \\(y_t\\) lo que facilita su interpretación.\nLas métricas definidas hasta ahora son útiles cuando se está analizando una serie temporal. En caso de que se necesite calcular el error de predicción sobre un conjunto de series temporales se suelen utilizar otras métricas que eliminen las unidades de cada serie temporal y así se puedan agregar los resultados. Entre estas métricas, nos encontramos los errores porcentuales, los errores escalados y los errores relativos.\nEl error porcentual (PE) se calcula como:\n\\[PE=\\frac{\\hat{e}_{T+l|T}}{y_{T+l}}\\cdot 100\\] Por tanto, una médida de sesgo porcentual sería el Mean Percentage Error (MPE), ver Tabla 3.4 y una medida de tamaño sería el Mean Absolute Percentage Error (MAPE), ver Tabla 3.5. A pesar de que la métrica MAPE es muy intuitiva, su uso cada vez es menor dado la asimetría que presenta, es decir, penaliza de forma diferente, de ahí su asimetría, los errores positivos y negativos. Para corregir dicha asimetría se ha propuesto el Symmetric MAPE (sMAPE), donde los denominadores son la media de valores absolutos de valores reales y predicciones.\nUn problema que presentan las métricas porcentuales es su utilización cuando la serie temporal presenta ceros entre sus valores. Por ejemplo, las ventas de una pieza de repuesto donde en algunos periodos temporales las ventas son cero. Para solucionar la problemática de las métricas porcentuales, se han propuesto los errores escalados como el Mean Absolute Scaled Error (MASE), donde el denominador que normaliza los errores está compuesto por el error absoluto medio de las predicciones Naïve un paso hacia delante. De esta forma se compara la precisión de las predicciones fuera de la muestra con la precisión de un modelo sencillo dentro de la muestra. La versión cuadrática del MASE es el Root Mean Squared Scaled Error (RMSSE)\nEn la Tabla 3.4 y Tabla 3.5 se promedian siempre funciones del error de predicción con un horizonte de \\(n\\) periodos hacia adelante desde el origen de predicción, que será por lo general el final de la muestra, la observación \\(T\\)-ésima.\n\n\nTabla 3.5: Métricas de precisión.\n\n\n\n\n\n\n\nMétrica\nFórmula\n\n\n\nSin escala\nMean Absolute Error\n\\(MAE=\\frac{1}{n}\\sum_{l=1}^{n}|\\hat{e}_{T+l|T}|\\)\n\n\n\nRoot Mean Squared Error\n\\(RMSE=\\sqrt{\\frac{1}{n}\\sum_{l=1}^{n}\\hat{e}^2_{T+l|T}}\\)\n\n\nPorcentual\nMean Absolute Percentage Error\n\\(MAPE=\\frac{100}{n}\\sum_{l=1}^{n}|\\frac{\\hat{e}_{T+l|T}}{y_t}|\\)\n\n\n\nSymmetric MAPE\n\\(sMAPE=\\frac{100}{n}\\sum_{l=1}^{n}|\\frac{\\hat{e}_{T+l|T}}{(|\\hat{y}_{T+l|T}|+|y_t|)/2}|\\)\n\n\n\nRoot Mean Squared Percentage Error\n\\(RMSPE=\\sqrt{\\frac{100}{n}\\sum_{l=1}^{n}\\frac{\\hat{e}^2_{T+l|T}}{y_t^2}}\\)\n\n\nEscalada\nMean Absolute Scaled Error\n\\(MASE=\\sqrt{\\frac{\\frac{1}{n}\\sum_{l=1}^{n}|\\hat{e}_{T+l|T}|}{\\frac{1}{T-1}\\sum_{t=1}^T |y_t-y_{t-1}|}}\\)\n\n\n\nRoot Mean Squared Scaled Error\n\\(RMSSE=\\sqrt{\\frac{\\frac{1}{n}\\sum_{l=1}^{n}\\hat{e}^2_{T+l|T}}{\\frac{1}{T-1}\\sum_{t=1}^T (y_t-y_{t-1})^2}}\\)\n\n\nRelativos\nRelative MAE\n\\(RelMAE=\\frac{MAE}{MAE_b}\\)\n\n\n\n\n\n\nResuelve\nSolución\n\n\n\nBorra toda la memoria y carga las librerías que necesites (rm, UComp, ggplot2). Representa la serie pasajeros de avión en la variable airpas frente al tiempo con autoplot. La escala es muy grande, divide por 100.\nTomando los datos desde enero de 1995 hasta febrero de 2020, estima las predicciones a partir de marzo con los métodos sencillos (el naive, naive estacional y media anual) hasta el final de la muestra. Muestra las predicciones y los valores reales (autoplot + autolayer).\n¿Qué modelo es el que tiene mayor precisión fuera de la muestra (Accuracy)? Estima los pasajeros que se han perdido por la pandemia (colSums).\n\n\n\nrm(list = ls())\nlibrary(UComp)\nlibrary(ggplot2)\ny = airpas / 100\nautoplot(y) + labs(title = \"Pasajeros\")\n\n\n\n\nTomando los datos desde enero de 1995 hasta febrero de 2020, estima las predicciones a partir de marzo con los métodos sencillos (el naive, naive estacional y media anual) hasta el final de la muestra. Muestra las predicciones y los valores reales (autoplot + autolayer).\n\n# Horizonte de predicción\nh = 31\n# Seleccionamos datos entre 1995 y 2020\nx = window(y, start=1995, end = 2020 + 1 / 12)\n# Predicción de modelos convirtiéndolos a objetos series temporales\nnaive = ts(rep(tail(x, 1), h), start = 2020 + 2 / 12, frequency = 12)\nsnaive = ts(head(rep(tail(x, 12), 3), h), start = 2020 + 2 / 12, frequency = 12)\nmediaAnual = ts(rep(mean(tail(x, 12)), h), start = 2020 + 2 / 12, frequency = 12)\n# Representando serie y predicciones\nautoplot(tail(y, 80)) + \n    autolayer(naive) + \n    autolayer(snaive) + \n    autolayer(mediaAnual)\n\n\n\n\n¿Qué modelo es el que tiene mayor precisión fuera de la muestra (Accuracy)? Estima los pasajeros que se han perdido por la pandemia (colSums).\n\n# Calculando métricas de error\nAccuracy(cbind(naive, snaive, mediaAnual), y)\n##                  ME      RMSE       MAE       MPE    PRMSE      MAPE    sMAPE\n## naive       53.6947  95.61781  82.93178  857.6759 2911.154  870.0941 75.65195\n## snaive     118.6199 139.26591 118.61992 1233.4289 4085.732 1233.4289 85.71962\n## mediaAnual 112.5124 137.54520 119.63512 1206.6896 3982.979 1209.5075 87.11351\n##                MASE    RelMAE Theil's U\n## naive      14.03755 0.7228638  2.340946\n## snaive     20.07834 1.0339347  3.285460\n## mediaAnual 20.25018 1.0427836  3.202833\n# Calculando viajeros perdidos\ncolSums(cbind(naive, snaive, mediaAnual) - rep(tail(y, h), 3))\n##      naive     snaive mediaAnual \n##   1664.536   3677.217   3487.884\n\nTodos los modelos están sesgados al alza (no son capaces de predecir el efecto de la pandemia) si se mira el ME y MPE. El modelo más preciso es naive, seguido de naive estacional y la media anual.\nLos pasajeros perdidos de los modelos, serían 166, 367 y 348 millones en los 31 meses evaluados según el naive, snaive y mediAnual, respectivamente.\n\n\n\n\n\n\n\n\n\nNota\n\n\n\nEn general no hay una medida de error que pueda darnos toda la información acerca de la precisión de las predicciones, y por tanto, se recomienda el uso de varias métricas cuando se realicen predicciones. Concretamente, se recomienda, al menos, utilizar una métrica de sesgo y otra métrica de precisión.\n\n\n\n3.2.2 Intervalos de predicción\nComo hemos discutido anteriormente, el suavizado exponencial simple es un método de predicción y, por tanto, sólo proporciona una predicción puntual. No obstante, la predicción puntual no nos indica nada acerca de su precisión. De hecho, en muchas ocasiones las predicciones puntuales se ofrecen con numerosos decimales lo que da una idea errónea de la precisión de la predicción. Para conocer la precisión de una predicción, se suele utilizar los intervalos de predicción, los cuales consisten en un límite superior e inferior entre los cuales se espera que el valor futuro se encuentre con una probabilididad determinada. Aunque hay diferentes formas de calcular los intervalos de predicción, dado que hasta el momento estamos trabajando con métodos de predicción, se utilizarán los errores de predicción para calcular el intervalo de predicción de una forma empírica. Más adelante, cuando se describan los modelos de predicción se podrá calcular los intervalos de predicción de una forma teórica.\nEn general, un intervalo de predicción proporciona un rango dentro del cuál esperamos que se encuentre el valor real \\(y_t\\) con una probabilidad determinada. Por ejemplo, si suponemos que la distribución de las observaciones futuras sigue una normal, un intervalo de predicción para la predicción un paso hacia adelante con una confianza del \\(100(1-\\alpha)\\%\\) sería:\n\\[\n[U_{t+1}, L_{t+1}]=[F_{t+1} \\pm z_{\\alpha/2} \\cdot \\hat{\\sigma}_{t+1}]\n\\]\ndonde \\(U_{t+1}\\) es el límite superior del intervalo de predicción, \\(L_{t+1}\\) es el límite inferior del intervalo de predicción, \\(\\hat{\\sigma}_{t+1}\\) es una estimación de la desviación típica de la distribución de las predicciones un paso hacia adelante, y \\(F_{t+1}\\) que es la predicción puntual un paso hacia adelante proporcionada, por ejemplo, con el suavizado exponencial simple. El valor \\(z_{\\alpha/2}\\) se obtiene de la distribución normal estandard. Por ejemplo, si el nivel de confianza es del 95%, entonces \\(z_{\\alpha/2}=1,96\\).\nSe puede observar como el intervalo de predicción completa la información proporcionada por la predicción puntual. Concretamente, nos da una indicación de la incertidumbre asociada a la prediccion puntual, o dicho de otra forma, si el intervalo de predicción es muy amplio, la incertidumbre de la predicción puntual será grande y viceversa.\nNormalmente, la mayor dificultad a la hora de calcular el intervalo de predicción se refiere al cálculo de \\(\\hat{\\sigma}_{t+1}\\). Una aproximación que se suéle utilizar es estimar \\(\\hat{\\sigma}_{t+1}\\) con la métrica \\(RMSE\\) obtenida del análisis de los errores de predicción, tal que \\(\\hat{\\sigma}_{t+1}=RMSE\\)."
  },
  {
    "objectID": "02-etsSSOE.html#suavizado-exponencial-doble-método-de-holt.-componentes-de-nivel-tendencia-y-error.",
    "href": "02-etsSSOE.html#suavizado-exponencial-doble-método-de-holt.-componentes-de-nivel-tendencia-y-error.",
    "title": "\n3  Suavizado Exponencial\n",
    "section": "\n3.3 Suavizado exponencial doble (Método de Holt). Componentes de nivel, tendencia y error.",
    "text": "3.3 Suavizado exponencial doble (Método de Holt). Componentes de nivel, tendencia y error.\nAnteriormente, analizamos dos técnicas (media móvil y suavizado exponencial simple) que nos daban la predicción en caso que el patrón de los datos fuera un nivel constante con fluctuaciones aleatorias. En el caso de que los datos exhiban una tendencia positiva o negativa, el suavizado exponencial o la media móvil no proporcionarían buenas predicciones, y por tanto, el suavizado exponencial debe ser modificado para incluir tendencias en el modelo.\nEn esta sección, se analizará la versión del suavizado exponencial que incluye la componente de tendencia, dicho de otra forma, se está asumiendo que el modelo generador de datos es:\n\\[\n    y(t)= N + T + e\n\\]\ndonde se ha incluido la componente de tendencia (\\(T\\)) en forma aditiva. Si incluimos la componente de tendencia se obtiene la siguiente expresión también conocida como el doble suavizado exponencial o método de Holt:\n\\[\n\\begin{eqnarray}\nL_t&=&\\alpha(Y_t) + (1-\\alpha)[L_{t-1} + b_{t-1}] \\\\\nb_t&=&\\beta(L_t - L_{t-1}) + (1-\\beta)b_{t-1}  \\\\\nF_{t+1} &=& L_t + b_t\n\\end{eqnarray}\n\\]\ndonde \\(L_t\\) es la componente de nivel y \\(b_t\\) la componente de tendencia, la cual se actualiza con la diferencia de \\(L_t - L_{t-1}\\). La nueva componente de tendencia, también incluye una nueva constante de suavizado \\(\\beta\\) que es un parámetro de diseño como \\(\\alpha\\).\n\n\nResuelve\nSolución con Excel\nSolución con R\n\n\n\nUn determinado productor está utilizando la técnica de doble suavizado exponencial para predecir la demanda de una pieza del equipamiento de control de contaminación, ver datos en las dos primeras columnas de la Tabla 3.6. Parece que los datos contienen cierta tendencia. Las constantes de suavizado que ha definido la empresa son \\(\\alpha=0.2\\) y \\(\\beta=0.4\\). Se supone que el nivel inicial (\\(L_1\\)) fue 11 unidades y la tendencia en dicho periodo (\\(b_1\\)) fue 2. Calcule las predicciones para dicha pieza mediante el método de Holt.\n\n\nTabla 3.6: Datos de demanda mensuales.\n\nMes\nDemanda\n\n\n\n1\n12\n\n\n2\n17\n\n\n3\n20\n\n\n4\n19\n\n\n5\n24\n\n\n6\n21\n\n\n7\n31\n\n\n8\n28\n\n\n9\n36\n\n\n\n\n\n\n\n\nTabla 3.7: Datos de demanda mensuales. Componentes de la series temporal estimados por el método de Holt.\n\n\n\n\n\n\n\n\n\nMes\nDemanda\nNivel (\\(L_t\\))\nTendencia (\\(b_t\\))\nHolt\nSES\n\n\n\n1\n12\n11\n2\n\n12\n\n\n2\n17\n13.8\n2.3\n13.0\n12.0\n\n\n3\n20\n16.9\n2.6\n16.1\n13.0\n\n\n4\n19\n19.4\n2.6\n19.5\n14.4\n\n\n5\n24\n22.4\n2.7\n22.0\n15.3\n\n\n6\n21\n24.3\n2.4\n25.2\n17.1\n\n\n7\n31\n27.6\n2.8\n26.7\n17.8\n\n\n8\n28\n29.9\n2.6\n30.3\n20.5\n\n\n9\n36\n33.2\n2.9\n32.4\n22.0\n\n\n\n\nEn la tercera y cuarta columna de esa misma tabla se muestran las estimaciones de las componentes de la serie temporal \\(L_t\\) y \\(b_t\\) junto con las predicciones obtenidas mediante el método de Holt (quinta columna) y el suavizado exponencial simple (sexta columna), donde para este último se utilizó un \\(\\alpha=0.2\\). En la Figura 3.6 se han dibujado la demanda junto con las predicciones. Se puede observar como las predicciones mediante el método de Holt capturan la tendencia incluida en la serie temporal, mientras que el suavizado exponencial simple (SES) no proporciona una buena predicción de la serie temporal.\n\n\nFigura 3.6: Resultados de predicción.\n\n\n\nComo hicimos con el suavizado exponencial simple podríamos programar una función que implementara las ecuaciones del método de Holt. Sin embargo, todas esas ecuaciones se han implementado en el paquete UComp. Concretamente, en la función ETS que se detallará más adelante. Por el momento, lo que necesitamos definir es ¿cuáles son las componentes de la serie temporal? De hecho, ETS hace referencia al Error, Tendencia y eStacionalidad. Hasta ahora sólo consideraremos ruidos aditivos. Según el gráfico de las ventas, la tendencia es aditiva y tampoco hay indicios de estacionalidad. Por tanto el modelo ETS equivalente al método de Holt será ETS=“AAN”. De la misma forma, el modelo ETS equivalente al suavizado exponencial simple será ETS=“ANN”. Hay que señalar que la optimización de los parámetros y la inicialización la realiza automáticamente la función ETS.\n\nrm(list = ls())\nlibrary(UComp)\nlibrary(ggplot2)\n\n# Ventas\nyts&lt;-ts(c(12,17,20,19,24,21,31,28,36),start=1,end=9)\n# Selecionando muestra\nx = window(yts, start=1, end = 8)\n# Horizonte de predicción\nh = 1\n# Estimando y prediciendo con Doble Suavizado Exponencial\nmholt = ETS(x, model=\"AAN\", h = h)\npredholt&lt;-mholt$yFor\nprint(predholt)\n## Time Series:\n## Start = 9 \n## End = 9 \n## Frequency = 1 \n##      Series 1\n## [1,] 31.67857\n# Estimando y prediciendo con Suavizado Exponencial Simple\nmses = ETS(x, model=\"ANN\", h=h)\npredSES&lt;-mses$yFor\nprint(predSES)\n## Time Series:\n## Start = 9 \n## End = 9 \n## Frequency = 1 \n##      Series 1\n## [1,] 28.14612\n# Podemos dibujar el ajuste \nautoplot(yts)+autolayer(mholt$comp[,2])\n\n\n\n\n\n\n\nAl igual que ocurrió con el suavizado exponencial simple, tenemos que decidir como inicializar el algoritmo, o lo que es lo mismo, estimar los valores iniciales de nivel (\\(L_1\\)) y pendiente (\\(b_1\\)). Una posibilidad es la siguiente:\n\\[\nL_1=\\frac{(Y_1+Y_2+\\ldots+Y_n)}{s}\n\\]\n\\[\nb_1=\\frac{1}{n-1}\\left[\\frac{(Y_{2}-Y_1)}{1}+\\frac{(Y_{3}-Y_2)}{1}+\\ldots+\\frac{(Y_{n}-Y_{n-1})}{1}\\right]\n\\]\ndonde \\(n\\) es el número de observaciones utilizadas para la inicialización. Por ejemplo, con las tres o cuatro primeras observaciones puede ser suficiente."
  },
  {
    "objectID": "02-etsSSOE.html#triple-suavizado-exponencial-holt-winters.-componentes-de-nivel-tendencia-y-estacionalidad",
    "href": "02-etsSSOE.html#triple-suavizado-exponencial-holt-winters.-componentes-de-nivel-tendencia-y-estacionalidad",
    "title": "\n3  Suavizado Exponencial\n",
    "section": "\n3.4 Triple suavizado exponencial (Holt-Winters). Componentes de nivel, tendencia y estacionalidad",
    "text": "3.4 Triple suavizado exponencial (Holt-Winters). Componentes de nivel, tendencia y estacionalidad\nLos métodos anteriores funcionan bien cuando no existe estacionalidad en los datos. En el caso de que también exista estacionalidad y sea multiplicativa, nuestro modelo sería:\n\\[\n\\begin{eqnarray}\nL_t&=&\\alpha(\\frac{Y_t}{S_{t-s}}) + (1-\\alpha)[L_{t-1} + b_{t-1}] \\label{nivelHW} \\\\\nb_t&=&\\beta(L_t - L_{t-1}) + (1-\\beta)b_{t-1}  \\label{trendBH}\\\\\nS_t&=&\\gamma\\frac{Y_t}{L_t} + (1-\\gamma)S_{t-s} \\\\\nF_{t+1} &=& (L_t + b_t )S_{t-s+1} \\label{forecastHW}\n\\end{eqnarray}\n\\tag{3.6}\\]\ndonde \\(s\\) es la duración de la estacionalidad. A este modelo se le conoce como modelo de Holt-Winters (HW). El algoritmo fue desarrollado por CC. Holt en 1957 y su estudiante Peter Winters en 1960. La cuestión sobre si la estacionalidad es aditiva o multiplicativa, volveremos sobre este punto más tarde.\nPara realizar las predicciones el modelo necesita estimar las componentes de nivel, tendencia y estacionalidad. La primera línea de la Ecuación 3.7 representa el nivel (\\(L_t\\)) subyacente de la serie temporal. Este es el nivel después de haber desestacionalizado la serie temporal y filtrado el efecto del error aleatorio o ruido. La siguiente línea representa el cambio de nivel de la serie es decir la tendencia (\\(b_t\\)). El último componente hace referencia a la estacionalidad (\\(S_t\\)) y se conoce como el índice estacional. Por ejemplo, si el índice estacional vale 1.2 significa que se espera que las serie temporal en ese periodo de tiempo sea un 20% superior al nivel promedio. Es importante señalar que, en el método de Holt-Winters, tenemos que indicar cuál es el valor de la estacionalidad \\(s\\) que no hay que confundir con la componente estacional \\(S_t\\). Por lo general, en series temporales económicas, dicho valor de estacionalidad \\(s\\) suele ser anual. Por tanto, si la serie es mensual (\\(s=12\\)), y si la serie es trimestral (\\(s=4\\)).\nPor otro lado los parámetros \\(\\alpha\\), \\(\\beta\\) y \\(\\gamma\\) son constantes y se denominan constantes de suavizado. Para cada componente (ecuación) hay una constante de suavizado que puede variar entre un rango de 0 a 1. Cuanto mayor sea la constante, mayor será el peso aplicado a la última observación y menor el peso aplicado a la última estimación. En otras palabras, el método se adaptará más rápido a cambios en el nivel, tendencia o estacionalidad. Sin embargo, también será más sensible al ruido. Finalmente, la última ecuación del algoritmo en Ecuación 3.7 muestra combinación de los componentes para producir la predicción un paso hacia adelante \\(F_{t+1}\\). Dicha última ecuación muestra como la combinación de la componente de tendencia y nivel es aditiva, mientras que la componente estacional se incluye de forma multiplicativa.\nAl igual que el resto de métodos de suavizado exponencial se pueden utilizar técnicas heurísticas para inicializar el algoritmo. En este caso, se necesitarán \\(2s\\) observaciones para inicializar el método de Holt-Winters, tal que, para la inicialización de la componente de nivel \\(L_t\\):\n\\[\nL_s=\\frac{\\sum_{i=1}^s Y_i}{s}\n\\]\nPara la inialización de la componente de tendencia \\(b_t\\):\n\\[\nb_s=\\frac{1}{s}[\\frac{Y_{s+1}-Y_1}{s}+\\frac{Y_{s+2}-Y_2}{s}+\\frac{Y_{s+3}-Y_3}{s}+\\ldots + \\frac{Y_{s+s}-Y_s}{s}]\n\\] Y, finalmente, para los índices estacionales:\n\\[\nS_1=\\frac{Y_1}{L_s}, S_2=\\frac{Y_2}{L_s},\\ldots,S_s\\frac{Y_s}{L_s}\n\\]\n\n\nResuelve\nSolución con Excel\nSolución con R\n\n\n\nEn la Tabla 3.8 se muestran las ventas trimestrales de un determinado producto. Se pide dibujar las ventas frente al tiempo e identificar los componentes. En caso de que hay indicios de una componente estacional utilice el método de HW para realizar la predicción del trimestre 25. Finalmente, dibuje las predicciones frente a los datos reales para los periodos anteriores. Compara las predicciones con el método SES y de Holt.\n\n\nTabla 3.8: Ejemplo con datos estacionales.\n\nTrimestre\nVentas\n\n\n\n1\n362\n\n\n2\n385\n\n\n3\n432\n\n\n4\n341\n\n\n5\n382\n\n\n6\n409\n\n\n7\n498\n\n\n8\n387\n\n\n9\n473\n\n\n10\n513\n\n\n11\n582\n\n\n12\n474\n\n\n13\n544\n\n\n14\n582\n\n\n15\n681\n\n\n16\n557\n\n\n17\n628\n\n\n18\n707\n\n\n19\n773\n\n\n20\n592\n\n\n21\n627\n\n\n22\n725\n\n\n23\n854\n\n\n24\n661\n\n\n25\n?\n\n\n\n\n\n\nEn la Figura 3.7 se puede apreciar la presencia de las componentes de nivel, tendencia y estacionalidad. Por tanto, un método que puede funcionar bien a la hora de realizar predicciones es el método HW.\n\n\nFigura 3.7: Ventas de un producto con caracter estacional.\n\nPara poder utilizar Ecuación 3.7 se necesita proporcionar el valor de la estacionalidad (\\(s\\)) de la serie temporal. En el caso del ejemplo de la Figura 3.7, el valor de \\(s=4\\), es decir, los valores siguen un patrón cíclico cada 4 observaciones y como los datos son trimestrales, tiene una estacionalidad anual.\nEn la Figura 3.8 se muestra las diferentes estimaciones correspondientes a cada componente, donde los valores de los parámetros optimizados han sido \\(\\alpha=0,82\\), \\(\\beta=0,055\\) y \\(\\gamma=0\\). En la columna Holt-Winter se muestra la predicción resultante.\n\n\nFigura 3.8: Estimación de las componentes de nivel, tendencia y estacionalidad, junto con la predicción en la columna Holt-Winters.\n\nLa Figura 3.9 representa la demanda real junto con las diferentes predicciones realizadas por los métodos de: i) suavizado exponencial simple, ii) doble suavizado exponencial (método de Holt) y iii) el método HW. Se puede comprobar visualmente que es el método de HW el que se aproxima mejor a la demanda real.\n\n\nFigura 3.9: Predicciones realizadas mediante los diferentes tipos de suavizado exponencial.\n\n\n\nSe pide dibujar las ventas frente al tiempo e identificar los componentes. En caso de que hay indicios de una componente estacional utilice el método de HW para realizar la predicción del trimestre 25. Finalmente, dibuje las predicciones frente a los datos reales para los periodos anteriores. Compara las predicciones con el método SES y de Holt.\n\nrm(list = ls())\nlibrary(UComp)\nlibrary(ggplot2)\n\n# Ventas\nventas&lt;-ts(c(362,385,432,341,382,409,498,387,473,513,582,474,544,582,681,557,628,707,773,592,627,725,854,661),start=1,frequency=4)\n# Dibujo los datos\nautoplot(ventas)\n\n\n\n# Selecionando muestra\nx = window(ventas, start=1, end = 23)\n# Horizonte de predicción\nh = 1\n# Estimando y prediciendo con Triple Suavizado Exponencial\nmhw = ETS(x, model=\"MAM\", h = h)\npredhw&lt;-mhw$yFor\nprint(predhw)\n##       Qtr1\n## 7 738.9385\n# Estimando y prediciendo con Doble Suavizado Exponencial\nmholt = ETS(x, model=\"AAN\", h = h)\npredholt&lt;-mholt$yFor\nprint(predholt)\n##       Qtr1\n## 7 771.5507\n# Estimando y prediciendo con Suavizado Exponencial Simple\nmses = ETS(x, model=\"ANN\", h=h)\npredSES&lt;-mses$yFor\nprint(predSES)\n##      Qtr1\n## 7 714.505\n# Podemos dibujar el ajuste \nautoplot(ventas)+autolayer(mholt$comp[,2]) + autolayer(mhw$comp[,2]) + autolayer(mses$comp[,2])"
  },
  {
    "objectID": "02-etsSSOE.html#clasificación-de-pegels",
    "href": "02-etsSSOE.html#clasificación-de-pegels",
    "title": "\n3  Suavizado Exponencial\n",
    "section": "\n3.5 Clasificación de Pegels",
    "text": "3.5 Clasificación de Pegels\nHasta ahora se han analizado las diferentes versiones de suavizado exponencial en función de los componentes que posea la serie temporal (nivel, tendencia, estacionalidad y error). Los métodos analizados son los más comunes, no obstante pueden existir más combinaciones , por ejemplo ¿cómo podríamos realizar la predicción de una serie temporal que posea nivel y estacionalidad combinados de una manera aditiva?\nPara solucionar esta pregunta Pegels (1969) propuso la Tabla 3.9. De esta forma, una vez que se hayan identificado los componentes de la serie temporal (tendencia o estacionalidad) y la forma en que se combinan (multiplicativa o aditiva) se selecciona el cuadrante de la tabla. De dicha tabla se obtienen los valores de \\(P_t\\), \\(Q_t\\), \\(R_t\\) y \\(T_t\\), y se sustituyen en las siguientes ecuaciones:\n\\[\n\\begin{eqnarray}\n    L_t&=&\\alpha P_t + (1-\\alpha)Q_t \\\\\n    b_t&=&\\beta R_t + (1-\\beta)b_{t-1} \\\\\n    S_t&=&\\gamma T_t + (1-\\gamma) S_{t-s}\n\\end{eqnarray}\n\\]\nFinalmente la predicción se obtiene mediante la ecuación \\(F_{t+m}\\) que se obtiene de la Tabla 3.9. Es importante señalar que hasta ahora hemos supuesto que las predicciones las realizamos un paso hacia delante, es decir, se calcula \\(F_{t+1}\\). No obstante, puede ser necesario predicciones con un horizonte temporal mayor, digamos \\(m\\) pasos adelante. Por ello, la constante \\(m\\) nos indica el número de pasos adelante que queremos realizar la predicción.\n\n\nTabla 3.9: Clasificación de Pegels.\n\n\n\n\n\n\n\nTendencia\n\nEstacionalidad\n\n\n\n\n\n(Ninguna)\n(Aditiva)\n(Multiplicativa)\n\n\n\n\\(P_t=Y_t\\)\n\\(P_t=Y_t-S_{t-s}\\)\n\\(P_t=Y_t/S_{t-s}\\)\n\n\n(Ninguna)\n\\(Q_t=L_{t-1}\\)\n\\(Q_t=L_{t-1}\\)\n\\(Q_t=L_{t-1}\\)\n\n\n\n\n\\(T_t=Y_t-L_t\\)\n\\(T_t=Y_t/L_t\\)\n\n\n\n\\(F_{t+m}=L_t\\)\n\\(F_{t+m}=L_t+S_{t+m-s}\\)\n\\(F_{t+m}=L_tS_{t+m-s}\\)\n\n\n————-\n———–\n————\n————–\n\n\n\n\\(P_t=Y_t\\)\n\\(P_t=Y_t-S_{t-s}\\)\n\\(P_t=Y_t/S_{t-s}\\)\n\n\n\n\\(Q_t=L_{t-1}+b_{t-1}\\)\n\\(Q_t=L_{t-1}+b_{t-1}\\)\n\\(Q_t=L_{t-1}+b_{t-1}\\)\n\n\n(Aditiva)\n\\(R_t=L_t-L_{t-1}\\)\n\\(R_t=L_t-L_{t-1}\\)\n\\(R_t=L_t-L_{t-1}\\)\n\n\n\n\n\\(T_t=Y_t-L_t\\)\n\\(T_t=Y_t/L_t\\)\n\n\n\n\\(F_{t+m}=L_t+mb_t\\)\n\\(F_{t+m}=L_t+mb_t+S_{t+m-s}\\)\n\\(F_{t+m}=(L_t+mb_t)S_{t+m-s}\\)\n\n\n——————–\n———————–\n——————————\n——————————-\n\n\n\n\\(P_t=Y_t\\)\n\\(P_t=Y_t-S_{t-s}\\)\n\\(P_t=Y_t/S_{t-s}\\)\n\n\n\n\\(Q_t=L_{t-1}b_{t-1}\\)\n\\(Q_t=L_{t-1}b_{t-1}\\)\n\\(Q_t=L_{t-1}b_{t-1}\\)\n\n\n(Multiplicativa)\n\\(R_t=L_t/L_{t-1}\\)\n\\(R_t=L_t/L_{t-1}\\)\n\\(R_t=L_t/L_{t-1}\\)\n\n\n\n\n\\(T_t=Y_t-L_t\\)\n\\(T_t=Y_t/L_t\\)\n\n\n\n\\(F_{t+m}=L_tb_t^m\\)\n\\(F_{t+m}=L_tb_t^m+S_{t+m-s}\\)\n\\(F_{t+m}=L_tb_t^mS_{t+m-s}\\)\n\n\n\n\n\n\nResuelve\nSolución\n\n\n\nUtilizando la clasificación de Pegels, calcular las ecuaciones de suavizado exponencial necesarias para realizar la predicción de una serie temporal cuyas componentes son nivel y tendencia (de forma aditiva) y estacionalidad de forma multiplicativa, es decir, comprobar que dichas ecuaciones coinciden con las obtenidas para el método HW.\n\n\nUtilizando la clasificación de Pegels, calcular las ecuaciones de suavizado exponencial necesarias para realizar la predicción de una serie temporal cuyas componentes son nivel y tendencia (de forma aditiva) y estacionalidad de forma multiplicativa, es decir, comprobar que dichas ecuaciones coinciden con las obtenidas para el método HW.\nSe puede apreciar que si las componentes son nivel y tendencia de forma aditiva y estacionalidad de forma multiplicativa coincide con el método de Holt-Winters y las ecuaciones son:\n\\[\n\\begin{eqnarray}\n    L_t&=&\\alpha P_t + (1-\\alpha)Q_t \\\\\n    b_t&=&\\beta R_t + (1-\\beta)b_{t-1} \\\\\n    S_t&=&\\gamma T_t + (1-\\gamma) S_{t-s}\n\\end{eqnarray}\n\\]\ndonde \\(P_t\\), \\(Q_t\\), \\(R_t\\), \\(T_t\\) se obtienen de la Tabla 3.9 y son:\n\\[\n\\begin{eqnarray}\nP_t&=&Y_t/S_{t-s} \\\\\nQ_t&=&L_{t-1}+b_{t-1} \\\\\nR_t&=&L_t-L_{t-1} \\\\\nT_t&=&Y_t/L_t\n\\end{eqnarray}\n\\] Finalmente, también de la misma tabla se combinan las componentes para calcular la predicción para un determinado horizonte de predicción \\(m\\), tal que:\n\\[\nF_{t+m}=(L_t+mb_t)S_{t+m-s}\n\\] Por tanto, las ecuaciones del algoritmos serán: \\[\n\\begin{eqnarray}\nL_t&=&\\alpha(\\frac{Y_t}{S_{t-s}}) + (1-\\alpha)[L_{t-1} + b_{t-1}]  \\\\\nb_t&=&\\beta(L_t - L_{t-1}) + (1-\\beta)b_{t-1}  \\\\\nS_t&=&\\gamma\\frac{Y_t}{L_t} + (1-\\gamma)S_{t-s} \\\\\nF_{t+m} &=& (L_t + mb_t )S_{t+m-s}\n\\end{eqnarray}\n\\tag{3.7}\\]"
  },
  {
    "objectID": "02-etsSSOE.html#suavizado-exponencial-en-formato-de-espacio-de-los-estados",
    "href": "02-etsSSOE.html#suavizado-exponencial-en-formato-de-espacio-de-los-estados",
    "title": "\n3  Suavizado Exponencial\n",
    "section": "\n3.6 Suavizado exponencial en formato de espacio de los estados",
    "text": "3.6 Suavizado exponencial en formato de espacio de los estados\nEl uso de los diferentes métodos de suavizado exponencial (SE) analizados previamente ha sido ampliamente utilizados en la industria y academia. No obstante, un hito importante en el uso de estos método fue en el 2002 cuando un trabajo publicado por Hyndman y colaboradores (R. J. Hyndman, A. B. Koehler, R. D. Snyder, S. Grose, A state space framework for automatic forecasting using exponential smoothing methods, International Journal of Forecasting 18 (2002) 439–454) reformulaban los métodos de SE en un marco estadístico como es el del espacio de los estados. Este hito posiblemente represente el avance más importante en temas de SE dado que al disponer de un modelo estadístico se abría la puerta a los métodos de suavizado exponencial a ser tratados como procesos estocásticos y, por tanto, poder utilizar herramientas de estimación de máxima verosimilitud, selección de modelos, cálculo de intervalos de confianza teóricos y otras herramientas. Se recuerda que en las secciones anteriores los métodos de SE sólo proporcionaban las predicciones puntuales (generalmente la media) y aunque éramos capaces de estimar los intervalos de confianza, dichos intervalos se calculan de forma empírica a través de los errores de predicción, concretamente, del RMSE. En el próximo capítulo, se detallará en más profundidad los conceptos asociados a una herramienta muy útil como son los modelos expresados en espacio de los estados.\nLos autores citados previamente, también extendieron la clasificación de Pegels para incluir otras casuísticas como el caso de tendencia amortiguada, (Hyndman, R. J., Koehler, A. B., Ord, J. K., & Snyder, R. D., 2008, Forecasting with exponential smoothing: The state space approach, Springer-Verlag). La Tabla 3.10 resume dicha clasificación. Esta taxonomía consiste en obtener todas las combinaciones posibles entre varias posibilidades de modelos para cada uno de los componentes: Error, tendencia y estacional (ETS, que en inglés hacen referencia a Error, Trend y Seasonal). Error: Multiplicativo (M) o aditivo (A). Tendencia o nivel: Ninguna (N), aditiva (A), aditiva amortiguada (Ad), multiplicativa (M), multiplicativa amortiguada (Md). Los autores del trabajo reconocen que el modelo con tendencia multiplicativa produce malos resultados predictivos. Estacional: Ninguna (N), aditiva (A), multiplicativa (M).\nOtra diferencia importante con respecto a la clasificación de Pegels, es la inclusión de la componente error (ETS). Sin embargo, hay que recordar que esta componente de error sólo afecta a las predicciones de la varianza que se usarán para el cálculo de los intervalos de predicción, pero no afecta a las predicciones puntuales que coinciden con las predicciones de los métodos de SE vistos en la clasificación de Pegels.\n\n\nTabla 3.10: Clasificación de Hyndman y colaboradores. Hay una tabla para error aditivo y otra para error multiplicativo\n\n\n\n\n\n\n\nTendencia\n\nEstacionalidad\n\n\n\n\n\n(N) Ninguna\n(A) Aditiva\n(M) Multiplicativa\n\n\n(N) Ninguna\nN,N\nA,A\nA,M\n\n\n(A) Aditiva\nA,N\nA,A\nA,M\n\n\n(Ad) Aditiva amortiguada\nAd,N\nAd,A\nAd,M\n\n\n(M) Multiplicativa\nM,N\nM,A\nM,M\n\n\n(Md) Multiplicativa amortiguada\nMd,N\nMd,A\nMd,M"
  },
  {
    "objectID": "03-etsSSOEenSS.html#modelos-ssoe-de-espacio-de-los-estados",
    "href": "03-etsSSOEenSS.html#modelos-ssoe-de-espacio-de-los-estados",
    "title": "\n4  Suavizado exponencial en Espacio de los Estados\n",
    "section": "\n4.1 Modelos SSOE de Espacio de los Estados",
    "text": "4.1 Modelos SSOE de Espacio de los Estados\nLa formulación con una sola fuente de ruido del modelo de espacio de los estados que vamos a utilizar en este capítulo se encuentra en la Ecuación 4.1.\n\\[\n\\begin{array}{rl}\n    \\text{Ecuación de observación:} &    y_t = w x_{t-1} + \\epsilon_t \\\\\n    \\text{Ecuación de transición:} & x_{t} = F x_{t-1} + g \\epsilon_{t}\n\\end{array}\n\\tag{4.1}\\]\nDicho sistema está compuesto por dos ecuaciones ligadas entre sí por dos elementos, \\(x_t\\) y \\(\\epsilon\\). La ecuación de transición refleja el comportamiento dinámico del sistema al conectar el vector de estados \\(x_t\\) de dimensión \\(n\\) con su pasado y una perturbación escalar \\(\\epsilon_t\\), que se supone normal con media cero y varianza constante \\(\\sigma^2\\). La ecuación de observación relaciona los datos de salida escalar \\(y_t\\) con los estados y la perturbación. Los demás elementos son las matrices del sistema de dimensiones apropiadas. En particular, \\(F\\) es una matriz cuadrada \\(n\\times n\\), \\(g\\) es la matriz de ganancias \\(n \\times 1\\) y \\(w\\) es un vector \\(1 \\times n\\).\nPara poder encajar los modelos no lineales es necesario generalizar el modelo, ver la Ecuación 4.2.\n\\[\n\\begin{array}{rl}\n       \\text{Ecuación de observación:} & y_t = w(x_{t-1}) + r(x_{t-1})\\epsilon_t \\\\\n        \\text{Ecuación de transición:} & x_{t} = f(x_{t-1}) + g(x_{t-1}) \\epsilon_{t}\n\\end{array}\n\\tag{4.2}\\]\nLa diferencia con el sistema lineal es que en esta ecuación \\(f(x_{t-1})\\), \\(g(x_{t-1})\\), \\(w(x_{t-1})\\) y \\(r(x_{t-1})\\) son funciones no lineales del vector de estados. Evidentemente, si se escogen funciones lineales y \\(r(x_{t-1})=1\\) se obtiene como caso particular la versión lineal en la Ecuación 4.1.\nHay un detalle que en principio puede parece trivial, pero que hace este sistema singular y especialmente fácil de tratar. Se trata de que hay una sola fuente de ruido aleatorio, \\(\\epsilon_t\\). Esto hace que los estados sean observables sin ninguna incertidumbre y, con una sencilla manipulación y conocido el vector de estados inicial \\(x_0\\), se puedan obtener recursivamente dichos estados.\nEn efecto, si en el sistema lineal de la Ecuación 4.1 se despeja la perturbación de la ecuación de observación (\\(\\epsilon_t = y_t - w x_{t-1}\\)) y se sustituye en la ecuación de transición tenemos que,\n\\[\nx_t=F x_{t-1} + g (y_t - w x_{t-1}).\n\\]\nDado un vector de estados inicial conocido \\(x_0\\), se puede calcular recursivamente toda la secuencia de estados.\nEn el modelo no lineal tenemos que \\(\\epsilon_t = [y_t - w(x_{t-1})]/r(x_{t-1})\\) y\n\\[\nx_t=f(x_{t-1}) + g(x_{t-1}) [y_t - w(x_{t-1})]/r(x_{t-1}).\n\\]"
  },
  {
    "objectID": "03-etsSSOEenSS.html#modelos-de-suavizado-exponencial-en-forma-de-espacio-de-los-estados",
    "href": "03-etsSSOEenSS.html#modelos-de-suavizado-exponencial-en-forma-de-espacio-de-los-estados",
    "title": "\n4  Suavizado exponencial en Espacio de los Estados\n",
    "section": "\n4.2 Modelos de suavizado exponencial en forma de Espacio de los Estados",
    "text": "4.2 Modelos de suavizado exponencial en forma de Espacio de los Estados\nEl modelo de suavizado exponencial está formado por combinaciones particulares de los componentes que se consideren apropiados en cada caso. A continuación se consideran los modelos más habituales para cada componente con su forma de espacio de los estados que se encuentran disponibles en el paquete UComp.\nLos componentes que vamos a considerar son el nivel \\(l_t\\), pendiente \\(b_t\\), estacionalidad \\(s_t\\) y ruido \\(\\epsilon_t\\). En general los modelos particulares de los componentes dependen de si se incluyen estacionalidad y de si el ruido es aditivo o multiplicativo.\nLas formas de espacio de los estados de las quince combinaciones posibles con ruido aditivo se encuentran en la Figura 4.1.\n\n\n\n\nFigura 4.1: Forma de espacio de los estados de modelos de suavizado exponencial con ruido aditivo.\n\n\n\nTodos los modelos con algún componente multiplicativo tienen necesariamente una representación en forma de espacio de estados no lineales de acuerdo con la Ecuación 4.2, el resto son lineales como en la Ecuación 4.1. La Figura 4.2 muestra las matrices del sistema de espacio de los estados lineal o no lineal con la que se puede representar cada uno de los que aparecen en la Figura 4.1. Por razones de espacio, los modelos no lineales con estacionalidad multiplicativa se muestran en la Figura 4.5. En todos ellos se ha utilizado la matriz de transición \\(\\Phi\\) del componente estacional aditivo, cuya definición es la siguiente\n\\[\n\\Phi=\\left[\n\\begin{array}{ccccc}\n   0 & 0 & \\dots & 0 & 1\\\\\n   1 & 0 & \\dots & 0 & 0\\\\\n   0 & 1 & \\dots & 0 & 0\\\\\n   \\vdots & \\vdots & \\ddots & \\vdots & \\vdots\\\\\n   0 & 0 & \\dots & 1 & 0\n\\end{array}\n\\right].\n\\]\n\n\n\n\nFigura 4.2: Matrices del sistema de cada uno de los modelos con ruido aditivo.\n\n\n\nEn el caso de los modelos con ruido multiplicativo todos son no lineales y se muestran en la Figura 4.3.\n\n\n\n\nFigura 4.3: Forma de espacio de los estados de modelos de suavizado exponencial con ruido multiplicativo\n\n\n\nLa Figura 4.4 muestra las matrices del sistema no lineal de espacio de los estados. Los modelos con estacionalidad multiplicativa se muestran en la Figura 4.5.\n\n\n\n\nFigura 4.4: Forma de espacio de los estados de modelos de suavizado exponencial con ruido multiplicativo\n\n\n\nFinalmente, todos los modelos con estacionalidad multiplicativa se muestran en la Figura 4.5.\n\n\n\n\nFigura 4.5: Forma de espacio de los estados de modelos de suavizado exponencial con ruido multiplicativo\n\n\n\nEl lector puede comprobar que efectivamente las matrices del sistema de las tablas anteriores reproducen los modelos que efectivamente se quieren representar. A continuación se proponen dos ejemplos con sus soluciones.\n\n\nResuelve\nSolución\n\n\n\nComprueba que efectivamente se obtiene el modelo ETS(A,A,A) utilizando las matrices del sistema de la Figura 4.2.\n\n\nEl sistema es lineal, por lo que se podrá escribir como\n\\[\n\\begin{array}{l}\n    y_t =  \\left[\\begin{array}{cccccc} 1 & 1 & 0 & 0 & 0 & 1\\end{array} \\right]\n          x_{t-1} + \\epsilon_t \\\\\n    x_t=\\left[\\begin{array}{c} l_t \\\\ b_t \\\\ s_t \\\\ s_{t-1} \\\\ s_{t-2} \\\\ s_{t-3}\\end{array} \\right] =\n       \\left[\\begin{array}{cccccc} 1 & 1 & 0 & 0 & 0 & 0 \\\\\n                                   0 & 1 & 0 & 0 & 0 & 0 \\\\\n                                   0 & 0 & 0 & 0 & 0 & 1 \\\\\n                                   0 & 0 & 1 & 0 & 0 & 0 \\\\\n                                   0 & 0 & 0 & 1 & 0 & 0 \\\\\n                                   0 & 0 & 0 & 0 & 1 & 0 \\\\\n       \\end{array} \\right]\n       \\left[\\begin{array}{c} l_{t-1} \\\\ b_{t-1} \\\\ s_{t-1} \\\\ s_{t-2} \\\\ s_{t-3} \\\\ s_{t-4}\\end{array} \\right] +\n       \\left[\\begin{array}{c} \\alpha \\\\ \\beta \\\\ \\gamma \\\\ 0 \\\\ 0 \\\\ 0\\end{array} \\right]\n       \\epsilon_t\n\\end{array}.\n\\]\nOperando tenemos las ecuaciones de la Figura 4.1:\n\\[\n\\begin{array}{l}\ny_t=l_{t-1}+b_{t-1}+s_{t-4} \\\\\nl_t=l_{t-1}+b_{t-1}+\\alpha \\epsilon_t \\\\\nb_t=b_{t-1}+\\beta \\epsilon_t \\\\\ns_t=s_{t-4}+\\gamma \\epsilon_t\n\\end{array}\n\\]\n\n\n\n\n\n\nResuelve\nSolución\n\n\n\nComprueba que efectivamente se obtiene el modelo ETS(M,M,M) utilizando las matrices del sistema de la Figura 4.5.\n\n\nEl sistema es no lineal, por lo que tendremos que utilizar el sistema de la Ecuación 4.1.\n\\[\n\\begin{array}{l}\ny_t=l_{t-1}b_{t-1}s_{t-4}+l_{t-1}b_{t-1}s_{t-4}\\epsilon_t \\\\\nx_t=\\left[\\begin{array}{c} l_t \\\\ b_t \\\\ s_t \\\\ s_{t-1} \\\\ s_{t-2} \\\\ s_{t-3}\\end{array} \\right] = \\left[\\begin{array}{cccccc} b_{t-1} & 0 & 0 & 0 & 0 & 0 \\\\\n                                   0 & 1 & 0 & 0 & 0 & 0 \\\\\n                                   0 & 0 & 0 & 0 & 0 & 1 \\\\\n                                   0 & 0 & 1 & 0 & 0 & 0 \\\\\n                                   0 & 0 & 0 & 1 & 0 & 0 \\\\\n                                   0 & 0 & 0 & 0 & 1 & 0 \\\\\n       \\end{array} \\right]\n       \\left[\\begin{array}{c} l_{t-1} \\\\ b_{t-1} \\\\ s_{t-1} \\\\ s_{t-2} \\\\ s_{t-3} \\\\ s_{t-4}\\end{array} \\right] +\n\\left[\\begin{array}{c} \\alpha l_{t-1}b_{t-1} \\\\ \\beta b_{t-1} \\\\ \\gamma s_{t-4} \\\\ 0 \\\\ 0 \\\\ 0\\end{array} \\right] \\epsilon_t\n\\end{array}\n\\]\nOperando tenemos las ecuaciones de la Figura 4.5:\n\\[\n\\begin{array}{l}\ny_t=l_{t-1}b_{t-1}s_{t-4}+l_{t-1}b_{t-1}s_{t-4}\\epsilon_t=l_{t-1}b_{t-1}s_{t-4}(1+\\epsilon_t) \\\\\nl_t=l_{t-1}b_{t-1}+\\alpha l_{t-1}b_{t-1} \\epsilon_t = l_{t-1}b_{t-1}(1+\\alpha \\epsilon_t)\\\\\nb_t=b_{t-1}+\\beta b_{t-1}\\epsilon_t = b_{t-1}(1+\\beta \\epsilon_t)\\\\\ns_t=s_{t-4}+\\gamma s_{t-4} \\epsilon_t = s_{t-4}(1+\\gamma \\epsilon_t)\n\\end{array}\n\\]\n\n\n\nAunque se pueden usar las \\(30\\) combinaciones, los modelos con tendencia multiplicativa tienden a evitarse, por las dificultades de estimación y la escasa eficacia en la predicción.\nLos parámetros no pueden tomar cualquier valor, sino que deben estar restringidos de forma que \\(0&lt;\\alpha&lt;1\\), \\(0&lt;\\beta&lt;\\alpha\\), \\(0&lt;\\gamma&lt;1-\\alpha\\), \\(0&lt;\\phi&lt;1\\), \\(0&lt;\\sigma^2&lt;\\infty\\). Algunas de estas restricciones no parecen lógicas porque provienen de la formulación explícita de los modelos como suavizados exponenciales. Puede haber otras restricciones basadas en regiones de admisibilidad de los parámetros (ver detalles en Hyndman et al., 2008)."
  },
  {
    "objectID": "03-etsSSOEenSS.html#estimación",
    "href": "03-etsSSOEenSS.html#estimación",
    "title": "\n4  Suavizado exponencial en Espacio de los Estados\n",
    "section": "\n4.3 Estimación",
    "text": "4.3 Estimación\n\n4.3.1 El principio de Máxima Verosimilitud\nCuando disponemos de una función de distribución dada, por ejemplo una gaussiana con media y varianzas conocidas (\\(\\mu\\) y \\(\\sigma^2\\), respectivamente), podemos obtener muestras aleatorias de la misma. Lo hemos hecho varias veces a lo largo de los capítulos anteriores. Asumiendo gaussianidad, la función se podría escribir como sigue\n\\[\np(y_t|\\mu, \\sigma^2) = (2\\pi \\sigma^2)^{-1/2} \\exp \\left[ -\\frac{(y_t-\\mu)^2}{2\\sigma^2} \\right].\n\\]\nEl lado izquierdo de la igualdad habría que leerlo como la función de densidad de la variable aleatoria \\(y_t\\) condicionada, es decir, dados unos valores fijos conocidos, de los parámetros \\(\\mu\\) y \\(\\sigma^2\\) de los que depende. De esta función se podrían extraer tantas muestras cuantas se desee.\nEl problema de estimación es justo el contrario, tenemos una muestra concreta conocida \\(y_t\\) que suponemos que se ha extraído de una distribución con unos parámetros desconocidos \\(\\mu\\) y \\(\\sigma^2\\). Es decir, el rol de la variable y las incógnitas se ha intercambiado. Ahora lo conocido es la muestra y lo desconocido son los parámetros. Esto se podría escribir como\n\\[\nL(\\mu, \\sigma^2|y_t) = (2\\pi \\sigma^2)^{-1/2} \\exp \\left[ -\\frac{(y_t-\\mu)^2}{2\\sigma^2} \\right].\n\\]\nEsta función de verosimilitud, que en realidad es la misma que la de densidad anterior, nos permite calcular probabilidades de combinaciones de los parámetros condicionados a la muestra de la que disponemos. Si buscamos los valores de \\(\\mu\\) y \\(\\sigma^2\\) que maximizan esta función estaríamos estimando esos valores que hace más verosímil, más probable, la aparición de la muestra concreta de la que disponemos. Esta es la estimación por máxima verosimilitud (MV).\nLas dos funciones anteriores se han escrito para una sola muestra escalar \\(y_t\\). En el caso de disponer de \\(T\\) muestras independientes con la misma media y varianza, la verosimilitud conjunta de todas ellas (de un vector \\(Y_T=\\{y_1,y_2,\\dots,y_T\\}\\)) será simplemente el producto de las individuales,\n\\[\nL(\\mu, \\sigma^2|Y_T) = (2\\pi \\sigma^2)^{-T/2} \\exp \\left[ -\\sum_{t=1}^T\\frac{(y_t-\\mu)^2}{2\\sigma^2} \\right].\n\\]\nEn el caso de que las muestras \\(y_t\\) no sean independientes, que será el caso más habitual, partiendo de la fórmula de la probabilidad conjunta que afirma que \\(p(y_1,y_2)=p(y_1)p(y_2|y_1)\\), podemos ver que para tres variables aleatorias tenemos\n\\[\np(y_1,y_2,y_3)=p(y_1,y_2)p(y_3|y_1,y_2)=p(y_1)p(y_2|y_1)p(y_3|y_1,y_2).\n\\]\nLa generalización a \\(T\\) variables aleatorias conduce a que la probabilidad de toda la muestra se puede escribir como\n\\[\np(y_1,y_2,\\dots,y_T)=p(Y_T)=\\prod_{t=1}^Tp(y_t|Y_{t-1}),\n\\tag{4.3}\\]\ndonde cada término del producto es la probabilidad de cada observación condicionada a la información disponible hasta la observación anterior.\n\n4.3.2 Estimación MV de modelos de espacio de los estados\nTodos los modelos vistos en este capítulo dependen de una serie de parámetros, que en general, serán desconocidos cuando se estén analizando series temporales particulares. Estos parámetros son \\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\), \\(\\phi\\) y \\(\\sigma^2\\). Además, debemos recordar que el valor de los estados depende crucialmente del vector de estados inicial de los mismos, que se han considerado conocidos hasta el momento, pero que en casos reales no lo son. En muestras largas, el efecto de las condiciones iniciales tiende a diluirse, pero no sucede lo mismo con muestras cortas.\n\nTodo lo dicho hasta el momento se ha hecho con el fin de poder establecer el tratamiento estadístico de los modelos sobre la base de la forma general de espacio de los estados, en lugar de tener que ir particularizando para cada modelo concreto. Así el resto del capítulo se referirá únicamente a la forma general de la Ecuación 4.1 y la Ecuación 4.2, entendiendo que los resultados son válidos para todos los casos particulares.\n\n\n\n\n\n\n\n\n\nDada la forma del modelo general lineal, que precisamente estipula una relación entre la cada observación y el pasado inmediato, tenemos que la distribución de cada \\(y_t\\) condicionado a su pasado es normal con media \\(wx_{t-1}\\) y varianza \\(\\sigma^2\\). Si añadimos el supuesto de normalidad tenemos que\n\\[\n    p(y_t|Y_{t-1})= (2\\pi \\sigma^2)^{-1/2} \\exp \\left[ -\\frac{(y_t-wx_{t-1})^2}{2\\sigma^2} \\right]=\n     (2\\pi \\sigma^2)^{-1/2} \\exp \\left( -\\frac{\\epsilon_t^2}{2 \\sigma^2} \\right)\n\\] La Ecuación 4.3 nos permite escribir la función de probabilidad de una forma sencilla,\n\\[\np(Y_t)= (2\\pi \\sigma^2)^{-T/2} \\prod_{t=1}^T \\exp \\left(  - \\frac{\\epsilon_t^2}{2\\sigma^2} \\right)\n\\tag{4.4}\\]\nEn el caso no lineal, tenemos que la distribución de cada \\(y_t\\) es normal con media \\(w(x_{t-1})\\) y varianza \\(r(x_{t-1})^2\\sigma^2\\), por tanto la función de densidad de una observación es\n\\[\n    p(y_t|Y_{t-1})=  [2\\pi r(x_{t-1})^2 \\sigma^2]^{-1/2} \\exp \\left[ \\frac{[y_t-w(x_{t-1})]^2}{2r(x_{t-1})^2\\sigma^2} \\right]= \\frac{p(\\epsilon_t)}{|r(x_{t-1})|},\n\\] donde se toma el valor positivo de las raíz cuadrada de \\(r(x_{t-1})^2\\) mediante el valor absoluto, puesto que la medida debe ser una probabilidad. La probabilidad de toda la muestra es,\n\\[\np(Y_t)= (2\\pi \\sigma^2)^{-T/2} \\left|  \\prod_{t=1}^T r(x_{t-1}) \\right|^{-1} \\prod_{t=1}^T \\exp \\left(  - \\frac{\\epsilon_t^2}{2\\sigma^2} \\right)\n\\]\nEsta es una expresión general que es válida también para modelos lineales en los que \\(r(x_{t-1})=1\\). Es habitual utilizar el logaritmo de esta expresión por comodidad. Además, para convertirla en la función de verosimilitud, se debe expresar en función de los parámetros de los que depende el modelo \\(\\theta=[\\begin{array}{ccc} \\alpha & \\beta & \\gamma & \\phi & \\sigma^2 \\end{array}]'\\), y condicionar a la muestra utilizada, es decir,\n\\[\n\\log[L(\\theta,x_0)|Y_T]= {-T/2} \\log (2\\pi \\sigma^2) -\\sum_{t=1}^T \\log\\left| r(x_{t-1}) \\right| -\\frac{1}{2} \\sum_{t=1}^T \\epsilon_t^2/\\sigma^2\n\\]\nLa forma de evaluar esta función para cualquier vector de parámetros y condiciones iniciales es tener en cuenta que tanto \\(r(x_{t-1})\\) como \\(\\epsilon_t\\) dependen de los mismos. Es decir, partiendo de un modelo concreto, un vector de estados iniciales conocido y valores de los parámetros fijos, podemos calcular recursivamente los valores de \\(\\epsilon_t\\), recordando que \\(\\epsilon_t = [y_t - w(x_{t-1})]/r(x_{t-1})\\) y \\(x_t=f(x_{t-1}) + g(x_{t-1}) [y_t - w(x_{t-1})]/r(x_{t-1})\\).\nEn la función anterior \\(\\sigma^2\\) en realidad juega el papel de factor de escala que a menudo se aísla de la función de verosimilitud. La forma de la función es tal que ese parámetro se puede concentrar fuera de la función. El proceso consiste en calcular el óptimo de la función derivando e igualando a cero solo para ese parámetro y sustituyendo de vuelta ese valor en la función. La derivada respecto a la varianza es\n\\[\n\\frac{\\partial log(L)}{\\partial \\sigma^2}=-\\frac{T}{2\\sigma^2}+\\frac{\\sum \\epsilon_t^2}{2\\sigma^4},\n\\]\nque, igualada a cero da un valor óptimo de la varianza de \\(\\hat{\\sigma}^2=\\sum \\epsilon_t^2/T\\). Es decir, la varianza muestral. Cuando se introduce este valor en la función de verosimilitud, esta se simplifica\n\\[\n\\begin{array}{rl}\n\\log[L(\\theta,x_0)|Y_T]= & {-T/2} \\log (2\\pi) -T/2 \\log(\\sum \\epsilon_t^2/T) -\\sum_{t=1}^T \\log\\left| r(x_{t-1}) \\right| -T/2 =\\\\\n=& c -T/2 \\log(\\sum \\epsilon_t^2) -\\sum_{t=1}^T \\log\\left| r(x_{t-1}) \\right|\n\\end{array}\n\\]\nMaximizar esta función es equivalente a minimizar\n\\[\nf(\\theta,x_0|Y_T) = T \\log(\\sum \\epsilon_t^2) + 2 \\sum_{t=1}^T \\log\\left| r(x_{t-1}) \\right|,\n\\]\nque será la función objetivo en la práctica. Para los modelos lineales en los que \\(r(x_{t-1})=1\\), la expresión anterior equivale al estimador mínimo cuadrático.\nLa función objetivo depende del vector de estados inicial \\(x_0\\) que se ha supuesto conocido, pero que no es conocido en la práctica. Existen al menos dos soluciones para este problema, dependiendo del supuesto que se haga sobre el vector de estados inicial:\n\nSi se consideran valores fijos, se puede añadir al vector de parámetros a estimar y minimizar la función objetivo respecto a las condiciones iniciales también. Esta es la aproximación utilizada en el paquete UComp.\nSi se consideran variables aleatorias con una determinada distribución sería necesario utilizar el filtro de Kalman con inicialización exacta (difusa en el caso de modelos no estacionarios) o utilizar un filtro de Información.\n\nEl problema de estimación, entonces, consiste en encontrar los valores de los parámetros (incluyendo las condiciones iniciales) que minimiza dicha función con algún algoritmo de optimización numérico. Tanto este como otros detalles más técnicos quedan fuera del alcance de este libro y el lector interesado puede acudir a la bibliografía.\n\n\nResuelve\nSolución\nR\nPython\nMATLAB/Octave\n\n\n\nCarga las librerías UComp (en R carga además ggplot2). Considera los datos de pasajeros de avión en logaritmos hasta febrero de 2020 incluido (airpas). Estima un modelo ETS(A,A,A) con la función ETS. Muestra los parámetros del modelo y realiza un diagnóstico de residuos (tests del objeto modelo). Representa también los componentes estimados. ¿Te parece un modelo adecuado?\n\n\n\n\n -------------------------------------------------------------\n  Model: ETS(A,A,A)\n  Box-Cox lambda: 1.00\n  Q-Newton: Function convergence.\n -------------------------------------------------------------\n                   Param        S.E.          |T|     |Grad|\n -------------------------------------------------------------\n      Alpha:       0.3367     0.0263      12.7829  1.960e-08\n       Beta:       0.0090     0.0036       2.4722  3.111e-09\n      Gamma:       0.3821     0.0288      13.2834  1.394e-08\n -------------------------------------------------------------\n   AIC:       0.2950   BIC:       0.4174   AICc:       0.2966\n            Log-Likelihood:     -73.5637\n -------------------------------------------------------------\n    Summary statistics:\n -------------------------------------------------------------\n         Missing data:      \n         Q( 1):      26.4152         Q( 4):      33.9828\n         Q( 8):      58.7637         Q(12):      63.1103\n   Bera-Jarque:    1366.6633       P-value:       0.0000\n       H( 206):       0.9863       P-value:       0.4606\n   Outliers (&gt;2.7 ES):    10\n         Q( 1):      60.4878         Q( 4):      85.1360\n         Q( 8):     104.5970         Q(12):     133.6029\n   Bera-Jarque:       8.0994       P-value:       0.0174\n       H( 202):       0.8119       P-value:       0.0698\n -------------------------------------------------------------\n\n\n\n\n\nSummary statistics:\n==================\n                        Serie 1\nData points:          622.00000\nMissing:               24.00000\nMinimum:               -0.17581\n1st quartile:          -0.02550\nMean:                  -0.00162\nP(Mean = 0):            0.38654\nMedian:                 0.00070\n3rd quartile:           0.02477\nMaximum:                0.36119\nInterquartile range:    0.05027\nRange:                  0.53700\nSatandard deviation:    0.04578\nVariance:               0.00210\nSkewness:               0.41718\nKurtosis:               7.29895\nAutocorrelation tests:\n=====================\n     SACF sa      LB p.val  SPACF sp\n1   0.202  +  24.484     0  0.202  +\n2   0.092  +  29.574     0  0.053  .\n3   0.020  .  29.815     0 -0.009  .\n4   0.061  .  32.036     0  0.056  .\n5  -0.123  -  41.121     0 -0.152  -\n6  -0.119  -  49.693     0 -0.080  .\n7  -0.080  .  53.531     0 -0.025  .\n8  -0.062  .  55.873     0 -0.036  .\n9  -0.030  .  56.425     0  0.014  .\n10  0.037  .  57.239     0  0.048  .\n11  0.041  .  58.281     0  0.011  .\n12  0.055  .  60.126     0  0.028  .\n13  0.249  +  98.303     0  0.231  +\n14  0.113  + 106.098     0  0.004  .\n15  0.041  . 107.139     0 -0.012  .\n16 -0.004  . 107.149     0 -0.010  .\n17 -0.030  . 107.697     0 -0.064  .\n18 -0.117  - 116.155     0 -0.057  .\n19 -0.199  - 140.723     0 -0.124  -\n20 -0.112  - 148.462     0 -0.023  .\n21 -0.009  . 148.513     0  0.065  .\n22 -0.016  . 148.670     0 -0.001  .\n23  0.043  . 149.798     0  0.037  .\n24  0.008  . 149.837     0 -0.055  .\n25  0.162  + 166.322     0  0.130  +\nGaussianity tests:\n=================\n\n    Shapiro-Wilk normality test\n\ndata:  x\nW = 0.94071, p-value = 1.057e-14\n\nRatio of variance tests:\n=======================\n Portion_of_data F_statistic p.value\n         0.33333      0.9808  0.8918\n\n\n\n\n\nEl modelo NO es apropiado porque hay autocorrelación, la distribución de las innovaciones no son normales, aunque seguramente no hay problemas de heterocedasticidad. Es decir, la transformación logarítmica parece adecuada.\n\n\n\n# Cargando librería\nlibrary(ggplot2)\nlibrary(UComp)\n# Seleccionando muestra\nx = window(log(airpas), end = c(2020, 2))\n# Estimando modelo ETS\nm = ETS(x, model=\"AAA\")\n# Mostrando modelo\nprint(m)\n# Mostrando componentes\nplot(m)\n# Tests de innovaciones\ntests(m)\n\n\n\n\n# Cargando librerías\nfrom UComp import *\n# Seleccionando muestra hasta febrero 2020\nx = window(np.log(airpas), end='2020-02-29')\n# Estimando modelo ETS\nm = ETS(x, model=\"AAA\")\n# Mostrando modelo\nm.validate()\n# Mostrando componentes\nm.plot()\n# Tests de innovaciones\ntests(m)\n\n\n\n\n% Cargando datos\nload data\n% Seleccionando muestra hasta febrero 2020\nx = log(airpas(1 : 614));\n% Estimando modelo ETS\nm = ETS(x, 12, model='AAA');\n% Mostrando modelo\nETSvalidate(m);\n% Mostrando componentes\nstackedplot(m.comp)\n% Tests de innovaciones\ntoolTEST(m.v);"
  },
  {
    "objectID": "03-etsSSOEenSS.html#selección-de-modelos",
    "href": "03-etsSSOEenSS.html#selección-de-modelos",
    "title": "\n4  Suavizado exponencial en Espacio de los Estados\n",
    "section": "\n4.4 Selección de modelos",
    "text": "4.4 Selección de modelos\nLa selección de modelos implica buscar el mejor modelo dentro de la familia de los modelos de suavizado exponencial, es decir, elegir el más apropiado de un total de \\(30\\) posibilidades diferentes teniendo en cuenta todas las posibles combinaciones de componentes.\nUna forma de hacerlo, no la única como veremos más adelante, es estimando todas las posibilidades y eligiendo aquel modelo que minimice un criterio de información previamente seleccionado. Los criterios de información son funciones que buscan el mayor ajuste de la muestra penalizando por el número de parámetros. La lógica es aplastante, puesto que, por regla general, siempre que se añadan parámetros a un modelo el ajuste mejora aunque los parámetros no sean realmente relevantes. Con el criterio de información nos aseguramos de que la mejora en términos de verosimilitud al añadir un parámetro es realmente relevante.\nExisten varios criterios de información (\\(p\\) es el número de parámetros del modelo):\n\n\nCriterio de información de Akaike: \\(AIC=-2log[L(\\theta,x_0)|Y_T]+2p\\).\n\nCriterio de información de Akaike corregido: \\(AICc=AIC+2p(p+1)/(T-p-1)\\).\n\nCriterio de información bayesiano: \\(BIC=-2log[L(\\theta,x_0)|Y_T]+p\\log(T)\\).\n\nDado que los criterios de información son función de la función de verosimilitud en el óptimo con signo menos, el mejor modelo será aquel que minimice el criterio en cuestión. El más utilizado en este contexto es \\(AICc\\).\n\n\nResuelve\nSolución\nR\nPython\nMATLAB/Octave\n\n\n\n\nConsidera los datos de los pasajeros de avión hasta febrero de 2020 en logaritmos. Selecciona el mejor modelo con la función ETS. Puedes ver los resultados de todos los modelos a medida que se van estimando si eliges la entrada verbose como TRUE. Muestra los parámetros del modelo, representa los componentes y realiza un diagnóstico de residuos. ¿Te parece un modelo adecuado? ¿Se obtiene el mismo modelo utilizando cualquier criterio de información?\n\n\n\n\n--------------------------------------------------------\n    Model            AIC           BIC          AICc\n--------------------------------------------------------\n   (A,N,N):        2.1785        2.1929        2.1785\n   (A,N,A):        0.3654        0.4734        0.3654\n   (A,A,N):        2.1830        2.2118        2.1830\n   (A,A,A):        0.2950        0.4174        0.2966\n  (A,Ad,N):        2.1841        2.2201        2.1841\n  (A,Ad,A):        0.5394        0.6689        0.5410\n   (M,N,N):        2.2149        2.2293        2.2149\n   (M,N,A):        0.4167        0.5247        0.4167\n   (M,N,M):        0.4457        0.5536        0.4457\n   (M,A,N):        2.2172        2.2459        2.2172\n   (M,A,A):        0.3334        0.4558        0.3350\n   (M,A,M):        0.5474        0.6698        0.5490\n  (M,Ad,N):        2.2181        2.2541        2.2181\n  (M,Ad,A):        0.3607        0.4903        0.3624\n  (M,Ad,M):        0.5660        0.6955        0.5676\n--------------------------------------------------------\n  Identification time:    0.30084 seconds\n--------------------------------------------------------\n -------------------------------------------------------------\n  Model: ETS(A,A,A)\n  Box-Cox lambda: 1.00\n  Q-Newton: Function convergence.\n -------------------------------------------------------------\n                   Param        S.E.          |T|     |Grad|\n -------------------------------------------------------------\n      Alpha:       0.3367     0.0263      12.7829  1.960e-08\n       Beta:       0.0090     0.0036       2.4722  3.111e-09\n      Gamma:       0.3821     0.0288      13.2834  1.394e-08\n -------------------------------------------------------------\n   AIC:       0.2950   BIC:       0.4174   AICc:       0.2966\n            Log-Likelihood:     -73.5637\n -------------------------------------------------------------\n    Summary statistics:\n -------------------------------------------------------------\n         Missing data:      \n         Q( 1):      26.4152         Q( 4):      33.9828\n         Q( 8):      58.7637         Q(12):      63.1103\n   Bera-Jarque:    1366.6633       P-value:       0.0000\n       H( 206):       0.9863       P-value:       0.4606\n   Outliers (&gt;2.7 ES):    10\n         Q( 1):      60.4878         Q( 4):      85.1360\n         Q( 8):     104.5970         Q(12):     133.6029\n   Bera-Jarque:       8.0994       P-value:       0.0174\n       H( 202):       0.8119       P-value:       0.0698\n -------------------------------------------------------------\n\n\nEl modelo seleccionado es ETS(A,A,A), que de hecho ya se había estimado anteriormente y vimos que no era adecuado. Los tres criterios de información seleccionan el mismo modelo.\n\n\n\nx = window(log(airpas), end = c(2020, 2))\nm = ETS(x, verbose=TRUE)\n\n\n\n\nx = window(np.log(airpas), end = '2020-02-29')\nm = ETS(x, verbose=True)\n\n\n\n\nx = log(airpas(1 : 614));\nm = ETS(x, 12, verbose=true);"
  },
  {
    "objectID": "03-etsSSOEenSS.html#predicción",
    "href": "03-etsSSOEenSS.html#predicción",
    "title": "\n4  Suavizado exponencial en Espacio de los Estados\n",
    "section": "\n4.5 Predicción",
    "text": "4.5 Predicción\nLa predicción consiste en estimar la distribución de los estados futuros y de la variable salida en función de la historia pasada. Es decir, si queremos obtener la distribución de la variable aleatoria \\(x_{T+h|T}\\) (que equivale a decir que queremos predecir el vector de estados \\(x_t\\), \\(h\\) periodos hacia adelante desde un origen \\(T\\) que suele ser el final de la muestra), tendremos que plantear un predictor (\\(\\hat{f}_{T+h}\\)) y una función objetivo para poder hablar de predicción óptima.\nLo interesante de este planteamiento es que la función de pérdida u objetivo puede ser simétrica, es decir, los errores de predicción por exceso se penalizan igual que por defecto, pero puede ser también asimétrica, de forma que los errores por defecto, por ejemplo, tengan una penalización mayor.\nLo más habitual es utilizar una función de pérdida simétrica y en particular el error cuadrático medio, es decir,\n\\[\nL(\\hat{e}_{T+h|T})=\\text{E} (\\hat{f}_{T+h}-x_{T+h})^2\n\\]\nLa minimización conduce a derivar respecto al predictor e igualar a cero,\n\\[\n\\frac{\\partial L}{\\partial \\hat{f}_{T+h}}=0=2\\hat{f}_{T+h}-2\\text{E}(x_{T+h}|T)\n\\]\nDe aquí se obtiene que \\(\\hat{f}_{T+h}=\\text{E}(x_{T+h}|T)\\). La predicción óptima con esta función objetivo es la media de la variable aleatoria en el futuro, condicionada a la información presente.\nDada la ecuación de transición, calcular la esperanza condicionada es muy sencillo. Considerando \\(l=1\\),\n\\[\n\\text{E}(x_{T+1}|T)=F\\text{E}(x_{T}|T)+g\\text{E}(\\epsilon_{T+1}|T)=Fx_T\n\\]\nPor tanto, la predicción óptima será \\(\\hat{x}_{T+1|T}=Fx_{T}\\), que equivale simplemente a aplicar la dinámica de la ecuación prescindiendo de los ruidos.\nDe igual forma, la predicción óptima para la variable salida del modelo será,\n\\[\n\\text{E}(y_{T+1}|T)=\\hat{y}_{T+1|T}=w\\text{E}(x_{T}|T)+\\text{E}(\\epsilon_{T+1}|T)=wx_T\n\\]\nLas predicciones para horizontes más largos se obtendrían aplicando recursivamente estas ecuaciones, como veremos más tarde.\nCon todos estos elementos podemos plantear tres tipos de predicción:\n\n\nPredicción puntual: consiste en estimar algún momento de posición de la distribución futura, la media por lo general. A menudo interesa estimar igualmente intervalos de predicción o confianza, por lo que será necesario también el error cuadrático medio de la predicción.\n\nPredicción por intervalos de confianza o predicción: consiste en estimar alguna medida de incertidumbre en torno a la predicción puntual. Dicha incertidumbre estará normalmente estimada por la varianza de las predicciones. Los intervalos de confianza se podrán construir asumiendo alguna distribución y un determinado nivel de confianza. Los intervalos también se pueden construir por simulación (como en el siguiente tipo de predicción) en el que solo se elige el nivel de confianza, y se utiliza la distribución empírica de los errores.\n\nPredicción de la distribución completa: es computacionalmente mucho menos eficiente al tener que hacerse por simulación, pero añade un punto de sumo interés que consiste en que se puede utilizar la distribución empírica de las predicciones, consiguiendo unos intervalos de predicción mucho más adaptados a la muestra empírica de la que se dispone.\n\n\n4.5.1 Predicción puntual\nComo ya hemos visto, la predicción puntual óptima consiste en estimar la media condicionada de los estados futuros. En el sistema lineal es relativamente fácil de estimar. Consideremos que hemos estimado un modelo con una muestra de tamaño \\(T\\), la predicción un periodo hacia adelante consiste simplemente en calcular la esperanza condicionada a la información contenida en muestra en el sistema lineal, es decir,\n\\[\n\\hat{x}_{T+1|T}=E(x_{T+1}|T)=E[(Fx_T+g \\epsilon_{T+1})|T]=Fx_T,\n\\]\ndonde se ha utilizado el hecho de que la predicción de \\(\\epsilon_T\\) es su media incondicionada, es decir, cero. Además se utilizan las estimaciones de los parámetros con toda la muestra. La predicción con un horizonte mayor se obtiene de forma recursiva, es decir,\n\\[\n\\hat{x}_{T+2|T}=E(x_{T+2}|T)=E[(Fx_{T+1}+g \\epsilon_{T+2})|T]=F\\hat{x}_{T+1|T}=FFx_T.\n\\]\nEn general tendremos que la predicción \\(h\\) periodos hacia adelante será \\(\\hat{x}_{T+h|T}=F^hx_T\\), donde \\(F^h\\) es el producto de de la matriz \\(F\\), \\(h\\) veces por sí misma. Utilizando este resultado, la predicción para la variable de salida será \\(\\hat{y}_{T+h|T}=E(y_{T+h}|T)=wF^{h-1}x_T\\).\n\n4.5.2 Varianza de las predicciones\nUn resultado útil para calcular las varianzas de las predicciones es sustituir repetidamente la ecuación de estados en sí misma y en la ecuación de observación para obtener fórmulas recursivas generales de los estados y la salida. Por ejemplo, para horizonte \\(2\\) tenemos\n\\[\n\\begin{array}{c}\nx_{T+2}=F x_{T+1} + g \\epsilon_{T+2}= F^2 x_T+ F g \\epsilon_{T+1}+g \\epsilon_{T+2} \\\\\ny_{T+2}=wx_{T+1}+\\epsilon_{T+2}=wFx_T+wg\\epsilon_{T+1}+\\epsilon_{T+2}\n\\end{array}\n\\]\nPara un horizonte genérico \\(h\\) tenemos\n\\[\n\\begin{array}{c}\nx_{T+h}= F^h x_T+ \\sum_{l=1}^h F^{h-l} g \\epsilon_{T+l} \\\\\ny_{T+h}=wF^{h-1}x_{T}+\\sum_{l=2}^{h-1} wF^{h-l}g\\epsilon_{T+l}+\\epsilon_{T+h}\n\\end{array}\n\\]\nTomando las esperanzas condicionadas tenemos las expresiones que ya hemos calculado anteriormente. La ventaja es que ahora podemos calcular las expresiones explícitas para las varianzas condicionadas, que son\n\\[\n\\begin{array}{c}\n\\text{Var}(\\hat{x}_{T+h}|Y_T)=E[(x_{T+h}-\\hat{x}_{T+h|T})(x_{T+h}-\\hat{x}_{T+h|T})'|Y_T]= \\sum_{l=1}^h F^{h-l} g g' (F^{h-l})' \\sigma^2 \\\\\n\\text{Var}(\\hat{y}_{T+h}|Y_T)=E(y_{T+h}-\\hat{y}_{T+h|T})^2=\\sum_{l=2}^{h-1} wF^{h-l}g g'(F^{h-l})' w' \\sigma^2+\\sigma^2\n\\end{array}\n\\]\nLos intervalos de confianza de las predicción se estiman de la forma habitual, como un intervalo en torno a la media que es una constante multiplicada por la desviación típica. Dicha constante depende del nivel de confianza elegido en una distribución \\(t\\) de Student con \\(T-p\\) grados de libertad.\nPara el modelo no lineal las predicciones serían similares, pero muchos de los casos particulares no tienen una formulación analítica explícita, aunque siempre se pueden obtener por simulación, como se especifica en la siguiente sección.\n\n\nResuelve\nSolución\nR\nPython\nMATLAB/Octave\n\n\n\n\nEstima el modelo adecuado para los pasajeros de avión hasta febrero de 2020 en logaritmos (ETS). Predice dos años hacia adelante la serie y representa la predicción puntual con su intervalo de confianza al 95% (en R puedes utilizar autoplot + autolayer). Para ello debes utilizar la media y la varianza de las predicciones en los campos yFor e yForV del objeto con contiene el modelo.\n\n\n\n\n\n\n\n\n\n\n# Limpiando memoria\nrm(list=ls())\n# Selecionando muestra\nx = window(log(airpas), end = c(2020, 2))\n# Horizonte de predicción\nh = 24\n# Estimando y prediciendo\nm = ETS(x, h = h)\n# Calculando bandas de confianza al 95%\nbands = cbind(m$yFor + 2 * sqrt(m$yForV), m$yFor - 2 * sqrt(m$yForV))\n# Mostrando gráficamente\nautoplot(tail(airpas, 60)) + \n    autolayer(exp(m$yFor)) + autolayer(exp(bands)) \n\n\n\n\n# Selecionando muestra\nx = window(np.log(airpas), end = '2020-02-29')\n# Horizonte de predicción\nh = 24\n# Estimando y prediciendo\nm = ETS(x, h = h)\n# Calculando bandas de confianza al 95%\nbands = np.exp(pd.concat((m.yFor + 2 * np.sqrt(m.yForV), \n                          m.yFor - 2 * np.sqrt(m.yForV)), axis=1))\n# Mostrando gráficamente\nplt.plot(airpas[-60:])\nplt.plot(bands)\nplt.plot(np.exp(m.yFor))\n\n\n\n\n% Limpiando memoria y cargando datos\nclear all\nload data\n% Selecionando muestra\nx = log(airpas(1 : 614));\n% Horizonte de predicción\nh = 24;\n% Estimando y prediciendo\nm = ETS(x, 12, h = h);\n% Calculando bandas de confianza al 95%\nbands = [m.yFor + 2 * sqrt(m.yForV) m.yFor - 2 * sqrt(m.yForV)];\n% Mostrando gráficamente\nt = 590 : length(airpas);\nplot(t, airpas(t), 615 : 614 + h, exp([bands m.yFor]))\n\n\n\n\n\n4.5.3 Predicción de la distribución completa\nLas predicciones de cualquier sistema lineal o no lineal también se pueden calcular simulando el ruido. Basta con considerar todos los valores estimados de los parámetros, así como el vector final de estados para obtener tantas predicciones como ruidos simulados. Estos valores futuros sirven para reconstruir las distribuciones de las predicciones con cualquier horizonte.\nUn aspecto final interesante es que el ruido que se simula puede provenir de las propiedades teóricas asumidas para el modelo, habitualmente una distribución normal (simulación de Montecarlo), o puede hacerse extrayendo aleatoriamente (con repetición) valores de los residuos estimados (simulación bootstrap). El interés del último caso es que se reproduce la distribución empírica de los residuos.\n\n\nResuelve\nSolución\nR\nPython\nMATLAB/Octave\n\n\n\nEstima el modelo adecuado para los pasajeros de avión en logaritmos hasta febrero de 2020 (ETS). Mira el diagnóstico de residuos en la tabla de estimación. Predice dos años hacia adelante la serie haciendo una simulación bootstrap para estimar la distribución de las predicciones (entrada bootstrap de ETS). Representa gráficamente la predicción con el intervalo de predicción calculado con las simulaciones al \\(95\\%\\) de confianza (quantile). Haz un test de gaussianidad de las predicciones un mes y doce meses hacia adelante (gaussTest) y explica si son gaussianas y si tiene sentido la simulación bootstrap.\n\n\n\n\n--------------------------------------------------------\n    Model            AIC           BIC          AICc\n--------------------------------------------------------\n   (A,N,N):        2.1785        2.1929        2.1785\n   (A,N,A):        0.3654        0.4734        0.3654\n   (A,A,N):        2.1830        2.2118        2.1830\n   (A,A,A):        0.2950        0.4174        0.2966\n  (A,Ad,N):        2.1841        2.2201        2.1841\n  (A,Ad,A):        0.5394        0.6689        0.5410\n   (M,N,N):        2.2149        2.2293        2.2149\n   (M,N,A):        0.4167        0.5247        0.4167\n   (M,N,M):        0.4457        0.5536        0.4457\n   (M,A,N):        2.2172        2.2459        2.2172\n   (M,A,A):        0.3334        0.4558        0.3350\n   (M,A,M):        0.5474        0.6698        0.5490\n  (M,Ad,N):        2.2181        2.2541        2.2181\n  (M,Ad,A):        0.3607        0.4903        0.3624\n  (M,Ad,M):        0.5660        0.6955        0.5676\n--------------------------------------------------------\n  Identification time:    0.32583 seconds\n--------------------------------------------------------\n -------------------------------------------------------------\n  Model: ETS(A,A,A)\n  Box-Cox lambda: 1.00\n  Q-Newton: Function convergence.\n -------------------------------------------------------------\n                   Param        S.E.          |T|     |Grad|\n -------------------------------------------------------------\n      Alpha:       0.3367     0.0263      12.7829  1.960e-08\n       Beta:       0.0090     0.0036       2.4722  3.111e-09\n      Gamma:       0.3821     0.0288      13.2834  1.394e-08\n -------------------------------------------------------------\n   AIC:       0.2950   BIC:       0.4174   AICc:       0.2966\n            Log-Likelihood:     -73.5637\n -------------------------------------------------------------\n    Summary statistics:\n -------------------------------------------------------------\n         Missing data:      \n         Q( 1):      26.4152         Q( 4):      33.9828\n         Q( 8):      58.7637         Q(12):      63.1103\n   Bera-Jarque:    1366.6633       P-value:       0.0000\n       H( 206):       0.9863       P-value:       0.4606\n   Outliers (&gt;2.7 ES):    10\n         Q( 1):      60.4878         Q( 4):      85.1360\n         Q( 8):     104.5970         Q(12):     133.6029\n   Bera-Jarque:       8.0994       P-value:       0.0174\n       H( 202):       0.8119       P-value:       0.0698\n -------------------------------------------------------------\n\n\n\n\n\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  x\nW = 0.93852, p-value &lt; 2.2e-16\n\n\n\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  x\nW = 0.97853, p-value &lt; 2.2e-16\n\n\n\n\n\n\n\n\nrm(list=ls())\nx = window(log(airpas), end = c(2020, 2))\nh = 24\nm = ETS(x, bootstrap = TRUE, h = h, verbose = TRUE)\n# Calculando bandas de confianza empíricas\nbands = ts(matrix(NA, h, 2), start=end(x) + 1/12, frequency = 12)\nfor (i in 1 : h){\n    bands[i, ] = quantile(m$ySimul[i, ], c(0.025, 0.975))\n}\n# Representando predicciones\nautoplot(tail(airpas, 60)) + \n    autolayer(exp(m$yFor)) + autolayer(exp(bands))\n# Tests de gaussianidad de distribución de errores de predicción\ngaussTest(m$ySimul[1, ])\ngaussTest(m$ySimul[12, ])\n\n\n\n\nx = window(np.log(airpas), end = '2020-02-29')\nh = 24\nm = ETS(x, bootstrap=True, h=h, verbose=True)\n# Calculando bandas de confianza empíricas\nbands = ts(np.full((h, 2), np.nan), start=m.yFor.index[0], freq='m')\nfor i in range(h):\n    bands.iloc[i] = np.quantile(m.ySimul[i, :], [0.025, 0.975])\n# Representando predicciones\nplt.plot(airpas[-60:])\nplt.plot(np.exp(bands))\nplt.plot(np.exp(m.yFor))\n# Tests de gaussianidad de distribución de errores de predicción\ngaussTest(m.ySimul[0, ])\ngaussTest(m.ySimul[11, ])\n\n\n\n\nclear all\nload data\nx = log(airpas(1 : 614));\nh = 24;\nm = ETS(x, 12, bootstrap=true, h = h, verbose=true);\n% Calculando bandas de confianza empíricas\nbands = nan(h, 2);\nfor i = 1 : h\n  bands(i, :) = quantile(m.ySimul(i, :), [0.025 0.975]);\nend\n% Representando predicciones\nt = 590 : length(airpas);\nplot(t, airpas(t), 615 : 614 + h, exp([bands m.yFor]))\n% Tests de gaussianidad de distribución de errores de predicción\ntoolTEST(m.ySimul(1, :));\ntoolTEST(m.ySimul(12, :));"
  },
  {
    "objectID": "03-etsSSOEenSS.html#análisis-completo-de-la-serie-de-pasajeros-de-avión",
    "href": "03-etsSSOEenSS.html#análisis-completo-de-la-serie-de-pasajeros-de-avión",
    "title": "\n4  Suavizado exponencial en Espacio de los Estados\n",
    "section": "\n4.6 Análisis completo de la serie de pasajeros de avión",
    "text": "4.6 Análisis completo de la serie de pasajeros de avión\n\n\nResuelve\nSolución\nR\nPython\nMATLAB/Octave\n\n\n\nYa estamos en condiciones de realizar un análisis completo de los pasajeros de avión, que puede hacerse siguiendo este ejemplo. El ejercicio de predicción que se propone es para el año 2019, que es el último año después de las profundas perturbaciones debidas a la pandemia. Compararemos los resultados con los de los métodos tipo naive.\nEs conveniente siempre limpiar la memoria y cargar los paquetes necesarios. Genera una serie nueva (por ejemplo, x) que sean los pasajeros de avión hasta finales de 2018. Realiza la predicción para 2019 con cuatro modelos: naive, naive estacional, media anual, ETS y ETS en logaritmos. Indica si el modelo ETS es aceptable mostrando los parámetros, los componentes y los tests. ¿Qué modelo es el que mejor predice el año 2019? Representa gráficamente todas las predicciones con los valores reales.\nPuedes también estimar cuántos pasajeros de avión se han perdido desde marzo de 2020 en adelante.\n\n\n\n\n -------------------------------------------------------------\n  Model: ETS(M,A,M)\n  Box-Cox lambda: 1.00\n  Q-Newton: Function convergence.\n -------------------------------------------------------------\n                   Param        S.E.          |T|     |Grad|\n -------------------------------------------------------------\n      Alpha:       0.3184     0.0263      12.0859  3.553e-07\n       Beta:       0.0078     0.0040       1.9396  1.066e-06\n      Gamma:       0.3705     0.0290      12.7913  1.776e-06\n -------------------------------------------------------------\n   AIC:      17.3254   BIC:      17.4499   AICc:      17.3270\n            Log-Likelihood:   -5180.6098\n -------------------------------------------------------------\n    Summary statistics:\n -------------------------------------------------------------\n         Missing data:      \n         Q( 1):      21.2068         Q( 4):      30.1929\n         Q( 8):      52.6640         Q(12):      55.6965\n   Bera-Jarque:    4270.0498       P-value:       0.0000\n       H( 202):       0.9096       P-value:       0.2507\n   Outliers (&gt;2.7 ES):     9\n         Q( 1):      64.0726         Q( 4):      93.2012\n         Q( 8):     107.5363         Q(12):     135.1078\n   Bera-Jarque:       7.1247       P-value:       0.0284\n       H( 198):       0.8127       P-value:       0.0727\n -------------------------------------------------------------\n\n\n\n\n\nSummary statistics:\n==================\n                        Serie 1\nData points:          596.00000\nMissing:               12.00000\nMinimum:               -0.16129\n1st quartile:          -0.02145\nMean:                   0.00217\nP(Mean = 0):            0.26186\nMedian:                 0.00263\n3rd quartile:           0.02881\nMaximum:                0.43473\nInterquartile range:    0.05025\nRange:                  0.59602\nSatandard deviation:    0.04678\nVariance:               0.00219\nSkewness:               1.05824\nKurtosis:              12.82666\nAutocorrelation tests:\n=====================\n     SACF sa      LB p.val  SPACF sp\n1   0.184  +  19.854     0  0.184  +\n2   0.086  +  24.203     0  0.054  .\n3   0.019  .  24.419     0 -0.006  .\n4   0.087  +  28.873     0  0.083  +\n5  -0.115  -  36.731     0 -0.152  -\n6  -0.113  -  44.305     0 -0.082  .\n7  -0.079  .  48.002     0 -0.029  .\n8  -0.060  .  50.127     0 -0.040  .\n9  -0.023  .  50.447     0  0.027  .\n10  0.038  .  51.310     0  0.051  .\n11  0.032  .  51.904     0  0.003  .\n12  0.048  .  53.299     0  0.027  .\n13  0.240  +  87.908     0  0.222  +\n14  0.093  +  93.154     0 -0.009  .\n15  0.027  .  93.577     0 -0.016  .\n16 -0.008  .  93.615     0 -0.010  .\n17 -0.031  .  94.181     0 -0.075  .\n18 -0.118  - 102.612     0 -0.059  .\n19 -0.191  - 124.817     0 -0.120  -\n20 -0.107  - 131.815     0 -0.028  .\n21 -0.007  . 131.843     0  0.068  .\n22 -0.014  . 131.967     0  0.000  .\n23  0.032  . 132.611     0  0.027  .\n24 -0.015  . 132.746     0 -0.068  .\n25  0.156  + 147.570     0  0.128  +\nGaussianity tests:\n=================\n\n    Shapiro-Wilk normality test\n\ndata:  x\nW = 0.91717, p-value &lt; 2.2e-16\n\nRatio of variance tests:\n=======================\n Portion_of_data F_statistic p.value\n         0.33333      0.9149  0.5374\n\n\n\n\n\n                     ME     RMSE       MAE         MPE     PRMSE      MAPE\nnaive        -5053.6817 6714.306 5430.9235 -19.6385890 25.959385 22.034494\nnaiveS       -1059.6768 1118.936 1059.6768  -4.9411708  5.236211  4.941171\nmediaAnual   -1059.6768 4545.890 4167.7368  -0.6446591 20.999410 19.319335\nm$yFor         695.0559 1080.267  805.5044   2.6233379  4.216031  3.315608\nexp(ml$yFor)   959.6937 1356.295 1043.8803   3.7791764  5.327165  4.311714\n                 sMAPE     MASE     RelMAE Theil's U\nnaive        26.008694 9.334455 0.19049909 1.9629025\nnaiveS        5.082549 1.821330 0.03717001 0.3959328\nmediaAnual   19.361054 7.163340 0.14619062 1.5878571\nm$yFor        3.235855 1.384469 0.02825447 0.3187926\nexp(ml$yFor)  4.179551 1.794180 0.03661592 0.4028102\n\n\n\n\n\n\n\nLos modelos elementales producen predicciones mejores que los modelos más sofisticados. Esto resulta bastante desesperanzador, pero no debemos olvidar que estamos evaluando los modelos sobre un solo origen de predicción. Esto es bastante inapropiado, puesto que un buen modelo será aquel que produzca mejores predicciones de forma sistemática, es decir, con varios orígenes de predicción, que es lo que haremos más adelante.\n\n\n\n# Limpiando memoria\nrm(list = ls())\n# Cargando librerías\nlibrary(UComp)\nlibrary(ggplot2)\n# Seleccionando muestra\nx = window(airpas, end = c(2018, 12))\n# Transformando variable\nxl = log(x)\n# Horizonte de predicción y periodo estacional\nh = 12   # Horizonte de predicción\nm = 12   # Observaciones por año\n# Estimando predicciones\nnaive = ts(rep(tail(x, 1), h), start = 2019, frequency = m)\nnaiveS = ts(tail(x, h), start = 2019, frequency = m)\nmediaAnual = ts(rep(mean(tail(x, m)), h), start = 2019, frequency = m)\nm = ETS(x, h = h)\nml = ETS(log(x), h = h)\n# Muestra estimación de modelo sin logaritmos\nprint(m)\n# Muestra componentes\nplot(m)\n# Tests de modelo sin logaritmos\ntests(m)\n# Matriz con todas las predicciones\npredicciones = cbind(naive, naiveS, mediaAnual, m$yFor, exp(ml$yFor))\n# Precisión de predicciones\nAccuracy(predicciones, airpas)\n# Representación gráfica\nautoplot(tail(airpas, 60)) + autolayer(predicciones)\n\n\n\n\nfrom UComp import *\n# Seleccionando muestra\nx = window(airpas, end='2018-12-31')\n# Transformando variable\nxl = np.log(x)\n# Horizonte de predicción y periodo estacional\nh = 12   # Horizonte de predicción\nm = 12   # Observaciones por año\n# Estimando predicciones\nm = ETS(x, h = h)\nml = ETS(np.log(x), h = h)\nnaive = ts(x[-1].repeat(h), start=m.yFor.index[0], freq='m')\nnaiveS = ts(x[-12:].repeat(1), start=m.yFor.index[0], freq='m')\nmediaAnual = ts(np.mean(x[-12:]).repeat(h), start=m.yFor.index[0], freq='m')\n# Muestra estimación de modelo sin logaritmos\nm.validate()\n# Muestra componentes\nm.plot()\n# Tests de modelo sin logaritmos\ntests(m)\n# Matriz con todas las predicciones\npredicciones = pd.concat((naive, naiveS, mediaAnual, m.yFor, np.exp(ml.yFor)), axis=1)\n# Precisión de predicciones\nAccuracy(predicciones, airpas)\n# Representación gráfica\nplt.plot(airpas[-60:])\nplt.plot(predicciones)\n\n\n\n\n% Limpiando memoria y cargando datos\nclear all\nload data\n% Seleccionando muestra\nx = airpas(1 : 600);\n% Transformando variable\nxl = log(x);\n% Horizonte de predicción y periodo estacional\nh = 12;   % Horizonte de predicción\n% Estimando predicciones\nnaive = repmat(x(end), h, 1);\nnaiveS = x(end - h + 1 : end);\nmediaAnual = repmat(mean(x(end - h + 1 : end)), h, 1);\nm = ETS(x, 12, h = h);\nml = ETS(log(x), 12, h = h);\n% Muestra estimación de modelo sin logaritmos\nETSvalidate(m);\n% Muestra componentes\nstackedplot(m.comp)\n% Tests de modelo sin logaritmos\ntoolTEST(m.v);\n% Matriz con todas las predicciones\npredicciones = [naive, naiveS, mediaAnual, m.yFor, exp(ml.yFor)];\n% Precisión de predicciones\nAccuracy(predicciones, airpas(1 : 600 + h), 12)\n% Representación gráfica\nt = (553 : length(airpas))';\nplot(t, airpas(t), (601 : 600 + h), predicciones)"
  },
  {
    "objectID": "03-etsSSOEenSS.html#bibliografía",
    "href": "03-etsSSOEenSS.html#bibliografía",
    "title": "\n4  Suavizado exponencial en Espacio de los Estados\n",
    "section": "Bibliografía",
    "text": "Bibliografía\nHyndman, R., Koehler, A.B., Ord, J.K. (2008), Forecasting with Exponential Smoothing: The State Space Approach. Springer Series in Statistics.\nHyndman, R.J., & Athanasopoulos, G. (2021) Forecasting: principles and practice, 3rd edition, OTexts: Melbourne, Australia. OTexts.com/fpp3."
  },
  {
    "objectID": "04-etsMSOE.html#espacio-de-los-estados-de-nuevo",
    "href": "04-etsMSOE.html#espacio-de-los-estados-de-nuevo",
    "title": "\n5  Suavizado Exponencial con mútiples ruidos\n",
    "section": "\n5.1 Espacio de los estados de nuevo",
    "text": "5.1 Espacio de los estados de nuevo\nEn la bibliografía de series temporales existe una discusión sobre la conveniencia de usar modelos con una sola fuente de error (single source of error en inglés, SSOE) como los el capítulo anterior o modelos con múltiples fuentes de error (multiple source of error, MSOE). La forma de espacio de los estados general para este tipo de modelos es\n\\[\n\\begin{array}{rl}\n    \\text{Ecuación de observación:} &    y_t = w x_t + \\epsilon_t \\\\\n    \\text{Ecuación de transición:} & x_{t} = F x_{t-1} + \\eta_t\n\\end{array}\n\\tag{5.1}\\]\nHay dos diferencias entre este sistema y el sistema lineal de la Ecuación 4.1 del capítulo anterior:\n\nLa ecuación de observación relaciona \\(y_t\\) con \\(x_t\\), no con \\(x_{t-1}\\). Este es un cambio simple de notación que se hace para poder acomodar con mayor facilidad algunos modelos estacionales.\nEn la ecuación de transición aparece un vector de ruidos \\(\\eta_t\\) sustituyendo al término anterior \\(g\\epsilon_t\\). Este vector de ruidos se asume gaussiano con vector de medias cero y matriz de varianzas y covarianzas \\(Q\\).\n\nLa principal ventaja, en principio, es que se trata de una formulación más general y se espera que sea más flexible, como de hecho veremos más adelante. Además, permitirá estimar los componentes de tendencia y estacionales con mayor precisión al poder utilizar toda la muestra en cada momento del tiempo mediante algoritmos de suavizado.\nPero tiene también inconvenientes, puesto que la formulación no lineal no es tan inmediata como antes (de hecho no la vamos a tratar en este libro). Además, su tratamiento estadístico, aún siendo lineal, es más complicado, aunque está ya muy bien establecido en la bibliografía y no ofrece ningún problema. La complicación viene de que la única conexión entre las ecuaciones de observación y transición son solo los estados, no los ruidos. Esto hace que no podamos despejar el ruido de la ecuación de observación y sustituir en la de transición como se hizo en el capítulo anterior. De esta forma los estados NO son observables y habrá que utilizar algoritmos especializados para estimarlos, como son el filtro de Kalman y algoritmos de suavizado. Esto hace que el tratamiento de este tipo de modelos necesite de un nivel técnico estadístico que está fuera del interés de este libro, centrado en aspectos más pragmáticos. El lector interesado siempre puede consultar la bibliografía para entender con todo lujo de detalle lo que se expone en este capítulo.\nQueda claro, por tanto, que el modelo lineal del capítulo anterior es un caso particular de la Ecuación 4.1 si \\(\\eta_t=g\\epsilon_t\\). Pero ahí no acaba la historia, porque se puede demostrar que el sistema de la Ecuación 4.1 se puede escribir como el sistema de la Ecuación 4.1, aunque los estados no son los mismos y su interpretación puede tener dificultades (ver Jerez et al., 2020). Es decir, en general, se puede ir en las dos direcciones.\nEste tema es en realidad una cuestión que interesa a los teóricos porque diluye en cierto modo la disputa sobre la generalidad de los modelos SSOE y MSOE. En cualquier caso, la relevancia práctica de este hecho es que si partimos de una especificación MSOE como la que se propone en este capítulo es prácticamente imposible obtener como caso particular un modelo SSOE y viceversa. La conclusión es que en realidad son dos familias de modelos, cada una con sus ventajas y sus inconvenientes, que se deben tratar como complementarias y que se pueden usar simultáneamente."
  },
  {
    "objectID": "04-etsMSOE.html#taxonomía",
    "href": "04-etsMSOE.html#taxonomía",
    "title": "\n5  Suavizado Exponencial con mútiples ruidos\n",
    "section": "\n5.2 Taxonomía",
    "text": "5.2 Taxonomía\nLos modelos de suavizado exponencial están estrechamente relacionados con los modelos de componentes no observables (Unobserved Components, ver por ejemplo, este artículo). Al igual que en el suavizado exponencial estándar, la familia de modelos completa va a estar configurada por combinaciones de distintas especificaciones para cada componente. En particular, la taxonomía para ruido aditivo aparece en la Figura 5.1.\n\n\n\n\nFigura 5.1: Forma de espacio de los estados de modelos de suavizado exponencial con ruido aditivo y varias fuentes de ruido.\n\n\n\nEl modelo de estacionalidad codificado como ‘D’ es nuevo y está formado por un conjunto de subcomponentes \\(s_{j,t}\\). Cada uno de ellos es una onda periódica estocástica que obedece a la forma\n\\[\n\\left[ \\begin{array}{c} s_{j,t} \\\\ s_{j,t}^* \\end{array} \\right] =\n\\left[ \\begin{array}{cc} \\cos \\omega_j & \\text{sen} \\omega_j \\\\\n                  -\\text{sen} \\omega_j &  \\cos \\omega_j \\end{array}  \\right]\n\\left[ \\begin{array}{c} s_{j,t-1} \\\\ s_{j,t-1}^* \\end{array} \\right]+\n\\left[ \\begin{array}{c} \\eta_{j,t} \\\\ \\eta_{j,t}^* \\end{array} \\right]\n\\]\ndonde \\(\\omega_j\\) es la frecuencia del subcomponente, y \\(\\eta_{j,t}\\) y \\(\\eta_{j,t}^*\\) son dos ruidos diferentes con varianza común \\(\\sigma_j^2\\), pero con varianzas diferentes a los de los demás subcomponentes estacionales, es decir, \\(\\sigma^2_j \\neq \\sigma^2_i, \\forall i \\neq j\\).\nPara comprender estos modelos es necesario tener en cuenta los siguientes puntos:\n\nEl componente estacional ‘D’ está formado por \\([m/2]\\) términos, donde los corchetes aquí indican la división entera, es decir, \\([m/2]=(m-1)/2\\) para \\(m\\) impar y \\([m/2]=m/2\\) para \\(m\\) par. Las frecuencias de cada componente es \\(\\omega_j=2 \\pi j /m, j=1,2,\\dots,[m/2]\\), o bien los periodos correspondientes son \\(m_j=2 \\pi / \\omega_j = m/j, j=1,2,\\dots,[m/2]\\). Por ejemplo, para una serie mensual \\(m=12\\), que tendrá \\(6\\) subcomponentes con periodos \\(12\\), \\(12/2=6\\), \\(12/3=4\\), \\(12/4=3\\), \\(12/5=2,4\\) y \\(12/6=2\\). En el caso de una serie trimestral \\(m=4\\) y solo tenemos dos subcomponentes de periodos \\(4\\) y \\(4/2=2\\).\nEl modelo ‘D’ para la estacionalidad es en realidad una descomposición en series de Fourier ortogonales permitiendo que cada frecuencia tenga una potencia diferente, en función de la varianza del ruido que afectan a cada una.\nEl componente estacional entra en la ecuación de observación alineado en el tiempo con la variable salida, ambos se especifican en \\(t\\). Esto es así porque, si bien tiene sentido el retardo \\(m\\) en dicho componente para la estacionalidad aditiva (en el que \\(s_{t-m}\\) es el componente estacional libre de ruido), no tiene ningún sentido para el nuevo modelo ‘D’.\nEn la Figura 5.1 todos los componentes son aditivos (lineales), incluyendo el ruido. El hecho de disponer de varias fuentes de error permite considerar todos estos modelos cuando no hay ruido en la ecuación de observación puesto que la aleatoriedad puede concentrarse en los demás ruidos, es decir, con \\(\\sigma_{\\epsilon}^2=0\\), lo que daría lugar a modelos PTS(N,.,.).\nEn estos modelos el ‘residuo’ siempre es la innovación, es decir, los errores de predicción un periodo hacia adelante. Estos no coinciden con la estimación de \\(\\epsilon_t\\) por medio de la ecuación de observación. Esta innovación NO es cero ni siquiera en el caso en que el modelo no tenga ruido en la ecuación de observación (modelos PTS(N,.,.)).\nLa tendencia \\(L\\) es un caso particular de \\(A\\) con \\(\\sigma_{\\eta_l}^2=0\\). Este caso se incluye explícitamente porque resulta muy relevante en la predicción y no era posible en modelos SSOE.\nLos parámetros del modelo (algunos los llaman hiper-parámetros) son las varianzas de todos los ruidos incluidos en el modelo. Como veremos más tarde, cualquiera de las varianzas se puede eliminar de la estimación concentrándola fuera de la función de verosimilitud.\n\n\n\nResuelve\nSolución\n\n\n\n¿Cuántos armónicos (subcomponentes estacionales) tendrá un componente estacional para una serie horaria con estacionalidad diaria? ¿Y un serie horaria con estacional semanal? ¿Y una serie diaria con estacionalidad semanal?\n\n\nLa primera tendrá 12 armónicos de periodo \\(24, 24/2, 24/3, \\dots, 24/12\\).\nLa segunda tendrá 84 armónicos de periodo \\(168, 168/2, 168/3, \\dots, 168/84\\).\nLa tercera tendrá 3 armónicos de periodo \\(7, 7/2\\) y \\(7/3\\).\nComo se ve, siempre que el periodo es par, el último periodo es 2, que se corresponde con lo que se suele conocer como la frecuencia Nyquist."
  },
  {
    "objectID": "04-etsMSOE.html#filtrado-y-suavizado",
    "href": "04-etsMSOE.html#filtrado-y-suavizado",
    "title": "\n5  Suavizado Exponencial con mútiples ruidos\n",
    "section": "\n5.3 Filtrado y suavizado",
    "text": "5.3 Filtrado y suavizado\nUna primera cuestión que surge con cualquiera de los modelos lineales considerados es cómo estimar óptimamente los estados. Como ya se ha dicho, en estos sistemas los estados no son observables y por tanto su estimación no es tan directa como resultaba en el sistema en forma de innovaciones. Además, como en toda estimación, los estados solo se podrán conocer con cierto nivel de incertidumbre, que vendrá representada por su matriz de covarianzas \\(\\hat{P}_t\\). Es decir, los estados serán variables aleatorias que evolucionan en el tiempo con una determinada distribución de probabilidad multivariante que tendrá sus momentos. Asumiendo normalidad, nos vamos a centrar en los primeros dos momentos de la distribución: el vector de medias y su matriz de covarianzas.\n\n5.3.1 Filtro de Kalman\nEl filtro de Kalman es precisamente un algoritmo recursivo que nos da la estimación óptima del vector de medias y matriz de covarianzas de los estados, basándose en todas la información pasada hasta el momento presente. La derivación del mismo queda fuera del alcance de este libro y se puede consultar, por ejemplo, en Harvey (1989) o Durbin and Koopman (2012).\nEn lo que sigue tendremos en cuenta la siguiente notación: \\(\\text{E}(x_t|Y_{t-1})=\\hat{x}_{t|t-1}\\), \\(\\text{E}(x_t|Y_{t})=\\hat{x}_{t|t}\\) y \\(\\text{E}(x_t|Y_{T})=\\hat{x}_{t|T}\\) son las estimaciones del vector de estados con toda la información disponible hasta \\(t-1\\), \\(t\\) y toda la muestra, respectivamente; \\(\\text{Var}(x_t|Y_{t-1})=\\hat{P}_{t|t-1}\\), \\(\\text{Var}(x_t|Y_{t})=\\hat{P}_{t|t}\\) y \\(\\text{Var}(x_t|Y_{T})=\\hat{P}_{t|T}\\) son las matrices de covarianza de los estados con la información hasta \\(t-1\\), \\(t\\) y toda la muestra, respectivamente; \\(\\hat{v}_t\\) y \\(\\hat{f}_t^2\\) son la innovación (el error de predicción un periodo hacia adelante) y su varianza escalar.\nDada la ?eq-sysLinear podemos calcular la esperanza y varianza condicionada a la información inmediatamente anterior de la siguiente forma\n\\[\n\\begin{array}{rl}\n\\text{E}(x_{t+1}|Y_{t})=\\hat{x}_{t+1|t}=&F\\hat{x}_{t|t} \\\\\n\\text{Var}(x_{t+1}|Y_{t})=\\hat{P}_{t+1|t}=&\\text{E}[x_{t+1}-\\hat{x}_{t+1|t}][x_{t+1}-\\hat{x}_{t+1|t}]'= \\\\\n=&\\text{E}[Fx_t+\\eta_t-F\\hat{x}_{t|t}][Fx_t+\\eta_t-F\\hat{x}_{t|t}]'= \\\\\n=& \\text{E}[F(x_t-\\hat{x}_{t|t})+\\eta_t][F(x_t-\\hat{x}_{t|t})+\\eta_t]'= \\\\\n=&F\\hat{P}_{t|t}F'+Q.\n\\end{array}\n\\]\nDe igual forma, las expresiones para las innovaciones y su varianza son\n\\[\n\\begin{array}{rl}\n\\hat{v}_t=y_t-\\text{E}(y_{t}|Y_{t-1})=&y_t-w\\hat{x}_{t|t-1} \\\\\n\\hat{f}_t^2=\\text{E}(y_t-w\\hat{x}_{t|t-1})^2=&\\text{E}(wx_t+\\epsilon_t-w\\hat{x}_{t|t-1})^2 \\\\\n=& \\text{E}[w(x_t-\\hat{x}_{t|t-1})+\\epsilon_t]^2\\\\\n=& w\\hat{P}_{t|t-1}w'+\\sigma^2\n\\end{array}\n\\]\nEstas cuatro ecuaciones son en realidad parte del filtro de Kalman, que en muchas formulcaciones se divide en dos fases. Primero, se hace una primera estimación de la media y la covarianza de los estados con la predicción un periodo hacia adelante de las mismas (fase de predicción), para después adaptar estas estimaciones incorporando la información que da el nuevo dato (fase de adaptación).\n\\[\n     \\begin{array}{cc}\n    \\text{Fase de predicción:} & \\\\\n    \\hat{x}_{t+1|t}=F \\hat{x}_{t|t}, & \\hat{P}_{t+1|t} = F \\hat{P}_{t|t} F'+ Q,\n    \\end{array}\n\\] \\[\n\\begin{array}{cc}\n    \\text{Fase de adaptación:} & \\\\\n    \\hat{v}_t=y_t-w \\hat{x}_{t|t-1},     &  \\hat{f}_t^2 = w \\hat{P}_{t|t-1} w'+ \\sigma^2, \\\\\n    \\hat{x}_{t|t}=\\hat{x}_{t|t-1}+\\hat{P}_{t|t-1} w' \\hat{f}_t^{-2} \\hat{v}_t,     & \\hat{P}_{t|t}=\\hat{P}_{t|t-1}-\\hat{P}_{t|t-1} w' \\hat{f}_t^{-2} w \\hat{P}_{t|t-1}, \\\\\n\\end{array}\n\\]\nLa formulación en ecuaciones de predicción y observación es especialmente conveniente para el tratamiento de observaciones ausentes. Cuando aparece una observación ausente solo se pueden aplicar las ecuaciones de predicción, y la predicción de los estados quedarán como interpolación de dichos valores ausentes. De hecho, el filtro de Kalman produce automáticamente la predicción extendiendo cualquier serie temporal con tantos valores ausentes como sea el horizonte de predicción.\nEste filtro depende, como es habitual en todos los sistemas dinámicos, de condiciones iniciales \\(\\hat{x}_0\\) y \\(\\hat{P}_0\\), para lo que existen distintas aproximaciones, dependiendo de si el modelo es estacionario o no. Este tema es bastante técnico, el lector interesado en los detalles puede consultarlos en Harvey (1989), Durbin and Koopman (2012) o Casals et al. (2020).\n\n5.3.2 Suavizado de intervalo fijo\nEl algoritmo de suavizado fijo produce la estimación de los estados y sus covarianzas con la información completa de toda la muestra, es decir, con toda la información pasada y futura. Al utilizar más información y ser una estimación centrada se consigue mayor precisión en las estimaciones y la eliminación de los desfases en la estimación. Existen muchas versiones, aquí se muestra una que corre en el sentido inverso del tiempo y se basa en las estimaciones previas del filtro de Kalman (ver detalles en la bibliografía).\n\\[\n\\begin{array}{cc}\n    r_{t-1}=w'\\hat{f}_{t-1}^{-2} \\hat{v}_t+L'_tr_t, & N_{t-1}=w'\\hat{f}_{t-1}^{-2}w+L_t'N_tL_t, \\\\\n    \\hat{x}_{t|T}=\\hat{x}_{t|t-1}+\\hat{P}_{t|t-1} r_{t-1}, & \\hat{P}_{t|T}=\\hat{P}_{t|t-1}-\\hat{P}_{t|t-1}N_{t-1}\\hat{P}_{t|t-1}.\n\\end{array}\n\\]\nLa Figura 5.2 muestra la estimación con el filtro de Kalman y el algoritmo de suavizado de la tendencia de la serie de pasajeros de avión. Como se ve a simple vista la estimación suavizada (en rojo) es efectivamente más suave que la filtrada.\n\n\n\n\nFigura 5.2: Diferencia de estimación de tendencias para pasajeros de avión entre algoritmo de filtrado (negra) y suavizado (roja).\n\n\n\n\n\n\n\n\n\nFiltrado frente a suavizado\n\n\n\nUtilizar el filtro de Kalman o el de suavizado es semejante a utilizar medias móviles no centradas o centradas. En las medias móviles no centradas solo se utilizan observaciones del pasado (como en el filtro de Kalman), mientras que en las medias móviles centradas se utilizan tanto observaciones del pasado como del futuro (como en el algoritmo de suavizado). El suavizado utiliza más información y or tanto las estimaciones tendrán menor varianzaa."
  },
  {
    "objectID": "04-etsMSOE.html#estimación-y-selección-de-modelos",
    "href": "04-etsMSOE.html#estimación-y-selección-de-modelos",
    "title": "\n5  Suavizado Exponencial con mútiples ruidos\n",
    "section": "\n5.4 Estimación y selección de modelos",
    "text": "5.4 Estimación y selección de modelos\nLos parámetros del modelo que habrá que estimar son las varianzas de todos los ruidos presentes. En el capítulo anterior, en la Ecuación 4.3, se mostró cómo la verosimilitud para toda la muestra se puede expresar como el producto de las probabilidades condicionadas en cada momento del tiempo \\(p(y_t|Y_{t-1})\\). Dado que \\(\\hat{v}_t=y_t-\\text{E}(y_t|Y_{t-1})\\) y \\(\\text{Var}(y_t|Y_{t-1})=\\hat{f}_t^2\\), en este caso cada una de esas probabilidades se puede escribir como\n\\[\np(y_t|Y_{t-1}) = (2\\pi \\hat{f}_t^2)^{-1/2} \\exp \\left[ -\\frac{\\hat{v}_t^2}{2\\hat{f}_t^2} \\right].\n\\]\n\n\n\n\n\nLa función de verosimilitud para toda la muestra es\n\\[\nL(\\theta,x_0,P_0)|Y_T)= (2\\pi )^{-T/2} \\prod_{t=1}^T |\\hat{f}_t|^{-1} \\exp \\left(  - \\frac{\\hat{v}_t^2}{2\\hat{f}_t^2} \\right)\n\\]\nSi se compara esta ecuación con su homóloga del capítulo anterior se verá que la innovación juega el rol del residuo y \\(f_t^2\\) el de su varianza. Es interesante darse cuenta de que, dado que la innovación depende de los estados y estos solo se pueden conocer con cierta incertidumbre, dada por su varianza \\(\\hat{f}_t^2\\), la varianza de la innovación ahora es mayor que la varianza del ruido de la ecuación de observación, es decir, \\(\\hat{f}_t^2&gt;\\sigma^2\\), como se deduce del propio filtro de Kalman. Además, \\(\\hat{f}_t^2\\), la varianza de la innovación, es cambiante en el tiempo, como lo indica el subíndice.\nEl logaritmo de la verosimilitud será\n\\[\n\\log[L(\\theta,x_0,P_0)|Y_T)]= -T/2 \\log (2\\pi) -1/2 \\sum_{t=1}^T \\left[ 2\\log(|\\hat{f}_t|) + \\hat{v}_t^2/\\hat{f}_t^2 \\right].\n\\]\nLa evaluación de esta función requiere la estimación de la innovación y su varianza, que son salidas del filtro de Kalman. Eso implica que para obtener un solo valor de la verosimilitud es necesario correr un algoritmo que va de principio a fin de la muestra. Por tanto, la maximización de esta función es un proceso computacional complejo que conlleva correr el filtro de Kalman muchas veces. Si el método de estimación requiere cálculo númerico del gradiente y/o hessiano, una sola iteración de dicho algoritmo requiere correr el filtro de Kalman al menos tantas veces como parámetros haya. Esa cantidad habrá que multiplicarla por el número de iteraciones que necesite el algoritmo para converger. Existen, no obstante, algunos desarrollos muy brillantes que permiten estimar el vector gradiente en bloque ejecutando una sola vez el filtro de Kalman (ver Durbin y Koopman, 2012).\nCualquiera de las varianzas se puede concentrar fuera de la verosimilitud. Como se ve, esta función de verosimilitud depende de las condiciones iniciales, que son muchas, pues no solo tenemos el vector de estados como en los modelos SSOE, sino que además ahora tenemos la matriz de covarianza inicial. En este caso resulta poco práctico ampliar el vector de parámetros con tantas condiciones iniciales. De hecho, se suele acudir a distintas técnicas de inicialización, como el filtro de Kalman aumentado o la inicialización difusa, ver la bibliografía.\nSe pueden utilizar los criterios de información habituales (AIC, AICc, BIC, etc.) para determinar que modelo es el que mejor ajusta la serie penalizando por el número de parámetros.\n\n\nResuelve\nSolución\nR\nPython\nMATLAB/Octave\n\n\n\nConsidera los datos de pasajeros de avión hasta Febrero de 2020 y estima un modelo PTS(A,A,D) para la serie en logaritmos con la función PTS. Muestra la optimización en curso con la entrada verbose. ¿Cuántas iteraciones han sido necesarias? Muestra la tabla de estimación y realiza un diagnóstico de residuos (tests del objeto modelo). Representa también los componentes estimados. ¿Te parece un modelo adecuado?\n\n\n\n\n Iter FunEval  Objective       Step\n          2     -4.95405      1.00000\n    1     4     -4.96725      1.00000\n    2     6     -5.00714      1.00000\n    3     8     -5.03806      1.00000\n    4    10     -5.09592      1.00000\n    5    12     -5.15536      1.00000\n    6    14     -5.19219      1.00000\n    7    16     -5.25635      1.00000\n    8    19     -5.26563      0.50000\n    9    22     -5.27054      0.50000\n   10    24     -5.27222      1.00000\n   11    26     -5.27257      1.00000\n   12    28     -5.27267      1.00000\n   13    30     -5.27268      1.00000\n   14    32     -5.27268      1.00000\n   15    34     -5.27268      1.00000\n   16    37     -5.27268      0.50000\nQ-Newton: Function convergence\nElapsed time:    0.26994 seconds\n-------------------------------------------------------------\n  Box-Cox lambda: 1.00\n  Model: (A,A,D)\n  Periods:  12.0 /  6.0 /  4.0 /  3.0 /  2.4 /  2.0\n  Q-Newton: Function convergence\n  (*)  concentrated out parameters\n -------------------------------------------------------------\n                      Param   asymp.s.e.        |T|     |Grad| \n -------------------------------------------------------------\n        Level:     2.32e-04     4.67e-05     4.9598   2.62e-05\n        Slope:     4.89e-07     3.27e-06     0.1495   1.80e-05\n   Seas(12.0):     1.32e-05     9.04e-06     1.4617   2.19e-05\n    Seas(6.0):     1.63e-06     7.04e-06     0.2315   1.19e-05\n    Seas(4.0):     2.43e-06     1.72e-05     0.1413   1.62e-05\n    Seas(3.0):     7.83e-07     4.04e-06     0.1940   1.74e-05\n    Seas(2.4):     3.67e-08     2.84e-06     0.0129   1.28e-05\n    Seas(2.0):     8.19e-07     7.74e-06     0.1057   2.03e-05\n    Irregular:     6.61e-04*  \n -------------------------------------------------------------\n   AIC:      -3.2397   BIC:      -3.0741   AICc:      -3.2380\n            Log-Likelihood:    1017.5765\n -------------------------------------------------------------\n    Summary statistics:\n -------------------------------------------------------------\n         Missing data:      \n         Q( 1):       0.0332         Q( 4):       5.7146\n         Q( 8):       7.3737         Q(12):       8.3664\n   Bera-Jarque:    2311.4271       P-value:       0.0000\n       H( 202):       0.7580       P-value:       0.0248\n   Outliers (&gt;2.7 ES):    10\n         Q( 1):       3.9602         Q( 4):       8.6131\n         Q( 8):      13.5313         Q(12):      19.3036\n   Bera-Jarque:      13.6763       P-value:       0.0011\n       H( 198):       0.8643       P-value:       0.1528\n -------------------------------------------------------------\n\n\n\n\n\nSummary statistics:\n==================\n                        Serie 1\nData points:          622.00000\nMissing:               24.00000\nMinimum:               -0.17723\n1st quartile:          -0.02417\nMean:                  -0.00119\nP(Mean = 0):            0.48960\nMedian:                 0.00011\n3rd quartile:           0.02143\nMaximum:                0.35500\nInterquartile range:    0.04560\nRange:                  0.53222\nSatandard deviation:    0.04224\nVariance:               0.00178\nSkewness:               0.57133\nKurtosis:               9.44397\nAutocorrelation tests:\n=====================\n     SACF sa     LB p.val  SPACF sp\n1  -0.005  .  0.014 0.907 -0.005  .\n2  -0.006  .  0.032 0.984 -0.006  .\n3  -0.008  .  0.068 0.995 -0.008  .\n4   0.093  +  5.334 0.255  0.093  +\n5  -0.022  .  5.625 0.344 -0.021  .\n6   0.015  .  5.755 0.451  0.016  .\n7   0.045  .  6.962 0.433  0.046  .\n8  -0.015  .  7.094 0.527 -0.024  .\n9  -0.011  .  7.162 0.620 -0.006  .\n10  0.027  .  7.609 0.667  0.025  .\n11 -0.028  .  8.085 0.706 -0.037  .\n12  0.002  .  8.089 0.778  0.008  .\n13  0.069  . 11.000 0.611  0.070  .\n14 -0.016  . 11.162 0.673 -0.024  .\n15 -0.021  . 11.432 0.721 -0.011  .\n16 -0.017  . 11.603 0.771 -0.018  .\n17  0.024  . 11.944 0.803  0.008  .\n18 -0.025  . 12.339 0.829 -0.016  .\n19 -0.112  - 20.172 0.384 -0.116  -\n20 -0.055  . 22.076 0.336 -0.062  .\n21  0.037  . 22.922 0.348  0.040  .\n22 -0.010  . 22.991 0.402 -0.008  .\n23  0.013  . 23.101 0.455  0.030  .\n24 -0.028  . 23.587 0.485 -0.017  .\n25  0.049  . 25.087 0.458  0.044  .\nGaussianity tests:\n=================\n\n    Shapiro-Wilk normality test\n\ndata:  x\nW = 0.92433, p-value &lt; 2.2e-16\n\nRatio of variance tests:\n=======================\n Portion_of_data F_statistic p.value\n         0.33333      1.3475  0.0365\n\n\n\n\n\nHan sido necesarias 16 iteraciones, que han requerido correr la función objetivo y el filtro de Kalman 37 veces. El modelo parece correcto, tanto por las propiedades estadísticas de las innovaciones (aunque los residuos no son normales), como por los componentes.\n\n\n\n\n\n\n\n\n\n\n\n\n# Cargando librerías\nlibrary(ggplot2)\nlibrary(UComp)\n# Seleccionando muestra\nx = log(window(airpas, end=c(2020, 2)))\n# Estimando modelo PTS\nm = PTS(x, model=\"AAD\", verbose = TRUE)\n# Mostrando estimación\nplot(m)\n# Tests de innovaciones\ntests(m)\n\n\n\n\n# Cargando librerías\nfrom UComp import *\n# Seleccionando muestra\nx = window(np.log(airpas), end='2020-02-29')\n# Estimando modelo PTS\nm = PTS(x, model=\"AAD\", verbose=True)\n# Mostrando estimación\nm.plot()\n# Tests de innovaciones\ntests(m)\n\n\n\n\nclear all\nload data\n% Seleccionando muestra\nx = log(airpas(1 : 614));\n% Estimando modelo PTS\nm = PTS(x, 12, model='AAD', verbose = true);\n% Mostrando estimación\nstackedplot(m.comp)\n% Tests de innovaciones\ntoolTEST(m.v)"
  },
  {
    "objectID": "04-etsMSOE.html#predicción",
    "href": "04-etsMSOE.html#predicción",
    "title": "\n5  Suavizado Exponencial con mútiples ruidos\n",
    "section": "\n5.5 Predicción",
    "text": "5.5 Predicción\nEn el capítulo anterior se mostró que la predicción óptima utilizando como función objetivo el error cuadrático medio es precisamente la media condicionada. En la práctica esto significa que la predicción se estima aplicando recursivamente las ecuaciones de predicción del filtro de Kalman, que es equivalente a suponer que la serie tiene valores ausentes al final de la muestra.\nNo obstante, si se quieren fórmulas explícitas de las predicciones, estas se pueden obtener por la sustitución recursiva de la ecuación de transición en sí misma. Para los distintos horizontes tenemos\n\\[\n\\begin{array}{c}\nx_{T+1}=F x_{T} + \\eta_{T+1}, \\\\\nx_{T+2}=F x_{T+1} + \\eta_{T+1}= F^2 x_T+ F \\eta_{T+1}+ \\eta_{T+2}, \\\\\n\\vdots \\\\\nx_{T+h}= F^h x_T+ \\sum_{l=1}^h F^{h-l} \\eta_{T+l}.\n\\end{array}\n\\]\nPara la salida tenemos\n\\[\n\\begin{array}{c}\ny_{T+1}=wFx_{T}+w\\eta_{t+1}+\\epsilon_{T+1}, \\\\\ny_{T+2}=wF^2x_T+wF\\eta_{T+1}+w\\eta_{t+2}+\\epsilon_{T+2}, \\\\\n\\vdots \\\\\ny_{T+h}=wF^{h}x_{T}+\\sum_{l=1}^{h} wF^{h-l}\\eta_{T+l}+\\epsilon_{T+h}.\n\\end{array}\n\\]\nTomando esperanzas y varianzas condicionadas de estas expresiones tenemos\n\\[\n\\begin{array}{c}\nE(\\hat{x}_{T+h}|Y_T)=F^hx_T \\\\\n\\text{Var}(\\hat{x}_{T+h}|Y_T)=E[(x_{T+h}-\\hat{x}_{T+h|T})(x_{T+h}-\\hat{x}_{T+h|T})'|Y_T]= \\sum_{l=1}^h F^{h-l} Q (F^{h-l})' \\\\\nE(\\hat{y}_{T+h}|Y_T)=wF^hx_T \\\\\n\\text{Var}(\\hat{y}_{T+h}|Y_T)= \\sum_{l=1}^h wF^{h-l} Q (F^{h-l})' w' + \\sigma^2\\\\\n\\end{array}\n\\]\nLas simulaciones tipo Montecarlo se pueden realizar en este tipo de modelos, teniendo en cuenta que hay muchos más ruidos que simular. La simulación bootstrap muestreando los ruidos estimados conlleva muchas más complicación, puesto que habría que estimar todos los ruidos del modelo. Se puede hacer, pero engloba cierta complejidad, el lector interesado puede consultar los detalles sobre los algoritmos de disturbance y el importance sampling en Durbin y Koopman (2012).\n\n\nResuelve\nSolución\nR\nPython\nMATLAB/Octave\n\n\n\nEstima el modelo adecuado para la serie de pasajeros en logaritmos hasta Febrero de 2020 (PTS). Predice dos años hacia adelante la serie y representa la predicción media con su intervalo de confianza al 95%. Para ello debes utilizar la media y la varianza de las predicciones en los campos yFor e yForV del objeto con contiene el modelo.\n\n\n\n\n------------------------------------------------------------\n Identification of PTS models:\n------------------------------------------------------------\n    Model            AIC           BIC          AICc\n------------------------------------------------------------\n  (N,N,N):       -1.3901       -1.3757       -1.3901\n  (A,N,N):       -0.6875       -0.6659       -0.6875\n  (N,N,L):       -2.9510       -2.8430       -2.9510\n  (A,N,L):       -2.9756       -2.8604       -2.9756\n  (N,N,D):       -3.0751       -2.9312       -3.0735\n  (A,N,D):       -3.1863       -3.0351       -3.1846\n  (N,A,N):       -1.3682       -1.3394       -1.3682\n  (A,A,N):       -0.6679       -0.6319       -0.6679\n  (N,A,L):       -2.9398       -2.8174       -2.9382\n  (A,A,L):       -2.9827       -2.8531       -2.9811\n  (N,A,D):       -3.0708       -2.9125       -3.0692\n  (A,A,D):       -3.2397       -3.0741       -3.2380\n (N,Ad,N):       -1.3101       -1.2813       -1.3101\n (A,Ad,N):       -0.7107       -0.6747       -0.7107\n (N,Ad,L):       -2.9445       -2.8221       -2.9429\n (A,Ad,L):       -2.9864       -2.8497       -2.9848\n (N,Ad,D):       -3.0702       -2.9046       -3.0685\n (A,Ad,D):       -3.2568       -3.0912       -3.2552\n------------------------------------------------------------\n  Identification time:    5.47811 seconds\n------------------------------------------------------------\n-------------------------------------------------------------\n  Box-Cox lambda: 1.00\n  Model: (A,Ad,D)\n  Periods:  12.0 /  6.0 /  4.0 /  3.0 /  2.4 /  2.0\n  Q-Newton: Unable to decrease objective function\n  (*)  concentrated out parameters\n -------------------------------------------------------------\n                      Param   asymp.s.e.        |T|     |Grad| \n -------------------------------------------------------------\n      Damping:       0.9900       0.0058   171.4025   2.08e-03\n        Level:     2.09e-04     4.63e-05     4.5235   5.42e-05\n        Slope:     9.17e-07     3.37e-06     0.2720   6.37e-03\n   Seas(12.0):     1.34e-05     9.06e-06     1.4760   5.37e-05\n    Seas(6.0):     1.66e-06     7.54e-06     0.2198   1.02e-04\n    Seas(4.0):     2.40e-06     1.57e-05     0.1528   5.65e-04\n    Seas(3.0):     7.85e-07     4.09e-06     0.1919   3.02e-05\n    Seas(2.4):     3.51e-08     2.84e-06     0.0124   9.92e-05\n    Seas(2.0):     8.10e-07     7.76e-06     0.1043   1.15e-04\n    Irregular:     6.68e-04*  \n -------------------------------------------------------------\n   AIC:      -3.2568   BIC:      -3.0912   AICc:      -3.2552\n            Log-Likelihood:    1022.8429\n -------------------------------------------------------------\n    Summary statistics:\n -------------------------------------------------------------\n         Missing data:      \n         Q( 1):       0.0627         Q( 4):       5.1021\n         Q( 8):       6.7093         Q(12):       8.1991\n   Bera-Jarque:    2340.9680       P-value:       0.0000\n       H( 202):       0.7570       P-value:       0.0243\n   Outliers (&gt;2.7 ES):    10\n         Q( 1):       3.5528         Q( 4):       6.6797\n         Q( 8):       9.8367         Q(12):      14.1620\n   Bera-Jarque:      14.1673       P-value:       0.0008\n       H( 198):       0.8627       P-value:       0.1498\n -------------------------------------------------------------\n\n\n\n\n\n\n\n\n# Borrando memoria\nrm(list=ls())\nlibrary(UComp)\nlibrary(ggplot2)\n# Seleccionando muestra\nx = window(airpas, end= c(2020, 2))\n# Estimando modelo PTS\nm = PTS(log(x), h=24, verbose = TRUE)\n# Calculando bandas de confianza\nbands = exp(cbind(m$yFor + 2 * sqrt(m$yForV), m$yFor - 2 * sqrt(m$yForV)))\n# Gráfico de predicciones\nautoplot(tail(airpas, 60)) + \n    autolayer(exp(m$yFor)) + autolayer(bands) \n\n\n\n\n# Seleccionando muestra\nx = window(airpas, end='2020-02-29')\n# Estimando modelo PTS\nm = PTS(np.log(x), h=24, verbose=True)\n# Calculando bandas de confianza\nbands = np.exp(pd.concat((m.yFor + 2 * np.sqrt(m.yForV), \n                          m.yFor - 2 * np.sqrt(m.yForV)), axis=1))\n# Gráfico de predicciones\nplt.plot(airpas[-60:])\nplt.plot(bands)\nplt.plot(np.exp(m.yFor))\n\n\n\n\n% Borrando memoria\nclear all\nload data\n% Selecionando muestra\nx = log(airpas(1 : 614));\nh = 24;\n% Estimando modelo PTS\nm = PTS(x, 12, h=h, verbose=true);\n% Calculando bandas de confianza al 95%\nbands = [m.yFor + 2 * sqrt(m.yForV) m.yFor - 2 * sqrt(m.yForV)];\n% Mostrando gráficamente\nt = 590 : length(airpas);\nplot(t, airpas(t), 615 : 614 + h, exp([bands m.yFor]))"
  },
  {
    "objectID": "04-etsMSOE.html#algunas-extensiones",
    "href": "04-etsMSOE.html#algunas-extensiones",
    "title": "\n5  Suavizado Exponencial con mútiples ruidos\n",
    "section": "\n5.6 Algunas extensiones",
    "text": "5.6 Algunas extensiones\nHay un sinfín de extensiones posibles de los modelos presentados en este capítulo y el anterior en muchas direcciones. De hecho, la representación en espacio de los estados da mucha flexibilidad en este sentido, como muestra la bibliografía. Puesto que este libro está concebido para un curso en tiempo finito no tiene sentido ir mucho más allá. Sí que vamos a presentar dos posibles extensiones implementadas en UComp, que pueden ser útiles a analistas de todos los niveles.\n\n5.6.1 Variables exógenas\nEl modelo de espacio de los estados es fácilmente generalizable para añadir variables exógenas. Basta con cambiar la ecuación de observación añadiendo un término lineal que incluya dichas variables como si fueran términos de regresión,\n\\[\n\\begin{array}{rl}\n       \\text{Modelo ETS lineal:} & y_t = w x_{t-1} + \\beta u_t + \\epsilon_t \\\\\n       \\text{Modelo ETS no lineal:} & y_t = w(x_{t-1}) + \\beta u_t + r(x_{t-1})\\epsilon_t \\\\\n       \\text{Modelo PTS:} & y_t = w x_t + \\beta u_t + \\epsilon_t\n\\end{array}\n\\]\ndonde \\(\\beta\\) es un vector de \\(k\\) parámetros y \\(u_t\\) es una matriz de variables exógenas de dimensión \\(k \\times T\\). Siempre que las variables exógenas se consideren no estocásticas, el cambio realmente solo afectará a las medias de la variable \\(y_t\\), pero no a las varianzas.\nAparte de las consideraciones más técnicas, es importante tener en cuenta al menos dos consideraciones adicionales:\n\nEl investigador debe examinar detenidamente las propiedades de las variables exógenas en términos de componentes. Por ejemplo, no está claro el sentido de modelos que incluyen componentes de tendencia y estacionalidad junto con variables exógenas que a su vez tienen tales componentes. ¿Cuál es la interpretación de dichos componentes?\nPara obtener las predicciones de la variable endógena son necesarias predicciones de las variables exógenas. Esto en la práctica puede suponer un obstáculo en el caso de que sean desconocidas o sean incluso más difíciles de predecir que la variable endógena.\n\n\n\nResuelve\nSolución\nR\nPython\nMATLAB/Octave\n\n\n\nRepresenta gráficamente la serie del caudal del río Nilo (Nile). Algunos estudios han intentado ver si ha habido una caída significativa permanente del caudal del río a partir del año 1899. Analiza si es el caso con modelos de suavizado exponencial. Para ello crea una variable escalón que sea ceros hasta 1898 y unos después de esa fecha. Incluye esa variable en el modelo como variable exógena con la entrada u de la función PTS. Estima el modelo, muestra la tabla de estimación y los componentes. Comprueba si el modelo es adecuado (tests).\n\n\n\n\n\n\n\n-------------------------------------------------------------\n  Box-Cox lambda: 1.00\n  Model: (A,N,N)\n  Periods: \n  Q-Newton: Function convergence\n  (*)  concentrated out parameters\n -------------------------------------------------------------\n                      Param   asymp.s.e.        |T|     |Grad| \n -------------------------------------------------------------\n        Level:       0.1088       4.1619     0.0261   7.06e-06\n    Irregular:   28489.4403*  \n      Beta(1):    -247.0036*     28.9906     8.5201 \n -------------------------------------------------------------\n   AIC:      12.3812   BIC:      12.4854   AICc:      12.3812\n            Log-Likelihood:    -615.0616\n -------------------------------------------------------------\n    Summary statistics:\n -------------------------------------------------------------\n         Missing data:      \n         Q( 1):       2.6992         Q( 4):       5.5316\n         Q( 8):       9.3030         Q(12):      13.8241\n   Bera-Jarque:       0.2038       P-value:       0.9031\n       H(  34):       0.8429       P-value:       0.6211\n   Outliers (&gt;2.7 ES):     1\n         Q( 1):       1.9887         Q( 4):       2.4948\n         Q( 8):       6.2509         Q(12):      11.4215\n   Bera-Jarque:       0.7522       P-value:       0.6865\n       H(  34):       0.8429       P-value:       0.6211\n -------------------------------------------------------------\n\n\n\n\n\nSummary statistics:\n==================\n                       Serie 1\nData points:          84.00000\nMissing:               0.00000\nMinimum:              -3.04161\n1st quartile:         -0.61269\nMean:                  0.07274\nP(Mean = 0):           0.49902\nMedian:                0.02827\n3rd quartile:          0.64145\nMaximum:               2.49036\nInterquartile range:   1.25414\nRange:                 5.53197\nSatandard deviation:   0.98186\nVariance:              0.96405\nSkewness:             -0.08521\nKurtosis:              0.40730\nAutocorrelation tests:\n=====================\n     SACF sa     LB p.val  SPACF sp\n1   0.194  .  3.290 0.070  0.194  .\n2   0.027  .  3.353 0.187 -0.011  .\n3  -0.072  .  3.820 0.282 -0.078  .\n4  -0.289  - 11.383 0.023 -0.272  -\n5  -0.147  . 13.355 0.020 -0.048  .\n6  -0.025  . 13.414 0.037  0.016  .\n7  -0.072  . 13.904 0.053 -0.107  .\n8   0.163  . 16.427 0.037  0.122  .\n9  -0.028  . 16.504 0.057 -0.143  .\n10 -0.067  . 16.944 0.076 -0.067  .\n11 -0.050  . 17.190 0.102 -0.069  .\n12 -0.014  . 17.209 0.142  0.063  .\n13  0.052  . 17.481 0.178  0.030  .\n14  0.032  . 17.586 0.226 -0.063  .\n15 -0.066  . 18.045 0.260 -0.091  .\n16  0.016  . 18.072 0.320  0.007  .\n17 -0.006  . 18.077 0.384  0.035  .\n18  0.043  . 18.283 0.437  0.052  .\n19 -0.031  . 18.387 0.497 -0.094  .\n20 -0.085  . 19.195 0.509 -0.113  .\n21 -0.038  . 19.358 0.562 -0.024  .\nGaussianity tests:\n=================\n\n    Shapiro-Wilk normality test\n\ndata:  x\nW = 0.98857, p-value = 0.6721\n\nRatio of variance tests:\n=======================\n Portion_of_data F_statistic p.value\n         0.33333      1.5394  0.2686\n\n\n\n\n\n\n\n\nrm(list = ls())\nlibrary(UComp)\nlibrary(ggplot2)\n# Gráfico\nautoplot(Nile)\n# Entrada tipo escalón\nn = length(Nile)\nu = rep(0, n)\nu[29 : n] = 1\n# Identificando modelo PTS\nm = PTS(Nile, u=u, verbose = TRUE)\n# Diagnóstico\nplot(tail(m$comp, 99))\ntests(m)\n\n\n\n\nfrom UComp import *\n# Gráfico\nplt.plot(Nile)\n# Entrada tipo escalón\nn = Nile.shape[0]\nu = np.zeros(n)\nu[28:] = 1\n# Identificando modelo PTS\nm = PTS(Nile, u=u, verbose=True)\n# Diagnóstico\nm.plot()\ntests(m)\n\n\n\n\nclear all\nload data\n% Gráfico\nplot(Nile)\n% Entrada tipo escalón\nn = length(Nile);\nu = repmat(0, n, 1);\nu(29 : end) = 1;\n% Identificando modelo PTS\nm = PTS(Nile, 1, u=u, verbose=true);\n% Diagnóstico\nPTSvalidate(m);\nstackedplot(m)\ntoolTEST(m.v)\n\n\n\n\n\n5.6.2 Ruidos con color\nA lo largo de todo el capítulo (y el anterior) se ha considerado que el ruido \\(\\epsilon_t\\) es ruido blanco. Cabe preguntarse qué sucede en la práctica cuando se realizan los tests de diagnóstico y se descubre que el ruido estimado en un caso concreto está correlacionado. Esto querría decir que el mejor modelo dentro de la familia del suavizado exponencial deja estructura por aprovechar para la predicción. este modelo sería superable por un modelo que incluyera dicha autocorrelación.\nLa respuesta puede ser muy variada, se podría concluir que los modelos de suavizado exponencial no son capaces de aprovechar toda la información de la serie y probar con otra familia de modelos diferente. Pero sin abandonar los modelos de suavizado exponencial aún cabe la posibilidad de extender el modelo para incluir modelos ‘coloreados’ del ruido, por ejemplo, ruidos que siguen un modelo \\(ARMA(p,q)\\). Este modelo tiene la siguiente formulación\n\\[\n\\hat{\\epsilon}_t=\\phi_1 \\hat{\\epsilon}_{t-1} + \\phi_2 \\hat{\\epsilon}_{t-2}+\\dots+\\phi_p \\hat{\\epsilon}_{t-p}+a_t+\\theta_1 a_{t-1}+\\theta_2 a_{t-2}+\\dots+\\theta_q a_{t-q}\n\\]\ndonde \\(a_t\\) sí es necesariamente ruido blanco. Es decir, el modelo ARMA está blanqueando los residuos, puesto que da como residuo un ruido blanco (\\(a_t\\)). El modelo ARMA relaciona la variable con \\(p\\) retardos de su pasado y \\(q\\) retardos de un ruido blanco. Se puede intuir que dicho modelo recoge la autocorrelación que pudo quedar en \\(\\hat{\\epsilon}_t\\). Esta hipótesis se debe comprobar nuevamente con los contrastes de hipótesis sobre este ruido blanco.\nEl modelo más general incluyendo esta versión para el ruido se puede escribir en forma de espacio de los estados y se podrían utilizar los mismos procedimientos para estimar y predecir el modelo completo. Naturalmente, el vector de parámetros a estimar aumenta en \\(p+q\\) parámetros adicionales.\n\n\nResuelve\nSolución\n\n\n\nComprueba que la forma de espacio de los estados de un modelo \\(ARMA(3, 2)\\) es\n\\[\n\\begin{array}{rl}\nx_t=\\left[ \\begin{array}{c} x_{1,t} \\\\ x_{2,t} \\\\ x_{3,t} \\end{array} \\right] = & \\left[ \\begin{array}{ccc} \\phi_1 & 1 & 0 \\\\\n                           \\phi_2 & 0 & 1 \\\\\n                           \\phi_3 & 0 & 0   \n\\end{array} \\right]\n\\left[ \\begin{array}{c} x_{1,t-1} \\\\ x_{2,t-1} \\\\ x_{3,t-1} \\end{array} \\right]+\n\\left[ \\begin{array}{c} 0 \\\\ \\theta_1 \\\\ \\theta_2  \\end{array} \\right] \\epsilon_t \\\\\ny_t = & \\left[ \\begin{array}{ccc} 1 & 0 & 0  \\end{array} \\right] x_t+ \\epsilon_t\n\\end{array}\n\\]\n\n\nLas ecuaciones de transición por separado son las siguientes:\n\\[\n\\begin{array}{l}\nx_{1,t}=\\phi_1 x_{1,t-1} + x_{2,t-1} \\\\\nx_{2,t}=\\phi_2 x_{1,t-1} + x_{3,t-1} + \\theta_1 \\epsilon_t \\\\\nx_{3,t}=\\phi_3 x_{1,t-1} + \\theta_2 \\epsilon_t \\\\\n\\end{array}\n\\]\nSi sustituimos la tercera en la expresión de \\(x_{2,t-1}\\) tenemos que\n\\[\nx_{2,t}=\\phi_2 x_{1,t-1} + \\phi_3 x_{1,t-2} + \\theta_2 \\epsilon_{t-1} + \\theta_1 \\epsilon_t \\\\\n\\]\nSi ahora sustituimos esta expresión en \\(t-1\\) en la primera ecuación tenemos\n\\[\nx_{1,t}=\\phi_1 x_{1,t-1} + \\phi_2 x_{1,t-2} + \\phi_3 x_{1,t-3} + \\theta_2 \\epsilon_{t-2} + \\theta_1 \\epsilon_{t-1}\n\\]\nFinalmente, la ecuación de observación indica que \\(y_t=x_{1,t}+\\epsilon_t\\), de donde se obtiene que \\(y_t\\) sigue el modelo ARMA(3,2) propuesto.\n\n\n\n\n\nResuelve\nSolución\nR\nPython\nMATLAB/Octave\n\n\n\nEl código siguiente en R simula un nivel constante de 100 con un ruido añadido que sigue un modelo ARMA(1,1). Represéntalo con tsDisplay. ¿Hay algo que te llame la atención? Prueba a estimar un modelo de suavizado exponencial (PTS), muestra la estimación, representa los componentes y comprueba si el ruido estimado es ruido blanco (tests).\n\nrm(list = ls())\ny = armaFilter(c(1,0.8), c(1, -0.8), rnorm(300)) + 100\n\nRepite la estimación forzando un ruido coloreado con la entrada armaIdent. Comprueba de nuevo que el modelo es adecuado y representa la serie con sus predicciones 12 periodos hacia adelante.\n\n\nSe observa que hay una caída en los valores de la ACF y de la PACF (en este caso, alternando signo). Este patrón se corresponde con un modelo ARMA(1,1). La estimación habitual y el test de residuos sería el siguiente\n\n\n-------------------------------------------------------------\n  Box-Cox lambda: 1.00\n  Model: (N,N,N)\n  Periods: \n  Q-Newton: Gradient convergence\n  (*)  concentrated out parameters\n -------------------------------------------------------------\n                      Param   asymp.s.e.        |T|     |Grad| \n -------------------------------------------------------------\n        Level:       1.4840*  \n -------------------------------------------------------------\n   AIC:       3.2367   BIC:       3.2613   AICc:       3.2367\n            Log-Likelihood:    -483.4980\n -------------------------------------------------------------\n    Summary statistics:\n -------------------------------------------------------------\n         Missing data:      \n         Q( 1):      39.3469         Q( 4):      76.9112\n         Q( 8):      78.8254         Q(12):      80.7317\n   Bera-Jarque:       3.5386       P-value:       0.1705\n       H( 100):       0.6381       P-value:       0.0256\n   Outliers (&gt;2.7 ES):     1\n         Q( 1):      37.7571         Q( 4):      74.1215\n         Q( 8):      76.8516         Q(12):      78.7351\n   Bera-Jarque:       5.0787       P-value:       0.0789\n       H( 100):       0.6462       P-value:       0.0301\n -------------------------------------------------------------\n\n\n\n\nSummary statistics:\n==================\n                        Serie 1\nData points:          284.00000\nMissing:                0.00000\nMinimum:               -3.43443\n1st quartile:          -0.98215\nMean:                  -0.01237\nP(Mean = 0):            0.86599\nMedian:                 0.02707\n3rd quartile:           0.93094\nMaximum:                3.05530\nInterquartile range:    1.91309\nRange:                  6.48973\nSatandard deviation:    1.23405\nVariance:               1.52287\nSkewness:               0.01696\nKurtosis:              -0.56212\nAutocorrelation tests:\n=====================\n     SACF sa     LB p.val  SPACF sp\n1   0.357  + 36.663     0  0.357  +\n2  -0.245  - 53.928     0 -0.427  -\n3  -0.194  - 64.860     0  0.116  .\n4  -0.150  - 71.385     0 -0.287  -\n5  -0.075  . 73.038     0  0.096  .\n6  -0.007  . 73.051     0 -0.174  -\n7   0.005  . 73.059     0  0.061  .\n8  -0.010  . 73.090     0 -0.137  -\n9  -0.064  . 74.293     0 -0.034  .\n10 -0.060  . 75.354     0 -0.077  .\n11 -0.010  . 75.382     0 -0.022  .\n12  0.054  . 76.263     0  0.023  .\n13  0.027  . 76.474     0 -0.096  .\n14 -0.103  . 79.682     0 -0.110  .\n15 -0.084  . 81.835     0 -0.017  .\n16  0.046  . 82.486     0  0.002  .\n17  0.070  . 83.986     0 -0.022  .\n18  0.013  . 84.038     0 -0.051  .\n19 -0.045  . 84.661     0 -0.069  .\n20  0.021  . 84.795     0  0.089  .\n21  0.095  . 87.559     0 -0.004  .\n22 -0.002  . 87.560     0 -0.049  .\n23 -0.037  . 87.986     0  0.021  .\n24  0.015  . 88.052     0 -0.014  .\n25 -0.032  . 88.367     0 -0.049  .\nGaussianity tests:\n=================\n\n    Shapiro-Wilk normality test\n\ndata:  x\nW = 0.99217, p-value = 0.1394\n\nRatio of variance tests:\n=======================\n Portion_of_data F_statistic p.value\n         0.33333      0.6292  0.0266\n\n\n\n\n\nEl modelo óptimo es un PTS(N,N,N), es decir, está detectando correctamente que no hay estacionalidad y además no hay tendencia (solo un nivel). Pero los residuos no son ruido blanco, por lo que tiene sentido añadir la posibilidad de ruido coloreado. Llama la atención, no obstante, que el nivel que se obtiene no es un nivel constante, sino que replica la serie original, esta es evidencia adicional de que el modelo no es correcto, pero para realizar esta observación estamos utilizando “información privilegiada”, puesto que sabemos cuál es el modelo correcto porque es un modelo simulado. En situaciones reales en las que no conocemos el modelo verdadero no hay posibilidad de llegar a esta conclusión.\nRepite la estimación forzando un ruido coloreado con la entrada armaIdent. Comprueba de nuevo que el modelo es adecuado y representa la serie con sus predicciones 12 periodos hacia adelante.\n\n\n-------------------------------------------------------------\n  Box-Cox lambda: 1.00\n  Model: (A,N,N)\n  Periods: \n  Q-Newton: Function convergence\n  (*)  concentrated out parameters\n -------------------------------------------------------------\n                      Param   asymp.s.e.        |T|     |Grad| \n -------------------------------------------------------------\n    Irregular:       0.8599*  \n        AR(1):      -0.7575       0.0386    19.6260   1.41e-05\n        MA(1):       0.8267       0.0294    28.0890   1.58e-05\n        Const:     100.2747       0.3981   251.8779   1.07e-06\n -------------------------------------------------------------\n   AIC:       2.7140   BIC:       2.7510   AICc:       2.7140\n            Log-Likelihood:    -404.1003\n -------------------------------------------------------------\n    Summary statistics:\n -------------------------------------------------------------\n         Missing data:      \n         Q( 1):       0.9783         Q( 4):       2.0162\n         Q( 8):       7.0215         Q(12):       8.1166\n   Bera-Jarque:      12.4966       P-value:       0.0019\n       H( 100):       0.9302       P-value:       0.7181\n   Outliers (&gt;2.7 ES):     3\n         Q( 1):       0.0544         Q( 4):       0.8158\n         Q( 8):       3.5379         Q(12):       4.8201\n   Bera-Jarque:       6.2617       P-value:       0.0437\n       H( 100):       0.8403       P-value:       0.3857\n -------------------------------------------------------------\n\n\n\n\nSummary statistics:\n==================\n                        Serie 1\nData points:          284.00000\nMissing:                0.00000\nMinimum:               -2.39479\n1st quartile:          -0.71441\nMean:                   0.00902\nP(Mean = 0):            0.87189\nMedian:                -0.00310\n3rd quartile:           0.71279\nMaximum:                2.78063\nInterquartile range:    1.42720\nRange:                  5.17541\nSatandard deviation:    0.94200\nVariance:               0.88736\nSkewness:               0.02870\nKurtosis:              -0.57083\nAutocorrelation tests:\n=====================\n     SACF sa     LB p.val  SPACF sp\n1   0.021  .  0.121 0.728  0.021  .\n2  -0.040  .  0.578 0.749 -0.040  .\n3   0.002  .  0.578 0.901  0.003  .\n4  -0.017  .  0.666 0.955 -0.019  .\n5   0.002  .  0.667 0.985  0.003  .\n6   0.062  .  1.791 0.938  0.061  .\n7  -0.005  .  1.797 0.970 -0.007  .\n8   0.064  .  2.992 0.935  0.069  .\n9  -0.062  .  4.141 0.902 -0.067  .\n10  0.013  .  4.189 0.938  0.024  .\n11 -0.013  .  4.236 0.962 -0.020  .\n12  0.048  .  4.926 0.960  0.050  .\n13  0.036  .  5.308 0.968  0.030  .\n14 -0.070  .  6.766 0.943 -0.076  .\n15 -0.070  .  8.246 0.914 -0.056  .\n16  0.065  .  9.521 0.890  0.057  .\n17  0.016  .  9.603 0.919  0.022  .\n18  0.030  .  9.882 0.936  0.020  .\n19 -0.040  . 10.364 0.943 -0.043  .\n20 -0.011  . 10.402 0.960 -0.005  .\n21  0.098  . 13.366 0.895  0.108  .\n22 -0.033  . 13.694 0.912 -0.035  .\n23 -0.042  . 14.232 0.920 -0.037  .\n24  0.029  . 14.487 0.935  0.007  .\n25 -0.024  . 14.671 0.949 -0.016  .\nGaussianity tests:\n=================\n\n    Shapiro-Wilk normality test\n\ndata:  x\nW = 0.99101, p-value = 0.07972\n\nRatio of variance tests:\n=======================\n Portion_of_data F_statistic p.value\n         0.33333      0.7271  0.1262\n\n\n\n\n\nEl modelo estimado ahora está correctamente identificado como un ARMA(1,1).\n\n\n\n# Identificación asumiendo ruido blanco\nm = PTS(y)\nprint(m)\ntests(m)\n# Identificación asumiendo ruido ARMA\nm = PTS(y, armaIdent = TRUE)\nprint(m)\ntests(m)\n\n\n\n\nfrom UComp import *\n# Simulación\ny = armaFilter([1,0.8], [1, -0.8], np.random.normal(0, 1, 300)) + 100\n# Identificación asumiendo ruido blanco\nm = PTS(y, s=1, verbose=True)\ntests(m)\n# Identificación asumiendo ruido ARMA\nm = PTS(y, s=1, armaIdent = True, verbose=True)\ntests(m)\n\n\n\n\nclear all\n% Simulación\ny = filter([1 0.8], [1 -0.8], randn(300, 1)) + 100;\n% Identificación asumiendo ruido blanco\nm = PTS(y, 1, verbose=true);\ntoolTEST(m.v)\n% Identificación asumiendo ruido ARMA\nm = PTS(y, 1, armaIdent=true, verbose=true);\ntoolTEST(m.v)\n\n\n\n\n\n\n\nResuelve\nSolución\nR\nPython\nMATLAB/Octave\n\n\n\nEl código siguiente simula un nivel constante de 100 con un ruido añadido que sigue un modelo ARMA(1,0) o AR(1). Represéntalo con tsDisplay. Estima un modelo de suavizado exponencial y comprueba si es correcto. Repite la estimación forzando la estimación de ruido coloreado con la entrada armaIdent. Comprueba de nuevo que el modelo es adecuado y representa la serie con sus predicciones 12 periodos hacia adelante.\n\nrm(list = ls())\ny = armaFilter(1, c(1, -0.8), rnorm(300)) + 100\n\n\n\n\n\n\n\n\nSe observa que hay una caída en los valores de la ACF y un solo valor importante en la PACF. Este patrón se corresponde con un modelo AR(1). La estimación habitual y el test de residuos sería el siguiente\n\n\n-------------------------------------------------------------\n  Box-Cox lambda: 1.00\n  Model: (N,N,N)\n  Periods: \n  Q-Newton: Gradient convergence\n  (*)  concentrated out parameters\n -------------------------------------------------------------\n                      Param   asymp.s.e.        |T|     |Grad| \n -------------------------------------------------------------\n        Level:       1.0981*  \n -------------------------------------------------------------\n   AIC:       2.9375   BIC:       2.9622   AICc:       2.9375\n            Log-Likelihood:    -438.6256\n -------------------------------------------------------------\n    Summary statistics:\n -------------------------------------------------------------\n         Missing data:      \n         Q( 1):       0.7072         Q( 4):      10.2280\n         Q( 8):      19.1433         Q(12):      25.1571\n   Bera-Jarque:       2.9872       P-value:       0.2246\n       H( 100):       0.9984       P-value:       0.9937\n   Outliers (&gt;2.7 ES):     2\n         Q( 1):       0.4642         Q( 4):      12.4293\n         Q( 8):      19.1149         Q(12):      27.1592\n   Bera-Jarque:       0.3583       P-value:       0.8360\n       H( 100):       0.9827       P-value:       0.9305\n -------------------------------------------------------------\n\n\n\n\nSummary statistics:\n==================\n                        Serie 1\nData points:          284.00000\nMissing:                0.00000\nMinimum:               -3.31307\n1st quartile:          -0.61751\nMean:                  -0.00052\nP(Mean = 0):            0.99340\nMedian:                 0.01915\n3rd quartile:           0.63748\nMaximum:                2.80384\nInterquartile range:    1.25499\nRange:                  6.11691\nSatandard deviation:    1.05422\nVariance:               1.11139\nSkewness:              -0.16909\nKurtosis:               0.34211\nAutocorrelation tests:\n=====================\n     SACF sa     LB p.val  SPACF sp\n1  -0.050  .  0.726 0.394 -0.050  .\n2  -0.131  -  5.661 0.059 -0.134  -\n3  -0.083  .  7.633 0.054 -0.099  .\n4  -0.073  .  9.167 0.057 -0.105  .\n5  -0.116  . 13.053 0.023 -0.160  -\n6  -0.086  . 15.200 0.019 -0.154  -\n7  -0.036  . 15.579 0.029 -0.133  -\n8   0.079  . 17.402 0.026 -0.024  .\n9   0.002  . 17.404 0.043 -0.086  .\n10  0.100  . 20.382 0.026  0.039  .\n11  0.003  . 20.385 0.040 -0.039  .\n12 -0.069  . 21.801 0.040 -0.090  .\n13  0.029  . 22.057 0.054  0.013  .\n14 -0.035  . 22.425 0.070 -0.053  .\n15 -0.045  . 23.043 0.083 -0.050  .\n16  0.020  . 23.168 0.109 -0.006  .\n17 -0.023  . 23.327 0.139 -0.058  .\n18 -0.021  . 23.467 0.173 -0.074  .\n19 -0.023  . 23.623 0.211 -0.079  .\n20  0.033  . 23.954 0.244 -0.032  .\n21  0.025  . 24.153 0.286 -0.040  .\n22  0.051  . 24.946 0.300  0.028  .\n23  0.007  . 24.963 0.352 -0.019  .\n24 -0.056  . 25.931 0.357 -0.080  .\n25  0.042  . 26.492 0.382  0.042  .\nGaussianity tests:\n=================\n\n    Shapiro-Wilk normality test\n\ndata:  x\nW = 0.992, p-value = 0.1283\n\nRatio of variance tests:\n=======================\n Portion_of_data F_statistic p.value\n         0.33333      1.0001  0.9996\n\n\n\n\n\nEl modelo óptimo es un PTS(N,N,N), es decir, está detectando correctamente que no hay estacionalidad y además no hay tendencia (solo un nivel). Pero los residuos PARECEN ruido blanco, por lo que un analista ingenuo podría concluir que el modelo es correcto. Sin embargo, se puede ver que el nivel se parece demasiado a la serie temporal, lo cual es indicio de error de especificación. En particular hay un problema de identificación,\nA continuación repite la estimación forzando la estimación de ruido coloreado con la entrada armaIdent. Comprueba de nuevo que el modelo es adecuado y representa la serie con sus predicciones 12 periodos hacia adelante.\n\n\n-------------------------------------------------------------\n  Box-Cox lambda: 1.00\n  Model: (A,N,N)\n  Periods: \n  Q-Newton: Function convergence\n  (*)  concentrated out parameters\n -------------------------------------------------------------\n                      Param   asymp.s.e.        |T|     |Grad| \n -------------------------------------------------------------\n    Irregular:       0.9614*  \n        AR(1):      -0.7519       0.0379    19.8608   1.74e-05\n        Const:     100.0320       0.2256   443.4388   1.18e-05\n -------------------------------------------------------------\n   AIC:       2.8114   BIC:       2.8361   AICc:       2.8114\n            Log-Likelihood:    -419.7124\n -------------------------------------------------------------\n    Summary statistics:\n -------------------------------------------------------------\n         Missing data:      \n         Q( 1):       1.2048         Q( 4):       2.7767\n         Q( 8):       8.8616         Q(12):      15.5452\n   Bera-Jarque:       0.1413       P-value:       0.9318\n       H( 100):       0.9636       P-value:       0.8531\n   Outliers (&gt;2.7 ES):     2\n         Q( 1):       1.6473         Q( 4):       4.3460\n         Q( 8):       9.1062         Q(12):      18.2479\n   Bera-Jarque:       1.1780       P-value:       0.5549\n       H( 100):       0.8837       P-value:       0.5377\n -------------------------------------------------------------\n\n\n\n\nSummary statistics:\n==================\n                        Serie 1\nData points:          284.00000\nMissing:                0.00000\nMinimum:               -3.06088\n1st quartile:          -0.60833\nMean:                   0.01997\nP(Mean = 0):            0.73310\nMedian:                -0.03497\n3rd quartile:           0.66593\nMaximum:                2.74593\nInterquartile range:    1.27426\nRange:                  5.80681\nSatandard deviation:    0.98589\nVariance:               0.97199\nSkewness:               0.03225\nKurtosis:              -0.05838\nAutocorrelation tests:\n=====================\n     SACF sa     LB p.val  SPACF sp\n1   0.062  .  1.100 0.294  0.062  .\n2  -0.039  .  1.541 0.463 -0.043  .\n3  -0.020  .  1.658 0.646 -0.015  .\n4  -0.028  .  1.889 0.756 -0.028  .\n5  -0.077  .  3.597 0.609 -0.075  .\n6  -0.054  .  4.460 0.615 -0.048  .\n7  -0.009  .  4.483 0.723 -0.010  .\n8   0.095  .  7.132 0.522  0.089  .\n9   0.029  .  7.376 0.598  0.012  .\n10  0.113  . 11.131 0.347  0.112  .\n11  0.022  . 11.268 0.421  0.006  .\n12 -0.049  . 11.974 0.448 -0.040  .\n13  0.033  . 12.296 0.504  0.057  .\n14 -0.027  . 12.522 0.564 -0.021  .\n15 -0.039  . 12.977 0.604 -0.014  .\n16  0.018  . 13.078 0.667  0.024  .\n17 -0.020  . 13.194 0.723 -0.030  .\n18 -0.017  . 13.287 0.774 -0.034  .\n19 -0.016  . 13.369 0.819 -0.023  .\n20  0.035  . 13.750 0.843  0.027  .\n21  0.030  . 14.037 0.868  0.014  .\n22  0.052  . 14.874 0.868  0.065  .\n23  0.011  . 14.913 0.898 -0.001  .\n24 -0.047  . 15.609 0.902 -0.051  .\n25  0.036  . 16.026 0.914  0.065  .\nGaussianity tests:\n=================\n\n    Shapiro-Wilk normality test\n\ndata:  x\nW = 0.99652, p-value = 0.7915\n\nRatio of variance tests:\n=======================\n Portion_of_data F_statistic p.value\n         0.33333       1.056  0.7932\n\n\n\n\n\nDe nuevo el modelo se identifica correctamente como un AR(1).\n\n\n\nrm(list = ls())\n# Simulación\nset.seed(150)\ny = armaFilter(1, c(1, -0.8), rnorm(300)) + 100\ntsDisplay(y)\n# Identificación asumiendo ruido blanco\nm = PTS(y, verbose = TRUE)\ntests(m)\n# Identificación asumiendo ruido ARMA\nm = PTS(y, armaIdent = TRUE, verbose=TRUE)\ntests(m)\n\n\n\n\nfrom UComp import *\n# Simulación\ny = armaFilter([1], [1, -0.8], np.random.normal(0, 1, 300)) + 100\ntsDisplay(y)\n# Identificación asumiendo ruido blanco\nm = PTS(y, s=1, verbose=True)\ntests(m)\n# Identificación asumiendo ruido ARMA\nm = PTS(y, s=1, armaIdent=True, verbose=True)\ntests(m)\n\n\n\n\nclear all\n% Simulación\ny = filter(1, [1 -0.8], randn(300, 1)) + 100;\ntoolTEST(y)\n% Identificación asumiendo ruido blanco\nm = PTS(y, 1, verbose=true);\ntoolTEST(m.v)\n% Identificación asumiendo ruido ARMA\nm = PTS(y, 1, armaIdent=true, verbose=true);\ntoolTEST(m.v)"
  },
  {
    "objectID": "04-etsMSOE.html#dos-ejemplos-completos",
    "href": "04-etsMSOE.html#dos-ejemplos-completos",
    "title": "\n5  Suavizado Exponencial con mútiples ruidos\n",
    "section": "\n5.7 Dos ejemplos completos",
    "text": "5.7 Dos ejemplos completos\n\n5.7.1 Análisis completo de la entrada de turistas por aire en España\n\n\nResuelve\nSolución\nR\nPython\nMATLAB/Octave\n\n\n\nYa estamos en condiciones de realizar un análisis completo de la serie de pasajeros de avión, que puede hacerse siguiendo este ejemplo. El ejercicio de predicción que se propone es para el año 2019 de la serie de pasajeros, que es el último año después de las profundas perturbaciones debidas a la pandemia. Luego extenderemos al resto de la muestra para ver qué pérdida de viajeros anuncia el modelo de suavizado exponencial en comparación con los métodos tipo naive.\nEs conveniente siempre limpiar la memoria y cargar los paquetes necesarios. Genera una serie nueva (por ejemplo, x) que sean los pasajeros hasta finales de 2018. Realiza la predicción para 2019 con cuatro modelos: naive, naive estacional, media anual, PTS (en logaritmos) y PTS con la muestra comenzando en 1995 (en logaritmos). Indica si el modelo PTS es aceptable mostrando los parámetros, los componentes y los tests. ¿Qué modelo es el que mejor predice el año 2019? Representa gráficamente todas las predicciones con los valores reales.\nEstima con los cinco métodos la pérdida de pasajeros de avión desde marzo de 2020.\n\n\n\n\n-------------------------------------------------------------\n  Box-Cox lambda: 1.00\n  Model: (A,Ad,D)\n  Periods:  12.0 /  6.0 /  4.0 /  3.0 /  2.4 /  2.0\n  Q-Newton: Function convergence\n  (*)  concentrated out parameters\n -------------------------------------------------------------\n                      Param   asymp.s.e.        |T|     |Grad| \n -------------------------------------------------------------\n      Damping:       0.9536       0.0059   161.8644   1.61e-06\n        Level:     2.15e-05     2.04e-05     1.0550   3.64e-07\n        Slope:     6.95e-06     7.35e-06     0.9455   8.23e-07\n   Seas(12.0):     2.20e-05     1.23e-05     1.7857   2.65e-07\n    Seas(6.0):     2.08e-06     1.66e-05     0.1251   9.23e-07\n    Seas(4.0):     1.29e-06     8.49e-06     0.1524   2.73e-06\n    Seas(3.0):     1.39e-06     1.01e-05     0.1366   3.69e-07\n    Seas(2.4):     2.46e-07     3.96e-06     0.0621   1.06e-06\n    Seas(2.0):     1.86e-07     4.98e-06     0.0373   2.64e-07\n    Irregular:     3.31e-04*  \n -------------------------------------------------------------\n   AIC:      -3.4856   BIC:      -3.1931   AICc:      -3.4717\n            Log-Likelihood:     524.9308\n -------------------------------------------------------------\n    Summary statistics:\n -------------------------------------------------------------\n         Missing data:      \n         Q( 1):       0.1184         Q( 4):       0.6482\n         Q( 8):       3.8427         Q(12):       9.6375\n   Bera-Jarque:      25.5935       P-value:       0.0000\n       H(  92):       0.8866       P-value:       0.5649\n   Outliers (&gt;2.7 ES):     6\n         Q( 1):       0.9178         Q( 4):       1.6832\n         Q( 8):       6.0620         Q(12):      11.2907\n   Bera-Jarque:       2.2358       P-value:       0.3270\n       H(  90):       0.8361       P-value:       0.3976\n -------------------------------------------------------------\n\n\n\n\n\nSummary statistics:\n==================\n                        Serie 1\nData points:          284.00000\nMissing:               12.00000\nMinimum:               -0.10471\n1st quartile:          -0.01583\nMean:                   0.00318\nP(Mean = 0):            0.10674\nMedian:                 0.00389\n3rd quartile:           0.02117\nMaximum:                0.12575\nInterquartile range:    0.03700\nRange:                  0.23046\nSatandard deviation:    0.03237\nVariance:               0.00105\nSkewness:              -0.10317\nKurtosis:               1.50795\nAutocorrelation tests:\n=====================\n     SACF sa     LB p.val  SPACF sp\n1  -0.005  .  0.007 0.931 -0.005  .\n2   0.000  .  0.007 0.996  0.000  .\n3  -0.007  .  0.023 0.999 -0.007  .\n4  -0.032  .  0.308 0.989 -0.032  .\n5  -0.008  .  0.326 0.997 -0.008  .\n6   0.033  .  0.625 0.996  0.033  .\n7  -0.046  .  1.222 0.990 -0.046  .\n8   0.071  .  2.660 0.954  0.070  .\n9  -0.060  .  3.678 0.931 -0.060  .\n10 -0.114  .  7.387 0.688 -0.114  .\n11  0.052  .  8.171 0.698  0.052  .\n12 -0.030  .  8.422 0.751 -0.029  .\n13  0.003  .  8.425 0.815  0.001  .\n14 -0.063  .  9.555 0.794 -0.077  .\n15  0.049  . 10.260 0.803  0.062  .\n16 -0.087  . 12.439 0.713 -0.094  .\n17  0.012  . 12.482 0.770  0.007  .\n18 -0.043  . 13.036 0.789 -0.029  .\n19 -0.073  . 14.619 0.746 -0.102  .\n20 -0.132  - 19.780 0.472 -0.140  -\n21  0.034  . 20.120 0.514  0.032  .\n22  0.065  . 21.391 0.497  0.076  .\n23  0.087  . 23.671 0.422  0.057  .\n24  0.003  . 23.674 0.480  0.000  .\n25 -0.028  . 23.906 0.525 -0.020  .\nGaussianity tests:\n=================\n\n    Shapiro-Wilk normality test\n\ndata:  x\nW = 0.97805, p-value = 0.0003361\n\nRatio of variance tests:\n=======================\n Portion_of_data F_statistic p.value\n         0.33333      1.0986  0.6584\n\n\n\n\n\n\n\n                        ME     RMSE       MAE        MPE     PRMSE      MAPE\nnaive           -5053.6817 6714.306 5430.9235 -19.638589 25.959385 22.034494\nnaiveS          -1059.6768 1118.936 1059.6768  -4.941171  5.236211  4.941171\nexp(m$yFor)       885.6997 1073.391  885.6997   3.735341  4.303053  3.735341\nexp(m1995$yFor)   664.8557  793.085  664.8557   2.896275  3.289618  2.896275\n                    sMAPE     MASE     RelMAE Theil's U\nnaive           26.008694 9.334455 0.19049909 1.9629025\nnaiveS           5.082549 1.821330 0.03717001 0.3959328\nexp(m$yFor)      3.645267 1.522305 0.03106746 0.3253726\nexp(m1995$yFor)  2.843283 1.142728 0.02332097 0.2487424\n\n\n\n\n\nLos modelos PTS son apropiados, pero las predicciones, a juzgar por las métricas de error son peores que las de los modelos sencillos, incluso los que no tienen estacionalidad.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Borrando memoria\nrm(list = ls())\n# Cargando librerías\nlibrary(UComp)\nlibrary(ggplot2)\n# Seleccionando muestras\nx = window(airpas, end = c(2018, 12))\nx1995 = window(airpas, start = 1995, end = c(2018, 12))\n# Horizonte y periodo estacional\nh = 12   # Horizonte\nm = 12   # Observaciones por año\n# Estimando métodos/modelos\nnaive = ts(rep(tail(x, 1), h), start = 2019, frequency = m)\nnaiveS = ts(tail(x, h), start = 2019, frequency = m)\nmediaAnual = ts(rep(mean(tail(x, m)), h), start = 2019, frequency = m)\nm = PTS(log(x), h = h)\nm1995 = PTS(log(x1995), h = h)\n# Mostrando modelo\nprint(m1995)\n# Mostrando componentes\nplot(m1995)\n# TEsts de innovaciones\ntests(m1995)\n# Todas las predicciones en una sola matriz\npredicciones = cbind(naive, naiveS, mediaAnual, exp(m$yFor), exp(m1995$yFor))\n# Precisión de predicciones\nAccuracy(predicciones,  airpas)\n# Gráfico de predicciones\nautoplot(tail(airpas, 120)) + autolayer(predicciones)\n\n\n\n\nfrom UComp import *\n# Seleccionando muestras\nx = window(airpas, end=\"2018-12-31\")\nx1995 = window(airpas, start=\"1995-01-31\" , end=\"2018-12-31\")\n# Horizonte y periodo estacional\nh = 12   # Horizonte\nm = 12   # Observaciones por año\n# Estimando métodos/modelos\nm = PTS(np.log(x), h = h, verbose=True)\nnaive = ts(x[-1].repeat(h), start=m.yFor.index[0], freq='m')\nnaiveS = ts(x[-12:].repeat(1), start=m.yFor.index[0], freq='m')\nmediaAnual = ts(np.mean(x[-12:]).repeat(h), start=m.yFor.index[0], freq='m')\nm1995 = PTS(np.log(x1995), h=h, verbose=True)\n# Mostrando componentes\nm1995.plot()\n# TEsts de innovaciones\ntests(m1995)\n# Todas las predicciones en una sola matriz\npredicciones = pd.concat((naive, naiveS, mediaAnual, \n                          np.exp(m.yFor), np.exp(m1995.yFor)), axis=1)\n# Precisión de predicciones\nAccuracy(predicciones, airpas)\n# Gráfico de predicciones\nplt.plot(airpas[-80:])\nplt.plot(predicciones)\n\n\n\n\n% Borrando memoria\nclear all\nload data\n% Seleccionando muestras\nx = airpas(1 : 600);\nx1995 = airpas(313 : 600);\n% Horizonte y periodo estacional\nh = 12;   % Horizonte\nm = 12;   % Observaciones por año\n% Estimando métodos/modelos\nnaive = repmat(x(end), h, 1);\nnaiveS = x(end - h + 1 : end);\nmediaAnual = repmat(mean(x(end - h + 1 : end)), h, 1);\nm = PTS(log(x), 12, h=h, verbose=true);\nm1995 = PTS(log(x1995), 12, h=h, verbose=true);\n% Mostrando componentes\nstackedplot(m1995.comp);\n% TEsts de innovaciones\ntoolTEST(m1995.v)\n% Todas las predicciones en una sola matriz\npredicciones = [naive naiveS mediaAnual exp(m.yFor) exp(m1995.yFor)];\n% Precisión de predicciones\nAccuracy(predicciones,  airpas(1 : 600 + h))\n% Gráfico de predicciones\nt = (553 : length(airpas))';\nplot(t, airpas(t), (601 : 600 + h), predicciones)\n\n\n\n\n\n5.7.2 Análisis completo del Producto Interior Bruto de España\n\n\nResuelve\nSolución\nR\nPython\nMATLAB/Octave\n\n\n\nGenera predicciones del PIB para el año 2019 y 2020 con muestras que comiencen en 1995 y 2005, respectivamente. Utilizar además el método naive, naive estacional y media del último año.\n\n\n\n\n------------------------------------------------------------\n Identification of PTS models:\n------------------------------------------------------------\n    Model            AIC           BIC          AICc\n------------------------------------------------------------\n  (N,N,N):        5.7628        5.8162        5.7628\n  (A,N,N):        5.2305        5.3106        5.2305\n  (N,N,L):        3.0705        3.2575        3.0809\n  (A,N,L):        3.1585        3.3722        3.1689\n  (N,N,D):        3.1004        3.3141        3.1109\n  (A,N,D):        3.2733        3.5137        3.2942\n  (N,A,N):        5.8095        5.9163        5.8095\n  (A,A,N):        5.0949        5.2285        5.0949\n  (N,A,L):        2.4365        2.6769        2.4573\n  (A,A,L):        2.4573        2.7245        2.4782\n  (N,A,D):        2.4276        2.6948        2.4485\n  (A,A,D):        2.4485        2.7423        2.4797\n (N,Ad,N):        5.8045        5.9113        5.8045\n (A,Ad,N):        5.0804        5.2139        5.0804\n (N,Ad,L):        2.4042        2.6446        2.4250\n (A,Ad,L):        2.4250        2.6921        2.4458\n (N,Ad,D):        2.3839        2.6510        2.4047\n (A,Ad,D):        2.4047        2.6986        2.4360\n------------------------------------------------------------\n  Identification time:    0.29650 seconds\n------------------------------------------------------------\n-------------------------------------------------------------\n  Box-Cox lambda: 1.00\n  Model: (N,Ad,D)\n  Periods:   4.0 /  2.0\n  Q-Newton: Function convergence\n  (*)  concentrated out parameters\n -------------------------------------------------------------\n                      Param   asymp.s.e.        |T|     |Grad| \n -------------------------------------------------------------\n      Damping:       0.8926       0.0370    24.0958   1.34e-05\n        Level:       0.0010       0.0468     0.0217   6.09e-07\n        Slope:       0.1449*  \n    Seas(4.0):       0.0125       0.0057     2.1844   5.98e-06\n    Seas(2.0):       0.0065       0.0026     2.5498   1.68e-07\n -------------------------------------------------------------\n   AIC:       2.3839   BIC:       2.6510   AICc:       2.4047\n            Log-Likelihood:    -104.4276\n -------------------------------------------------------------\n    Summary statistics:\n -------------------------------------------------------------\n         Missing data:      \n         Q( 1):       0.0235         Q( 4):       0.2640\n         Q( 8):       4.5742         Q(12):       5.9148\n   Bera-Jarque:      20.2832       P-value:       0.0000\n       H(  32):       0.5749       P-value:       0.1226\n   Outliers (&gt;2.7 ES):     2\n         Q( 1):       3.1364         Q( 4):       3.8338\n         Q( 8):       8.8073         Q(12):      13.9930\n   Bera-Jarque:       0.4503       P-value:       0.7984\n       H(  30):       0.6023       P-value:       0.1708\n -------------------------------------------------------------\n\n\n------------------------------------------------------------\n Identification of PTS models:\n------------------------------------------------------------\n    Model            AIC           BIC          AICc\n------------------------------------------------------------\n  (N,N,N):        5.7780        5.8503        5.7780\n  (A,N,N):        5.2402        5.3487        5.2402\n  (N,N,L):        3.2970        3.5502        3.3327\n  (A,N,L):        3.5625        3.8519        3.6161\n  (N,N,D):        3.3502        3.6395        3.4037\n  (A,N,D):        3.6112        3.9367        3.6648\n  (N,A,N):        5.8628        6.0074        5.8628\n  (A,A,N):        5.2501        5.4309        5.2679\n  (N,A,L):        2.9169        3.2424        2.9704\n  (A,A,L):        2.9526        3.3143        3.0240\n  (N,A,D):        2.9197        3.2813        2.9911\n  (A,A,D):        2.9554        3.3532        3.0625\n (N,Ad,N):        5.8494        5.9941        5.8494\n (A,Ad,N):        5.2258        5.4066        5.2436\n (N,Ad,L):        2.8559        3.1814        2.9095\n (A,Ad,L):        2.8916        3.2533        2.9630\n (N,Ad,D):        2.8590        3.2207        2.9304\n (A,Ad,D):        2.8947        3.2925        3.0018\n------------------------------------------------------------\n  Identification time:    0.22267 seconds\n------------------------------------------------------------\n-------------------------------------------------------------\n  Box-Cox lambda: 1.00\n  Model: (N,Ad,L)\n  Periods: \n  Q-Newton: Function convergence\n  (*)  concentrated out parameters\n -------------------------------------------------------------\n                      Param   asymp.s.e.        |T|     |Grad| \n -------------------------------------------------------------\n      Damping:       0.8239       0.0567    14.5239   1.64e-06\n        Level:       0.0523       0.0521     1.0043   1.59e-06\n        Slope:       0.2754*  \n         Seas:       0.0558       0.0206     2.7034   7.38e-06\n -------------------------------------------------------------\n   AIC:       2.8559   BIC:       3.1814   AICc:       2.9095\n            Log-Likelihood:     -70.9652\n -------------------------------------------------------------\n    Summary statistics:\n -------------------------------------------------------------\n         Missing data:      \n         Q( 1):       0.0154         Q( 4):       2.3773\n         Q( 8):       7.2200         Q(12):       9.9895\n   Bera-Jarque:       7.1529       P-value:       0.0280\n       H(  18):       0.2404       P-value:       0.0041\n   Outliers (&gt;2.7 ES):     1\n         Q( 1):       2.3978         Q( 4):       4.8419\n         Q( 8):       7.4508         Q(12):      11.2062\n   Bera-Jarque:       1.6524       P-value:       0.4377\n       H(  18):       0.3325       P-value:       0.0245\n -------------------------------------------------------------\n\n\n-------------------------------------------------------------\n  Box-Cox lambda: 1.00\n  Model: (N,Ad,D)\n  Periods:   4.0 /  2.0\n  Q-Newton: Function convergence\n  (*)  concentrated out parameters\n -------------------------------------------------------------\n                      Param   asymp.s.e.        |T|     |Grad| \n -------------------------------------------------------------\n      Damping:       0.8926       0.0370    24.0958   1.34e-05\n        Level:       0.0010       0.0468     0.0217   6.09e-07\n        Slope:       0.1449*  \n    Seas(4.0):       0.0125       0.0057     2.1844   5.98e-06\n    Seas(2.0):       0.0065       0.0026     2.5498   1.68e-07\n -------------------------------------------------------------\n   AIC:       2.3839   BIC:       2.6510   AICc:       2.4047\n            Log-Likelihood:    -104.4276\n -------------------------------------------------------------\n    Summary statistics:\n -------------------------------------------------------------\n         Missing data:      \n         Q( 1):       0.0235         Q( 4):       0.2640\n         Q( 8):       4.5742         Q(12):       5.9148\n   Bera-Jarque:      20.2832       P-value:       0.0000\n       H(  32):       0.5749       P-value:       0.1226\n   Outliers (&gt;2.7 ES):     2\n         Q( 1):       3.1364         Q( 4):       3.8338\n         Q( 8):       8.8073         Q(12):      13.9930\n   Bera-Jarque:       0.4503       P-value:       0.7984\n       H(  30):       0.6023       P-value:       0.1708\n -------------------------------------------------------------\n\n\n\n\n\nSummary statistics:\n==================\n                       Serie 1\nData points:          88.00000\nMissing:               8.00000\nMinimum:              -2.27747\n1st quartile:         -0.30912\nMean:                  0.09936\nP(Mean = 0):           0.22094\nMedian:                0.12797\n3rd quartile:          0.50506\nMaximum:               1.83184\nInterquartile range:   0.81418\nRange:                 4.10931\nSatandard deviation:   0.72033\nVariance:              0.51887\nSkewness:             -0.66860\nKurtosis:              1.59604\nAutocorrelation tests:\n=====================\n     SACF sa     LB p.val  SPACF sp\n1  -0.018  .  0.028 0.866 -0.018  .\n2  -0.035  .  0.129 0.938 -0.035  .\n3  -0.011  .  0.139 0.987 -0.012  .\n4  -0.001  .  0.139 0.998 -0.003  .\n5  -0.185  .  3.133 0.679 -0.186  .\n6   0.073  .  3.601 0.730  0.067  .\n7   0.001  .  3.601 0.824 -0.011  .\n8  -0.087  .  4.283 0.831 -0.091  .\n9  -0.050  .  4.516 0.874 -0.053  .\n10  0.078  .  5.092 0.885  0.039  .\n11 -0.064  .  5.482 0.906 -0.047  .\n12  0.033  .  5.584 0.936  0.026  .\n13  0.066  .  6.009 0.946  0.035  .\n14 -0.072  .  6.517 0.952 -0.082  .\n15  0.110  .  7.736 0.934  0.149  .\n16  0.052  .  8.014 0.948  0.017  .\n17 -0.004  .  8.016 0.966  0.010  .\n18 -0.123  .  9.617 0.944 -0.100  .\n19  0.071  . 10.166 0.949  0.046  .\n20 -0.063  . 10.596 0.956 -0.022  .\n21 -0.001  . 10.596 0.970  0.013  .\n22  0.009  . 10.606 0.980 -0.009  .\nGaussianity tests:\n=================\n\n    Shapiro-Wilk normality test\n\ndata:  x\nW = 0.96178, p-value = 0.0173\n\nRatio of variance tests:\n=======================\n Portion_of_data F_statistic p.value\n         0.33333      0.7795  0.5381\n\n\n\n\n\n\n\n                 ME      RMSE      MAE      MPE     PRMSE     MAPE    sMAPE\nnaive      7.568453 10.914300 8.026720 7.909775 11.842121 8.313688 7.684942\nnaiveS     4.114374  9.219851 6.267516 4.578402 10.168076 6.533763 6.088110\nmediaAnual 4.114374  8.875164 6.686414 4.581301  9.692367 6.866342 6.479363\nm1995$yFor 7.731254 11.848528 7.761535 8.124600 12.936317 8.152940 7.411094\nm2005$yFor 7.035921 11.209751 7.119249 7.438447 12.293345 7.516432 6.845194\n               MASE    RelMAE Theil's U\nnaive      3.022146 0.2627953  1.338894\nnaiveS     2.359787 0.2051989  1.149623\nmediaAnual 2.517507 0.2189136  1.095838\nm1995$yFor 2.922301 0.2541132  1.462605\nm2005$yFor 2.680474 0.2330847  1.389910\n\n\n\n\n\nLos modelos PTS son apropiados, pero las predicciones, a juzgar por las métricas de error son peores que las de los modelos sencillos. No obstante, este ejercicio tiene dos problemas:\n\nEl periodo de evaluación tiene un tramo atípico, el año 2020, que es en el que las predicciones son mejores para los modelos sencillos. Sin embargo, los modelos PTS son mejores en el año 2019.\nSe está evaluando las predicciones con un único periodo de evaluación. Un modelo será superior si sistemáticamente predice mejor, no si lo hace en un solo caso. Esto se completará en el capítulo siguiente.\n\n\n\n\n\n# Limpiando memoria\nrm(list = ls())\n# Cargando librerías\nlibrary(UComp)\nlibrary(ggplot2)\n# Seleccionando muesttra\nx1995 = window(gdp, start = 1995, end = c(2018, 4))\n# Transformando variable\nx2005 = window(x1995, start = 2005)\n# Horizonte de predicción y periodo estacional\nh = 8   # Horizonte de predicción\nm = 4   # Observaciones por año\n# Estimando predicciones\nnaive = ts(rep(tail(x1995, 1), h), start = 2019, frequency = m)\nnaiveS = ts(rep(tail(x1995, m), h / m), start = 2019, frequency = m)\nmediaAnual = ts(rep(mean(tail(x1995, m)), h), start = 2019, frequency = m)\nm1995 = PTS(x1995, h=h, verbose=TRUE)\nm2005 = PTS(x2005, h=h, verbose=TRUE)\n# Muestra estimación de modelo sin logaritmos\nprint(m1995)\n# Muestra componentes\nplot(m1995)\n# Tests de modelo sin logaritmos\ntests(m1995)\n# Matriz con todas las predicciones\npredicciones = cbind(naive, naiveS, mediaAnual, m1995$yFor, m2005$yFor)\n# Precisión de predicciones\nAccuracy(predicciones, gdp)\n# Representación gráfica\nautoplot(tail(gdp, 60)) + autolayer(predicciones)\n\n\n\n\nfrom UComp import *\n# Seleccionando muesttra\nx1995 = window(gdp, end=\"2018-12-31\")\n# Transformando variable\nx2005 = window(x1995, start = \"2005-01-31\")\n# Horizonte de predicción y periodo estacional\nh = 8   # Horizonte de predicción\nm = 4   # Observaciones por año\n# Estimando predicciones\nm1995 = PTS(x1995, h=h, verbose=True)\nnaive = ts(x1995[-1].repeat(h), start=m1995.yFor.index[0], freq='q')\naux = np.hstack((x1995[-4:].values, x1995[-4:].values))\nnaiveS = ts(aux, start=m1995.index[0], freq='q')\nmediaAnual = ts(np.mean(x1995[-4:]).repeat(h), start=m1995.yFor.index[0], freq='q')\nm2005 = PTS(x2005, h=h, verbose=True)\n# Muestra estimación de modelo sin logaritmos\n# Muestra componentes\nm1995.plot()\n# Tests de modelo sin logaritmos\ntests(m1995)\n# Matriz con todas las predicciones\npredicciones = pd.concat((naive, naiveS, mediaAnual, m1995.yFor, m2005.yFor), axis=1)\n# Precisión de predicciones\nAccuracy(predicciones, gdp)\n# Representación gráfica\nplt.plot(gdp[-50:])\nplt.plot(predicciones)\n\n\n\n\n% Limpiando memoria\nclear all\nload data\n% Seleccionando muesttra\nx1995 = gdp(1 : 96);\n% Transformando variable\nx2005 = x1995(41 : end);\n% Horizonte de predicción y periodo estacional\nh = 8;   % Horizonte de predicción\nm = 4;   % Observaciones por año\n% Estimando predicciones\nnaive = repmat(x1995(end), h, 1);\nnaiveS = repmat(x1995(end - m + 1 : end), h / m, 1);\nmediaAnual = repmat(mean(x1995(end - m + 1 : end)), h, 1);\nm1995 = PTS(x1995, 4, h=h, verbose=true);\nm2005 = PTS(x2005, 4, h=h, verbose=true);\n% Muestra componentes\nstackedplot(m1995.comp)\n% Tests de modelo sin logaritmos\ntoolTEST(m1995.v)\n% Matriz con todas las predicciones\npredicciones = [naive naiveS mediaAnual m1995.yFor m2005.yFor];\n% Precisión de predicciones\nAccuracy(predicciones, gdp(1 : 104))\n% Representación gráfica\nt = (50 : length(gdp))';\nplot(t, gdp(t), (97 : 104), predicciones)"
  },
  {
    "objectID": "04-etsMSOE.html#bibliografía",
    "href": "04-etsMSOE.html#bibliografía",
    "title": "\n5  Suavizado Exponencial con mútiples ruidos\n",
    "section": "Bibliografía",
    "text": "Bibliografía\nCasals, J., García-Hirnaux, A., Jerez, M., Sotoca, S. Trindade, A.A. (2020), State-Space Methods for Time Series Analysis. Blackwell’s. Oxford.\nDurbin, J., Koopman, S. J. (2012), Time Series Analysis by State Space Methods. Oxford University Press. 2nd edition.\nHyndman, R., Koehler, A.B., Ord, J.K. (2008), Forecasting with Exponential Smoothing: The State Space Approach. Springer Series in Statistics.\nHarvey, A.C. (1989), Forecasting, Structural Time Series Models and the Kalman Filter. Cambridge University Press. Cambridge.\nPedregal, D. J. (2022). Automatic Identification and Forecasting of Structural Unobserved Components Models with UComp. Journal of Statistical Software, 103(1), 1–33. https://doi.org/10.18637/jss.v103.i09"
  },
  {
    "objectID": "05-extensiones.html#la-transformación-box-cox-de-nuevo",
    "href": "05-extensiones.html#la-transformación-box-cox-de-nuevo",
    "title": "\n6  Aplicaciones\n",
    "section": "\n6.1 La transformación Box-Cox de nuevo",
    "text": "6.1 La transformación Box-Cox de nuevo\nA pesar de que esta cuestión ya se abordó en el capítulo inicial, la práctica lleva a puntualizar ciertas cuestiones. Al margen de la cuestión de la heterocedasticidad, la transformacón logarítmica, que es un caso particular de la transformación Box-Cox con \\(\\lambda=0\\), tiene unas consecuencias que merece la pena analizar.\nAsumamos una descomposición aditiva para una serie en logaritmos, \\(\\log(y_t)=T_t+s_{t-m}+\\epsilon_t\\). En esta ecuación \\(T_t=l_{t-1}+b_{t-1}=l_t-\\alpha\\epsilon_t\\) es en realidad el nivel libre de ruido en el momento \\(t\\). El modelo para la serie en la escala original será \\(y_t=e^{T_t}e^{s_{t-m}}e^{\\epsilon_t}\\) o dicho de otro modo, la descomposición para el logaritmo equivale a en realidad una descomposición multiplicativa del tipo ETS(M,A,M).\nEste hallazgo hace que en realidad estimar un modelo con la transformación logarítmica es equivalente a estimar algunos modelos multiplicativos. Por ello, al margen de la cuestión sobre la heterocedasticidad, es conveniente estudiar si la transformación logarítmica tiene sentido en series concretas. Si se descarta dicha transformación se puede afirmar que los modelos multiplicativos no van a superar a los aditivos.\nEs por esto que en el paquete UComp implementa una forma particular de estimar la constante \\(\\lambda\\) algo más compleja que el método indicado en Ecuación 2.1. El procedimiento se basa en la descomposición heurística de la serie en cuestión con una tendencia estimada como una media móvil y un componente estacional como una regresión armónica similar al componente estacional ‘D’ pero con coeficientes constantes (varianzas fijas). Teniendo en cuenta esta descomposición se calcula la función de verosimilitud de la descomposición para la serie original, la serie en logaritmos y con la transformación Box-Cox como en Ecuación 2.1. Se toma el \\(\\lambda\\) que produce el máximo de los tres.\nEn la evaluación de las verosimilitudes, hay que considerar el cambio de escala y cómo afecta a la función. Es decir, las tres verosimilitudes hay que expresarlas en la escala original de la serie. Esto se puede hacer teniendo en cuenta la función de probabilidad de la transformación de una variable aleatoria, que es el producto de la función de densidad de la variable por el determinante del jacobiano de la transformación. Si llamamos \\(y_t^{(\\lambda)}\\) a la variable transformada tenemos que\n\\[\nf(y_1,y_2,\\dots,y_T)=f(y_1^{(\\lambda)},y_2^{(\\lambda)},\\dots,y_T^{(\\lambda)})|J|.\n\\]\nEl jacobiano de la transformación en este caso tiene por elementos en la diagonal \\(\\partial y_i^{(\\lambda)} / \\partial y_i=y_i^{\\lambda-1}\\), además todos los elementos fuera de la diagonal principal son cero. De aquí se obtiene que\n\\[\n|J|=\\prod_{i=1}^T y_i^{\\lambda-1}.\n\\]\nPor lo tanto, la verosimilitud calculada para el modelo con el logaritmo y con \\(\\lambda\\) óptimo se pondrán en la misma escala que la variable original multiplicando la verosimilitud estimado por \\(|J|\\) para el valor correspondiente de \\(\\lambda\\).\n\n\nResuelve\nSolución\nR\nPython\nMATLAB/Octave\n\n\n\nEstima el modelo óptimo para el PIB español incluyendo en la optimización el valor del parámetro \\(\\lambda\\) de la transformación Box-Cox, con datos hasta el último trimestre de 2019 (entrada lambda de ETS o PTS). Comprueba que el modelo tiene sentido, sobre todo que los residuos son homocedásticos.\n\n\n\nrm(list = ls())\nlibrary(UComp)\nlibrary(ggplot2)\nx = window(gdp, end=c(2019, 4))\nm = PTS(x, lambda = NULL)\nprint(m)\n## -------------------------------------------------------------\n##   Box-Cox lambda: 1.00\n##   Model: (N,Ad,D)\n##   Periods:   4.0 /  2.0\n##   Q-Newton: Function convergence\n##   (*)  concentrated out parameters\n##  -------------------------------------------------------------\n##                       Param   asymp.s.e.        |T|     |Grad| \n##  -------------------------------------------------------------\n##       Damping:       0.8938       0.0356    25.0871   8.77e-07\n##         Level:       0.0138       0.0459     0.2997   1.97e-07\n##         Slope:       0.1327*  \n##     Seas(4.0):       0.0120       0.0056     2.1514   1.21e-06\n##     Seas(2.0):       0.0059       0.0023     2.5480   1.86e-06\n##  -------------------------------------------------------------\n##    AIC:       2.3486   BIC:       2.6091   AICc:       2.3686\n##             Log-Likelihood:    -107.4283\n##  -------------------------------------------------------------\n##     Summary statistics:\n##  -------------------------------------------------------------\n##          Missing data:      \n##          Q( 1):       0.0151         Q( 4):       0.2638\n##          Q( 8):       4.6922         Q(12):       6.3480\n##    Bera-Jarque:      21.8404       P-value:       0.0000\n##        H(  32):       0.6810       P-value:       0.2823\n##    Outliers (&gt;2.7 ES):     2\n##          Q( 1):       3.1355         Q( 4):       3.7753\n##          Q( 8):       8.7195         Q(12):      13.7607\n##    Bera-Jarque:       0.6227       P-value:       0.7324\n##        H(  32):       0.6635       P-value:       0.2513\n##  -------------------------------------------------------------\n\n\ntests(m)\n## Summary statistics:\n## ==================\n##                        Serie 1\n## Data points:          92.00000\n## Missing:               8.00000\n## Minimum:              -2.26145\n## 1st quartile:         -0.29717\n## Mean:                  0.09020\n## P(Mean = 0):           0.24788\n## Median:                0.11231\n## 3rd quartile:          0.47663\n## Maximum:               1.81451\n## Interquartile range:   0.77380\n## Range:                 4.07596\n## Satandard deviation:   0.71038\n## Variance:              0.50463\n## Skewness:             -0.68275\n## Kurtosis:              1.69070\n## Autocorrelation tests:\n## =====================\n##      SACF sa     LB p.val  SPACF sp\n## 1  -0.016  .  0.021 0.884 -0.016  .\n## 2  -0.036  .  0.136 0.934 -0.036  .\n## 3  -0.010  .  0.146 0.986 -0.012  .\n## 4   0.002  .  0.146 0.997  0.000  .\n## 5  -0.174  .  2.906 0.714 -0.175  .\n## 6   0.064  .  3.282 0.773  0.060  .\n## 7   0.010  .  3.291 0.857 -0.002  .\n## 8  -0.106  .  4.350 0.824 -0.110  .\n## 9  -0.063  .  4.733 0.857 -0.065  .\n## 10  0.088  .  5.496 0.856  0.052  .\n## 11 -0.061  .  5.858 0.883 -0.050  .\n## 12  0.027  .  5.931 0.920  0.025  .\n## 13  0.073  .  6.473 0.927  0.038  .\n## 14 -0.067  .  6.932 0.937 -0.081  .\n## 15  0.111  .  8.216 0.915  0.156  .\n## 16  0.033  .  8.329 0.938 -0.005  .\n## 17 -0.013  .  8.348 0.959 -0.010  .\n## 18 -0.127  . 10.116 0.928 -0.098  .\n## 19  0.061  . 10.533 0.939  0.035  .\n## 20 -0.062  . 10.974 0.947 -0.031  .\n## 21 -0.022  . 11.029 0.962 -0.011  .\n## 22  0.011  . 11.042 0.974 -0.012  .\n## 23  0.057  . 11.426 0.978  0.030  .\n## Gaussianity tests:\n## =================\n## \n##  Shapiro-Wilk normality test\n## \n## data:  x\n## W = 0.96007, p-value = 0.01062\n## \n## Ratio of variance tests:\n## =======================\n##  Portion_of_data F_statistic p.value\n##          0.33333      0.7914  0.5475\n\n\n\n\nEl mejor valor es \\(\\lambda=1\\), por lo que la serie es homocedástica. Los tests de residuos, además, indican que el modelo es adecuado.\n\n\n\nrm(list = ls())\nlibrary(UComp)\nlibrary(ggplot2)\nx = window(gdp, end=c(2019, 4))\nm = PTS(x, lambda = NULL)\nprint(m)\ntests(m)\n\n\n\n\nfrom UComp import *\nx = window(gdp, end=\"2019-12-31\")\nm = PTS(x, lambdaBoxCox=np.nan, verbose=True)\ntests(m)\n\n\n\n\nclear all\nload data\nx = gdp(1 : 100);\nm = PTS(x, 4, lambda = NaN, verbose=true);\ntoolTEST(m.v)"
  },
  {
    "objectID": "05-extensiones.html#combinación-y-selección-de-modelos",
    "href": "05-extensiones.html#combinación-y-selección-de-modelos",
    "title": "\n6  Aplicaciones\n",
    "section": "\n6.2 Combinación y selección de modelos",
    "text": "6.2 Combinación y selección de modelos\nEn el mundo real no suele haber metodologías que funcionen sistemáticamente mejor que otras. Dada la complejidad de los datos reales, la falta de adecuación de los modelos a dichos datos, los atípicos, a veces la falta de robustez de los programas utilizados, etc. producen una variabilidad en los resultados a veces muy grande.\nAún así nos interesa conseguir las predicciones que mejor reflejen el comportamiento futuro de las series que queremos analizar. Este problema se puede abordar al menos de dos formas alternativas:\n\n\nSelección de modelos. Se deben contemplar dos casos:\n\nSe va a utilizar una sola familia de modelos, p.ej. ETS. En este caso se suele utilizar criterios de información intramuestral (AIC, BIC, AICc, etc.). Es el método que hemos utilizado hasta el momento para seleccionar modelos de suavizado exponencial SSOE o MSOE.\nSe va a utilizar una batería de modelos que provienen de distintas familias, por ejemplo, ETS frente a PTS y métodos heurísticos. En este caso no se deben usar los criterios de información porque no son uniformes en general en distintas familias. En este caso es recomendable utilizar métricas de predicción en el marco de un experimento. Para ello se estiman todos los modelos en una determinada muestra (training), se evalúa en la muestra de validación (validation) con las métricas que se consideren oportunas. El modelo que mejor se comporte es el que se utiliza para realizar las predicciones finales. No siempre el modelo seleccionado es el que predice mejor fuera de la muestra, por lo que se requiere estar continuamente revisando las métricas de distintos modelos.\n\n\n\nCombinación de predicciones. Una aproximación radicalmente diferente es combinar las predicciones de los distintos modelos con algún criterio objetivo. Suponiendo que los métodos o modelos que se combinan son adecuados para la series en cuestión, esta es una forma de hacer robustas las predicciones.\n\nUn planteamiento general sobre la combinación de predicciones es plantearla como una media ponderada de las predicciones individuales. Si tenemos \\(n\\) predicciones individuales \\(\\hat{y}_{1,T+h|T},\\hat{y}_{2,T+h|T},\\dots,\\hat{y}_{n,T+h|T}\\), la combinación será \\(\\hat{y}_{T+h|T}=\\sum_{i=1}^n w_i \\hat{y}_{i,T+h|T}\\), siendo todos los pesos positivos y \\(\\sum_{i=1}^n w_i = 1\\).\nExiste una amplia bibliografía sobre el tema de las combinaciones de métodos. La conclusión general es que combinaciones sencillas como la media (que es un caso particular de la combinación anterior) o la mediana de las predicciones resultan ser muy eficientes por los resultados que se obtienen frente a la facilidad con que se pueden estimar. La Figura 6.1 muestra el efecto de calcular la media de un conjunto dispar de predicciones. La ventaja de la mediana es que es mucho más robusto que la media frente a alguna predicción que por alguna razón resulta disparatada.\n\n\n\n\nFigura 6.1: Media de predicciones.\n\n\n\nAmbos procedimientos se pueden mezclar, puesto que algunos de los métodos que se analicen con la selección de modelos pueden ser combinaciones de otros individuales, que también pueden entrar individualmente en la competición. De hecho, es muy habitual comparar modelos individuales con distintas combinaciones de ellos mismos.\n\n\nResuelve\nSolución\nR\nPython\nMATLAB/Octave\n\n\n\nPredice el PIB español con datos hasta el final de 2017 para los dos años siguientes con seis métodos: método media del último año, naive, naive estacional, PTS para la serie original, PTS para la serie en logaritmos, media de todos los métodos, mediana de todos los métodos (excluyendo la media). Representa todas las predicciones y en un gráfico y di cuál es el mejor. ¿Qué métricas tendría el método naive si el origen de predicción hubiera sido el primer trimestre?\n\n\n\n##                      ME      RMSE       MAE         MPE     PRMSE      MAPE\n## media       -3.50050283 4.5903150 4.0337826 -3.12159265 4.1043375 3.6345058\n## naive       -0.36192895 2.9913983 2.5728024 -0.25591024 2.7555511 2.3560954\n## snaive      -3.50050282 3.6855744 3.5005028 -3.18847869 3.3548188 3.1884787\n## m           -0.08339210 0.4730638 0.3992363 -0.06744118 0.4304333 0.3644698\n## ml           0.05076619 0.3588330 0.3302354  0.05084466 0.3309259 0.3033153\n## combMedia   -1.47911210 2.0300628 1.7784344 -1.31651562 1.8158753 1.6044065\n## combMediana -1.37794555 2.1629458 1.6501252 -1.22099551 1.9201893 1.4783638\n##                 sMAPE\n## media       3.7157288\n## naive       2.3548993\n## snaive      3.2458552\n## m           0.3646971\n## ml          0.3031962\n## combMedia   1.6194325\n## combMediana 1.4964150\n\n\n\n\nTanto el gráfico como las métricas de error indican que las predicciones de ambos modelos PTS son muy similares, aunque es marginalmente superior con logaritmos, aunque sabemos que no son necesarios.\nLas combinaciones son realmente malas porque ambas incluyen la predicción de métodos que no tienen estacionalidad. Es especialmente interesante el caso de la mediana, que, al estar combinando un número impar de métodos siempre coincide con alguno de ellos. Esto puede desfigurar la estacionalidad hasta cierto punto.\nEn el caso de que el origen de predicción hubiera sido el primer cuatrimestre de cualquier año el modelo naive hubera sido muy malo, porque la serie tiene tendencia ascendente y el naive predeciría en el momento en el que es más bajo. Los errores de predicción del modelo naive tienen estacionalidad, lo que es un indicativo claro de que el modelo es incorrecto.\n\n\n\n# Horizonte de predicción\nh = 8\n# Muestra\nx = window(gdp, end = c(2017, 4))\n# Calculando predicciones\nmedia = rep(mean(tail(x, 4)), h)\nnaive = rep(tail(x, 1), h)\nsnaive = rep(tail(x, 4), 2)\nm = PTSmodel(x, h = h)$yFor\nml = exp(PTSmodel(log(x), h = h)$yFor)\npredicciones = cbind(media, naive, snaive, m, ml)\n# Combinaciones de predicciones\ncombMedia = rowMeans(predicciones)\ncombMediana = rowMedians(predicciones)\n# Todas las predicciones en una matriz\npredicciones = cbind(media, naive, snaive, m, ml, combMedia, combMediana)\n# Precisión de predicciónes\nAccuracy(predicciones, window(gdp, start=2018, end=c(2019, 4)))\n# Representación gráfica\nautoplot(window(gdp, start=2016)) + autolayer(predicciones)\n\n\n\n\n# Horizonte de predicción\nh = 8\n# Muestra\nx = window(gdp, end = \"2017-12-31\")\n# Calculando predicciones\nm = PTSmodel(x, h = h).yFor\nnaive = ts(x[-1].repeat(h), start=m.index[0], freq='q')\naux = np.hstack((x[-4:].values, x[-4:].values))\nsnaive = ts(aux, start=m.index[0], freq='q')\nmedia = ts(np.mean(x[-4:]).repeat(h), start=m.index[0], freq='q')\nml = np.exp(PTSmodel(np.log(x), h = h).yFor)\npredicciones = pd.concat((media, naive, snaive, m, ml), axis=1)\n# Combinaciones de predicciones\ncombMedia = ts(np.mean(predicciones, axis=1), start=m.index[0], freq='q')\ncombMediana = ts(np.median(predicciones, axis=1), start=m.index[0], freq='q')\n# Todas las predicciones en una matriz\npredicciones = pd.concat((media, naive, snaive, m, ml, combMedia, combMediana), axis=1)\n# Precisión de predicciónes\nAccuracy(predicciones, window(gdp, start=\"2018-01-31\", end=\"2019-12-31\"))\n# Representación gráfica\nplt.plot(gdp[-50:])\nplt.plot(predicciones)\n\n\n\n\n% Horizonte de predicción\nh = 8;\n% Muestra\nx = gdp(1 : 92);\n% Calculando predicciones\nmedia = repmat(mean(x(end - 3 : end)), h, 1);\nnaive = repmat(x(end), h, 1);\nsnaive = repmat(x(end - 3 : end), 2, 1);\nm = PTSmodel(x, 4, h=h).yFor;\nml = exp(PTSmodel(log(x), 4, h=h).yFor);\npredicciones = [media naive snaive m ml];\n% Combinaciones de predicciones\ncombMedia = mean(predicciones, 2);\ncombMediana = median(predicciones, 2);\n% Todas las predicciones en una matriz\npredicciones = [media naive snaive m ml combMedia combMediana];\n% Precisión de predicciónes\nAccuracy(predicciones, gdp(93 : 100))\n% Representación gráfica\nt = (50 : length(gdp))';\nplot(t, gdp(t), (93 : 100), predicciones)\n\n\n\n\n\n\n\n\n\n\nImportante\n\n\n\nUn reloj parado da dos veces al día la hora correcta. Puede suceder que un modelo completamente erróneo arroje la mejor predicción en una situación particular por puro azar. Pero interesa utilizar el modelo que en media dé la mejor predicción, es decir, que en repetidas ocasiones con numerosos orígenes de predicción dé las mejores métricas."
  },
  {
    "objectID": "05-extensiones.html#sistemas-de-apoyo-a-la-predicción",
    "href": "05-extensiones.html#sistemas-de-apoyo-a-la-predicción",
    "title": "\n6  Aplicaciones\n",
    "section": "\n6.3 Sistemas de apoyo a la predicción",
    "text": "6.3 Sistemas de apoyo a la predicción\nUn método de predicción es superior a otro si predice sistemáticamente mejor o cuando de forma agregada las métricas de error son superiores. La forma rigurosa de contrastar esto en casos concretos es plantear un experimento multidimensional con varios orígenes de predicción, varios horizontes de predicción, distintos métodos y cuando sea preciso múltiples series temporales. Es decir, que al menos debería tener 3 dimensiones.\nRespecto al origen y horizonte se suele utilizar la validación cruzada, que consiste en obtener predicciones con orígenes de predicción que se van moviendo hacia adelante, como indica la Figura 6.2. El movimiento hacia adelante se puede hacer de forma que se mantenga el inicio de la muestra fijo y el número de datos que se utiliza en la modelización va creciendo a medida que se mueve el origen de predicción (parte superior de la Figura 6.2). Otra versión, que se muestra en la parte inferior, consiste en mantener constante la ventana de datos.\n\n\n\n\nFigura 6.2: Validación cruzada con tamaño variable y fijo de ventana.\n\n\n\nPara una sola serie temporal podemos tener las tres dimensiones que se han comentado, distintos orígenes de predicción, distintos horizontes, y varios métodos. En ese caso, la información sobre cualquiera de las métricas de error de la Figura 2.1 se puede disponer en una matriz tridimensional, como indica la Figura 6.3. La información se puede sintetizar en gráficas que muestra cómo evoluciona una determinada medida de error en función del horizonte de predicción para un conjunto de métodos.\n\n\n\n\nFigura 6.3: Dimensiones del experimento de predicción.\n\n\n\nLa síntesis que se muestra en la Figura Figura 6.3 se puede obtener agregando los orígenes calculando medias para todas las observaciones de izquierda a derecha en el cubo. De esta forma se ve cómo se comportan los métodos a medida que crece el horizonte de predicción. Pero nada impide que se utilicen otras formas de agregación, por ejemplo, se podría agregar de arriba a abajo y el resultado indicaría cómo se comportan los métodos respecto al origen de predicción, puede ser que haya determinados orígenes especialmente difíciles de manejar. La última posibilidad sería agregar del frente hacia el fondo, pero esta es menos interesante, puesto que mostraría orígenes frente a horizontes mezclando los métodos.\nSe puede aún agregar más, aunque va perdiendo el interés. Por ejemplo, se puede obtener la evolución agregada de todos los métodos en función del horizonte o del horizonte frente a los métodos. Finalmente, aunque un tanto exagerado, todo el cubo se puede reducir a un solo número que reflejaría la media de todos los criterios.\nNada impide, y de hecho se hace muy a menudo, añadir una cuarta dimensión si disponemos de un conjunto de series temporales. En ese caso tendríamos un cubo por cada serie temporal o una matriz de errores con cuatro dimensiones. En ese caso, es especialmente conveniente controlar qué series específicas son las que más contribuyen al error, e intentar refinar los métodos en esos casos.\n\n\nResuelve\nSolución\nR\nPython\nMATLAB/Octave\n\n\n\nSiempre que se va a iniciar un proceso complejo como el de este ejercicio es conveniente reiniciar toda la memoria y cargar las librerías necesarias. Considera los modelos media del último año, naive, naive estacional, PTS, ETS, mediana de todos menos naive, media de todos menos naive. Calcula las predicciones de los modelos con ventanas solapadas para \\(h = 4\\) desde un origen inicial en el último trimestre de 2004. Para ello utiliza las funciones forecastMethods y MASE que se muestran más abajo para R (en los demás lenguajes se pueden ver en sus respectivas pestañas). La función forecastMethods sirve para predecir una serie temporal con todos los métodos que se incluyan y MASE calcula todas las métricas de error de una serie temporal frente a todas las predicciones para esa serie. Utiliza las funciones slide y plotSlide para generar las predicciones 2 años hacia adelante. ¿Qué modelo es el mejor predictor para esta serie?\nLa función forecastMethods necesita como entradas la serie temporal y devuelve las predicciones por todos los métodos en una matriz cuyas filas son el horizonte de predicción y columnas cada uno de los métodos.\nLa función MASE calcula el mean absolute squared error de las predicciones para una sola serie temporal y la predicción por un solo método. Necesita como entradas la predicción y los valores reales dentro y fuera de la muestra (training y testing). La función asume que las predicciones se corresponden con los periodos muestrales de las últimas observaciones de la serie real.\n\n# Función que predice todos los modelos sobre una serie\nforecastMethods = function(x, h = 4){\n    # Función que devuelve predicciones de todos los métodos en columnas\n    # Algunos ajustes dependiendo de si la entrada es una serie\n    if (is.ts(x)){\n        m = frequency(x)\n    } else {\n        m = h\n    }\n    # Número de años de predicción\n    nYears = ceiling(h / m)\n    # Prediciendo modelos\n    naive = rep(tail(x, 1), h)\n    snaive = rep(tail(x, m), nYears)[1 : h]\n    mediaAnual = rep(mean(tail(x, m)), h)\n    pETS = ETSmodel(x, h = h)$yFor\n    pPTS = PTSmodel(x, h = h)$yFor\n    pred = cbind(snaive, mediaAnual, pETS, pPTS)\n    # Combinación de modelos\n    media = rowMeans(pred)\n    mediana = rowMedians(pred)\n    # Todos los modelos en una matriz\n    predicciones = cbind(naive, snaive, mediaAnual, pETS, pPTS, media, mediana)\n    # Nombres de columnas\n    colnames(predicciones) = c(\"naive\", \"snaive\", \"mediaAnual\", \"ETS\", \"PTS\",\n                               \"media\", \"mediana\")\n    return(predicciones)\n}\n# Función que calcula el MASE sobre una predicción para una serie temporal\nMASE = function(px, x){\n    if (is.ts(x)){\n        m = frequency(x)\n    } else {\n        m = 1\n    }\n    n = length(x)\n    h = length(px)\n    tx = x[(n - h + 1) : n]\n    error1 = mean(abs(x[(m + 1) : (n - h)] - x[1 : (n - h - m)]))\n    return(cumsum(abs(px - tx)) / error1 / (1 : h))\n}\n\n\n\n\n# Borrando memoria\nrm(list = ls())\n# Cargando librerías\nlibrary(UComp)\nlibrary(ggplot2)\n\n\n\n\n\n\nCada método se ha ejecutado sobre 57 orígenes de predicción diferentes y queda patente que los métodos más sofisticados son los que mejor predicen. En este caso las combinaciones de métodos no superan a los métodos individuales.\n\n\n\n# Seleccionando muestra, origen de predicción, horizonte y step\ny = window(gdp, end=c(2019, 4))\norig = length(window(gdp, end=c(2003, 4)))\nh = 8\nstep = 1\n# Estimando todas las predicciones\nout = slide(y, orig, forecastMethods, h = h, step=step)\n# Muestra predicciones agregadas por métodos\naux = plotSlide(out, y, orig, step, MASE)\n\n\n\n\nfrom UComp import *\n\n# Función que devuelve predicciones de todos los métodos en columnas\ndef forecastMethods(x, h=4, s=4):\n    nYears = np.ceil(h / s)\n    naive = np.repeat(x[-1], h)\n    snaive = np.tile(x[-s:], int(nYears))[:h]\n    mediaAnual = np.repeat(np.mean(x[-s:]), h)\n    pETS = ETSmodel(x, h=h, s=s).yFor\n    pPTS = PTSmodel(x, h=h, s=s).yFor\n    pred = np.column_stack((snaive, mediaAnual, pETS, pPTS))\n    media = np.mean(pred, axis=1)\n    mediana = np.median(pred, axis=1)\n    predicciones = np.column_stack((naive, snaive, mediaAnual, pETS, pPTS, media, mediana))\n    return predicciones\n\n\n# Función para calcular el MASE\ndef MASE(px, actual, s=4):\n    n = len(actual)\n    h = len(px)\n    tx = actual[-h:]\n    error1 = np.mean(np.abs(actual[s : (n - h)] - actual[0: (n - h - s)]))\n    return np.cumsum(np.abs(px - tx)) / error1 / np.arange(1, h + 1)\n\n\n# Seleccionando muestra, origen de predicción, horizonte y step\ny = window(gdp, end=\"2019-12-31\")\norig = len(window(gdp, end=\"2003-12-31\"))\nh = 8\nstep = 1\n# Estimando todas las predicciones\nout = slide(y, orig, forecastMethods, h, step, s=4)\n# Muestra predicciones agregadas por métodos\naux = plotSlide(out, y, orig, step, MASE, s=4)\n\n\n\n\nLa solución para MATLAB/Octave implica grabar las dos funciones siguientes en archivos independientes para poder ser utilizadas en el código de la solución.\n\n% Función que devuelve predicciones de todos los métodos en columnas\nfunction predicciones = forecastMethods(x, s, h)\n    % Función que devuelve predicciones de todos los métodos en columnas\n    if nargin &lt; 3\n        h = 4;\n    end\n    nYears = ceil(h / s);\n    naive = repmat(x(end), h, 1);\n    snaive = repmat(x(end - s + 1 : end), nYears, 1);\n    snaive = snaive(1 : h);\n    mediaAnual = repmat(mean(x(end - s + 1 : end)), h, 1);\n    pETS = ETSmodel(x, s, h=h).yFor;\n    pPTS = PTSmodel(x, s, h=h).yFor;\n    pred = [snaive mediaAnual pETS pPTS];\n    media = mean(pred, 2);\n    mediana = median(pred, 2);\n    predicciones = [naive snaive mediaAnual pETS pPTS media mediana];\nend\n\n% Función que calcula la métrica de error MASE\nfunction mase = MASE(px, actual, s)\n    n = length(actual);\n    h = length(px);\n    tx = actual(n - h + 1 : n);\n    error1 = mean(abs(actual(s + 1 : n - h) - actual(1 : n - h - s)));\n    mase = cumsum(abs(px - tx)) / error1 ./ (1 : h)';\nend\n\n% Inicializando memoria\nclear all\nload data\n% Seleccionando muestra, origen de predicción, horizonte y step\ny = gdp(1 : 100);\norig = 36;\nh = 8;\nstep = 1;\n% Estimando todas las predicciones\nout = slide(y, 4, orig, @forecastMethods, h=h, step=step);\n% Muestra predicciones agregadas por métodos\naux = plotSlide(out, y, 4, orig, step, @MASE);\n\n\n\n\n\n\n\n\n\n\nImportante\n\n\n\nLas combinaciones de métodos suelen superar a los individuales en casos en que se estén prediciendo muchas series temporales, no tanto para series individuales, aunque se utilicen múltiples orígenes de predicción"
  },
  {
    "objectID": "06-introR.html#comandos-generales-útiles",
    "href": "06-introR.html#comandos-generales-útiles",
    "title": "Apéndice A: Introducción a los lenguajes de programación",
    "section": "Comandos generales útiles",
    "text": "Comandos generales útiles\nR\nLa instalación de R (aquí) y RStudio (aquí) es elemental desde las webs respectivas.\nUna vez instalados estos dos elementos básicos, ya se puede utilizar una serie de comandos generales de gran utilidad:\n\n\ninstall.packages para instalar paquetes desde el repositorio de CRAN.\n\nremove.packages para desinstalar paquetes.\n\ngetwd y setwd para visualizar y cambiar la carpeta de trabajo por defecto.\nEl signo de interrogación (?) delante de cualquier comando muestra su ayuda.\n\nls() muestra los objetos en memoria.\n\ndir() muestra los archivos en la carpeta.\n\nrm borra objetos de la memoria. rm(list=ls()) borra toda la memoria.\n\ncat(\"\\14\") limpia la consola.\nPython\nLa instalación de Python (aquí) es elemental desde la web. Existen muchos entornos de desarrollo integrados (IDE) para manejar Python. Aquí recomendamos PyCharm (aquí).\nPython se ha desarrollado con una mentalidad muy flexible, incluso para usarse desde una ventana de comandos en cualquier sistema operativo. Aquí se ofrece una serie de comandos generales de gran utilidad:\n\n\npip3 install nombre_paquete para instalar paquetes desde una ventana de comandos del repositorio PyPI.\n\npip3 uninstall nombre_paquete para desinstalar paquetes desde una ventana de comandos.\n\nos.getcwd() y os.chdir() del paquete os para visualizar y cambiar la carpeta de trabajo por defecto. Se tienen que importar con import os.\n\nprint(nombre_funcion.__doc__) para ver la documentación de una función.\nMATLAB/OCtave\nMATLAB es un programa de pago que ofrece algunas opciones gratuitas y se puede instalar desde la red (aquí). Octave sin embargo es gratuita y corre código que es compatible a grades rasgos con MATLAB (aquí).\nAlgunos comandos de utilidad son:\n\n\ncd y cd para visualizar y cambiar la carpeta de trabajo por defecto.\n\nhelp para mostrar la ayuda de cualquier función.\n\nwhat muestra los objetos en memoria.\n\ndir muestra los archivos en la carpeta.\n\nclear borra objetos de la memoria. clear all borra toda la memoria.\n\nclc limpia la consola."
  },
  {
    "objectID": "06-introR.html#tipos-de-variable",
    "href": "06-introR.html#tipos-de-variable",
    "title": "Apéndice A: Introducción a los lenguajes de programación",
    "section": "Tipos de variable",
    "text": "Tipos de variable\nEs importante tener en cuenta que cuando se crean variables, siempre se distingue entre letras mayúsculas y minúsculas.\n\n\nR\nPython\nMATLAB/Octave\n\n\n\n\n# El operador de asignación es `&lt;-` o `=`:\na &lt;- 1\nA &lt;- 2\nB = a %% 3\nb = TRUE\nc = FALSE\n\n\n\n\na = 1\nA = 2\nB = a // 3\nb = True\nc = False\n\n\n\n\na = 1;\nA = 2;\nB = a %% 3;\nb = true;\nc = false;\n\n\n\n\nVariables reales:\n\n\nR\nPython\nMATLAB/Octave\n\n\n\n\n3/1.6\n1/0\n0/0\nNA\nNULL\n\n\n\n\n3/1.6\n1/0\n0/0\nimport numpy as np\nnp.nan\nNone\n\n\n\n\n3/1.6\n1/0\n0/0\nnan\n% En MATLAB no hay un equivalente a NULL, se suele usar NaN\n\n\n\n\nVectores:\nPara utilizar vectores y matrices en Python es necesario utilizar algún paquete específico. El más usado es numpy.\nMATLAB/Octave no diferencia entre vectores y matrices, en realidad todo son matrices, por lo que es posible definir los vectores como filas o columnas, según convenga en cada caso.\n\n\nR\nPython\nMATLAB/Octave\n\n\n\n\ny = c(1, 2, 3, 4, 5, 6)\nlength(y)\ny[2 : 5]\ny / 2\ny * 2\n\n\n\n\nimport numpy as np\ny = np.array([1, 2, 3, 4, 5, 6])\ny.shape\n# Los índices en Python comienzan en 0\ny[1 : 5]\ny / 2\ny * 2\n\n\n\n\ny = [1 2 3 4 5 6];\nsize(y)\ny(2 : 5)\ny / 2\ny * 2\n\n\n\n\nMatrices:\n\n\nR\nPython\nMATLAB/Octave\n\n\n\n\nm = matrix(c(1, 2, 3, 4, 5, 6), 2, 3)\nm[1,]\nm[,2]\nA = dim(m)\nA[1]\nA[2]\n\n\n\n\nm = np.array([[1, 2, 3], [4, 5, 6]])\nm[0, :]\nm[:, 1]\nA = m.shape\nA[0]\nA[1]\n\n\n\n\nm = [1 2 3; 4 5 6]\nm(1, :)\nm(:, 2)\nA = size(m);\nA(1)\nA(2)\n\n\n\n\nHipermatrices (matrices de más de dos dimensiones):\n\n\nR\nPython\nMATLAB/Octave\n\n\n\n\nm &lt;- array(1:30, c(2, 5, 3))\nA = dim(m)\nA[1]\nA[2]\nA[3]\nm[1,,]\nm[,2,]\nm[,,1]\n\n\n\n\nm = np.arange(1, 31).reshape((2, 5, 3))\nA = m.shape\nA[0]\nA[1]\nA[2]\nm[0, :, :]\nm[:, 1, :]\nm[:, :, 0]\n\n\n\n\nm = reshape(1 : 30, [2 5 3])\nA = size(m)\nA(1)\nA(2)\nA(3)\nm(1, :, :)\nm(:, 2, :)\nm(:, :, 1)\n\n\n\n\nCadenas de caracteres y vectores de cadenas:\n\n\nR\nPython\nMATLAB/Octave\n\n\n\n\n\"esto es una cadena de caracteres\"\nnombre = c(\"Yo\", \"Tú\", \"Él\", \"Ella\")\n\n\n\n\n\"esto es una cadena de caracteres\"\nnombre = [\"Yo\", \"Tú\", \"Él\", \"Ella\"]\n\n\n\n\n\"esto es una cadena de caracteres\"\nnombre = [\"Yo\" \"Tú\" \"Él\" \"Ella\"]\n\n\n\n\nFrames:\n\n\nR\nPython\nMATLAB/Octave\n\n\n\n\nedad = c(10, 20, 30, 40)\nDATA = data.frame(nombre, edad)\nprint(DATA$nombre[3])\nprint(DATA$edad[3])\n\n\n\n\nimport pandas as pd\nedad = [10, 20, 30, 40]\nDATA = pd.DataFrame({'nombre': nombre, 'edad': edad})\nDATA[\"nombre\"][2]\nDATA[\"edad\"][2]\n\n\n\n\nedad = 10 : 10 : 40;\nDATA = struct('nombre', nombre, 'edad', edad)\nDATA.nombre(3)\nDATA.edad(3)\n\n\n\n\nListas:\nEn MATLAB/Octave las listas se pueden asimilar a las variables tipo cell.\n\n\nR\nPython\nMATLAB/Octave\n\n\n\n\nC &lt;- list (c(1, 2, 3), c(TRUE, FALSE, TRUE), c(\"a\", \"b\"))\nC[1]\nC[2]\nC[3]\nC[[1]]\nC[[2]]\nC[[3]]\ntypeof(C[1])\ntypeof(C[[1]])\n\n\n\n\nC = [[1, 2, 3], [True, False, True], [\"a\", \"b\"]]\nprint(C)\nC[0]\nC[1]\nC[2]\nC[0][1]\nC[1][0]\nC[2][0 : 2]\ntype(C)\ntype(C[0])\n\n\n\n\nC = {[1, 2, 3], [true, false, true], [\"a\", \"b\"]}\nC{1}\nC{2}\nC{3}\nC{1}(2)\nC{2}(1)\nC{3}(1 : 2)\nclass(C)\nclass(C{1})"
  },
  {
    "objectID": "06-introR.html#operaciones-con-matrices-y-vectores",
    "href": "06-introR.html#operaciones-con-matrices-y-vectores",
    "title": "Apéndice A: Introducción a los lenguajes de programación",
    "section": "Operaciones con matrices y vectores",
    "text": "Operaciones con matrices y vectores\nTransposición de matrices:\n\n\nR\nPython\nMATLAB/Octave\n\n\n\n\nA = matrix(c(1, 2, 3, 4, 5, 6), 2, 3)\nt(A)\n\n\n\n\nA = np.array([[1, 3, 5], \n              [2, 4, 6]])\nnp.transpose(A)\n\n\n\n\nA = [1 3 5; 2 4 6]\nA'\n\n\n\n\nProducto de matrices:\n\n\nR\nPython\nMATLAB/Octave\n\n\n\n\nB = matrix(runif (9), 3, 3)\nA %*% B\nA * 2\nA * A\nA[1, 2]\nA[2, 1]\nA[, 1]\n\n\n\n\nB = np.random.uniform(0, 1, (3, 3))\nprint(B)\nnp.dot(A, B)\nA * 2\nA * A\nA[0, 1]\nA[1, 0]\nA[:, 0]\n\n\n\n\nB = rand(3, 3)\nA * B\nA * 2\nA .* A\nA(1, 2)\nA(2, 1)\nA(:, 1)\n\n\n\n\nInversión de matrices:\n\n\nR\nPython\nMATLAB/Octave\n\n\n\n\nC = solve(B)\nC %*% B\n\n\n\n\nC = np.linalg.inv(B)\nprint(C)\nnp.dot(C, B)\n\n\n\n\nC = inv(B)\nC * B\n\n\n\n\nDiagonal de una matriz:\n\n\nR\nPython\nMATLAB/Octave\n\n\n\n\ndiag(A)\ndiag(B)\ndiag(C)\n\n\n\n\nnp.diag(A)\nnp.diag(B)\nnp.diag(C)\n\n\n\n\ndiag(A)\ndiag(B)\ndiag(C)\n\n\n\n\n\n\nResuelve\nR\nPython\nMATLAB/Octave\n\n\n\nComprueba la carpeta de trabajo.\nMira los archivos que hay en el directorio por defecto.\nMira las variables que tienes en memoria.\nBorra todas las variables de memoria y comprueba que la memoria está vacía.\nCrea dos matrices aleatorias de dimensión 4 x 2 y asígnalas a las variables a y b.\nCalcula la matriz transpuesta de b\nCalcula a+b, a-b, a*b'\nGenera un ruido aleatorio de longitud 3000, procedente de una distribución uniforme.\nCalcula la media y la varianza y muéstralas por pantalla.\nModifícala para que la media sea exactamente 0 y la varianza exactamente 1. Comprobar la forma de la distribución con un histograma.\nRepite el ejercicio anterior para una distribución N(0, 1).\nBorra la ventana de gráficos.\n\n\n\n# Carpeta de trabajo.\ngetwd()\n# Archivos en el directorio por defecto.\ndir()\n# Variables en memoria.\nls()\n# Borrar todas las variables y comprobar memoria.\nrm(list = ls())\nls()\n# Creando dos matrices aleatorias uniformes de dimensión 4 x 2, a y b.\na &lt;- matrix(runif(8), 4, 2)\nprint(a)\nb &lt;- matrix(runif(8), 4, 2)\nprint(b)\n# Transpuesta de `b`.\nt(b)\n# a+b, a-b, a*b'.\na + b\na - b\na %*% t(b)\n# Genera un ruido aleatorio uniforme de longitud 3000.\na &lt;- runif(3000)\nhead(a)\n# Media y la varianza.\ncat(paste(\"Media: \", mean(a), \" /  Varianza: \", var(a)))\n# Modifícando para que la media sea exactamente 0 y la varianza exactamente 1. Histograma.\na &lt;- (a - mean(a)) / sd(a)\ncat(paste(\"Media: \", mean(a), \" /  Varianza: \", var(a)))\nhist(a)\n# Repetimos para distribución N(0, 1).\na &lt;- rnorm(3000)\ncat(paste(\"Media: \", mean(a), \" /  Varianza: \", var(a)))\na &lt;- (a-mean(a))/sd(a)\ncat(paste(\"Media: \", mean(a), \" /  Varianza: \", var(a)))\nhist(a)\n# Borrar gráficos.\ndev.off()\n\n\n\n\n# Carpeta de trabajo.\nimport os\nos.getcwd()\n# Archivos en el directorio por defecto.\nos.listdir()\n# Variables en memoria.\nglobals().keys()\n# Creando dos matrices aleatorias uniformes de dimensión 4 x 2, a y b.\na = np.random.uniform(0, 1, (4, 2))\nb = np.random.uniform(0, 1, (4, 2))\n# Transpuesta de `b`.\nnp.transpose(b)\n# a+b, a-b, a*b'.\na + b\na - b\nnp.dot(a, np.transpose(b))\n# Genera un ruido aleatorio uniforme de longitud 3000.\na = np.random.uniform(0, 1, (3000))\n# Media y la varianza.\nprint(\"Media: \" + f\"{np.mean(a):.4f}\"  + \"   /   Varianza: \" + f\"{np.var(a):.4f}\")\n# Modifícando para que la media sea exactamente 0 y la varianza exactamente 1. Histograma.\na = (a - np.mean(a)) / np.std(a)\nprint(\"Media: \" + f\"{np.mean(a):.4f}\"  + \"   /   Varianza: \" + f\"{np.var(a):.4f}\")\nimport matplotlib.pyplot as plt\nplt.hist(a)\nplt.show()\n# Repetimos para distribución N(0, 1).\na = np.random.normal(0, 1, (3000))\nprint(\"Media: \" + f\"{np.mean(a):.4f}\"  + \"   /   Varianza: \" + f\"{np.var(a):.4f}\")\na = (a - np.mean(a)) / np.std(a)\nprint(\"Media: \" + f\"{np.mean(a):.4f}\"  + \"   /   Varianza: \" + f\"{np.var(a):.4f}\")\nplt.hist(a)\n# Cerrar gráficos.\nplt.close()\n\n\n\n\n% Carpeta de trabajo.\ncd\n% Archivos en el directorio por defecto.\ndir\n% Variables en memoria.\nwhos\n% Creando dos matrices aleatorias uniformes de dimensión 4 x 2, a y b.\na = rand(4, 2)\nb = rand(4, 2)\n% Transpuesta de `b`.\nb'\n% a+b, a-b, a*b'.\na + b\na - b\na * b'\n% Genera un ruido aleatorio uniforme de longitud 3000.\na = rand(3000, 1);\n% Media y la varianza.\n['Media: ' num2str(mean(a)) '   /   Varianza: ' num2str(var(a))]\n% Modifícando para que la media sea exactamente 0 y la varianza exactamente 1. Histograma.\na = (a - mean(a)) / std(a);\n['Media: ' num2str(mean(a)) '   /   Varianza: ' num2str(var(a))]\nhist(a)\n% Repetimos para distribución N(0, 1).\na = randn(3000, 1);\n['Media: ' num2str(mean(a)) '   /   Varianza: ' num2str(var(a))]\na = (a - mean(a)) / std(a);\n['Media: ' num2str(mean(a)) '   /   Varianza: ' num2str(var(a))]\nhist(a)\n% Borrar gráficos\nclf"
  },
  {
    "objectID": "06-introR.html#comandos-que-crean-matrices",
    "href": "06-introR.html#comandos-que-crean-matrices",
    "title": "Apéndice A: Introducción a los lenguajes de programación",
    "section": "Comandos que crean matrices:",
    "text": "Comandos que crean matrices:\nMatriz identidad:\n\n\nR\nPython\nMATLAB/Octave\n\n\n\n\ndiag(3)\n\n\n\n\nnp.eye(3)\n\n\n\n\neye(3)\n\n\n\n\nMatriz de ceros o unos:\n\n\nR\nPython\nMATLAB/Octave\n\n\n\n\nmatrix(0, 3, 4)\nmatrix(1, 1, 2)\n\n\n\n\nnp.zeros((3, 4))\nnp.ones((1, 2))\n\n\n\n\nzeros(3, 4)\nones(1, 2)\n\n\n\n\nDimensiones de matrices:\n\n\nR\nPython\nMATLAB/Octave\n\n\n\n\nA = matrix(c(1, 2, 3, 4, 5, 6), 2, 3)\nB = matrix(runif (9), 3, 3)\nC = solve(B)\nn &lt;- dim(A)\nn[1]\nn[2]\nn\n\n\n\n\nA = np.array([[1, 3, 5], [2, 4, 6]])\nB = np.random.uniform(0, 1, (3, 3))\nC = np.linalg.inv(B)\nn = A.shape\nn[0]\nn[1]\nn\n\n\n\n\nA = [1, 3, 5; 2, 4, 6]\nB = rand(3, 3)\nC = inv(B)\nn = size(A)\nn(1)\nn(2)\nn\n\n\n\n\nConcatenación vertical y horizontal:\n\n\nR\nPython\nMATLAB/Octave\n\n\n\n\nprint(A)\nprint(B)\nprint(C)\nrbind(A, B)\ncbind(t(A), C)\n\n\n\n\nprint(A)\nprint(B)\nprint(C)\nnp.vstack((A, B))\nnp.hstack((np.transpose(A), C))\n\n\n\n\nA\nB\nC\n[A; B]\n[A' C]\n\n\n\n\nOperadores relacionales:\n\n\nR\nPython\nMATLAB/Octave\n\n\n\n\n1 &lt; 2\n2 &lt; 1\na = c(1, 2, 0)\nb = c(2, 2, 0)\na &lt; b\n\n1 &gt; 2\n2 &gt; 1\na &gt; b\n\n1 &lt;= 2\n2 &lt;= 1\n1 &lt;= 1\na &lt;= b\n\n1 &gt;= 2\n2 &gt;= 1\n1 &gt;= 1\na &gt;= b\n\n1 == 1\n2 == 1\na == b\n\n1 !=1\n2 != 1\na != b\n\n\n\n\n1 &lt; 2\n2 &lt; 1\na = [1, 2, 0]\nb = [2, 2, 0]\na &lt; b\n\n1 &gt; 2\n2 &gt; 1\na &gt; b\n\n1 &lt;= 2\n2 &lt;= 1\n1 &lt;= 1\na &lt;= b\n\n1 &gt;= 2\n2 &gt;= 1\n1 &gt;= 1\na &gt;= b\n\n1 == 1\n2 == 1\na == b\n\n1 !=1\n2 != 1\na != b\n\n\n\n\n1 &lt; 2\n2 &lt; 1\na = [1 2 0]\nb = [2 2 0]\na &lt; b\n\n1 &gt; 2\n2 &gt; 1\na &gt; b\n\n1 &lt;= 2\n2 &lt;= 1\n1 &lt;= 1\na &lt;= b\n\n1 &gt;= 2\n2 &gt;= 1\n1 &gt;= 1\na &gt;= b\n\n1 == 1\n2 == 1\na == b\n\n1 !=1\n2 != 1\na != b\n\n\n\n\nOperadores lógicos:\n\n\nR\nPython\nMATLAB/Octave\n\n\n\n\n1 &lt; 2 && 2 == 1\n1 &lt; 2 && 2 &lt; 3\na &lt; b && b &gt;= 0\n\n1 &lt; 2 | 2 &lt; 1\na &lt; b | b &gt;= 0\n\n!(1 &lt; 2)\n!(1 &lt; 2 && 2 == 1)\n\n\n\n\n1 &lt; 2 and 2 == 1\n1 &lt; 2 and 2 &lt; 3\na &lt; b and b &gt;= 0\n\n1 &lt; 2 or 2 &lt; 1\na &lt; b or b &gt;= 0\n\n!(1 &lt; 2)\n!(1 &lt; 2 and 2 == 1)\n\n\n\n\n1 &lt; 2 && 2 == 1\n1 &lt; 2 && 2 &lt; 3\na &lt; b && b &gt;= 0\n\n1 &lt; 2 | 2 &lt; 1\na &lt; b | b &gt;= 0\n\n~(1 &lt; 2)\n~(1 &lt; 2 && 2 == 1)"
  },
  {
    "objectID": "06-introR.html#archivos-scripts-y-funciones",
    "href": "06-introR.html#archivos-scripts-y-funciones",
    "title": "Apéndice A: Introducción a los lenguajes de programación",
    "section": "Archivos scripts y funciones\n",
    "text": "Archivos scripts y funciones\n\nUna de las ventajas de estos lenguajes es que pueden utilizarse sin necesidad de introducir los comandos uno a uno en la consola, para ello se pueden utilizar dos tipos de archivos:\n\nArchivos de comandos (script). Son sencillamente un conjunto de comandos de cada lenguaje aglutinados en un fichero con extensión adecuada (.R para R, .py para Python y .mpara MATLAB/Octave. Estos archivos se puede ejecutar desde la consola.\nFunciones. Son en realidad comandos escritos en un fichero con la extensión adecuada y que tienen una cierta unidad. Cada función es un objeto, y se pueden almacenar varias funciones en un solo archivo, excepto en MATLAB/Octave, que tiene que almacenarse cada función en archivos separados. Lo que distingue una función de un script es que la función tiene una sintaxis determinada y definen un trozo de código independiente que realiza una tarea concreta, Todas las variables definidas en una función son locales, es decir, están definidas mientras se ejecuta la función y solo son accesibles desde dentro de la función. Las funciones se comunican con el resto del código mediante las entradas y una única salida.\n\nLa sintaxis general de funciones en cada lenguaje es la siguiente:\n\n\nR\nPython\nMATLAB/Octave\n\n\n\n\nNombre &lt;- function(entrada1=valor_defecto1, entrada2=valor_defecto2,\n                   ...){\n    # Comandos\n    return(salida)\n}\n\n\n\n\ndef Nombre(entrada1=valor_defecto1, entrada2=valor_defecto2, ...):\n    # Comandos\n    return salida1, salida2, ... \n\n\n\n\nfunction [salida1, salida2, ...] = Nombre(entrada1, entrada2, ...)\n    # Comandos\nend\n\n\n\n\nPropiedades:\n\nLas funciones R solo tienen una salida. Los demás lenguajes permiten varias salidas de cualquier tipo.\nLos objetos definidos dentro de la función son locales.\nLas funciones suelen llamar a otras funciones, e incluso pueden llamarse a sí mismas. Esta característica hay que utilizarla con mucho cuidado, pues se entra con facilidad en bucles infinitos.\nEs muy útil definir valores por defecto de las entradas, siempre que sea posible.\n\n\n\nResuelve\nSolución\nR\nPython\nMATLAB/Octave\n\n\n\n\nEscribe una función que se llame generar para generar números aleatorios de una distribución normal. La función debe tener como argumento de entrada la dimensión del vector y debe devolver como argumento de salida el vector aleatorio. Prueba la función para generar vectores aleatorios de dimensión 10, 100, 1000 y 10000.\n\n\n\n## Media:  -0.465952425454804  /  Varianza:  1.10432621445651\n\n\n\n## Media:  -0.102445117083682  /  Varianza:  1.07330804589228\n\n\n\n## Media:  -0.000774454870652572  /  Varianza:  1.02527041360597\n\n\n\n## Media:  -0.0114210270928876  /  Varianza:  0.999263792078524\n\n\n\n\n\n\n\ngenerar &lt;- function(n = 3000){\n    a &lt;- rnorm(n)\n    cat(paste(\"Media: \", mean(a), \" /  Varianza: \", var(a)))\n    hist(a)\n    return(a)\n}\na &lt;- generar(10)\na &lt;- generar(100)\na &lt;- generar(1000)\na &lt;- generar(10000)\n\n\n\n\ndef generar(n = 3000):\n    a = np.random.normal(0, 1, (n))\n    print(\"Media: \" + f\"{np.mean(a):.4f}\"  + \"   /   Varianza: \" + f\"{np.var(a):.4f}\")\n    plt.hist(a)\n    return a\na = generar(10)\na = generar(100)\na = generar(1000)\na = generar(10000)\n\n\n\n\nfunction a = generar(n)\n    if nargin &lt; 1\n        n = 3000;\n    end\n    a = randn(n, 1);\n    ['Media: ' num2str(mean(a)) '   /   Varianza: ' num2str(var(a))]\n    hist(a)\nend\na = generar(10);\na = generar(100);\na = generar(1000);\na = generar(10000);"
  },
  {
    "objectID": "06-introR.html#bifurcaciones",
    "href": "06-introR.html#bifurcaciones",
    "title": "Apéndice A: Introducción a los lenguajes de programación",
    "section": "Bifurcaciones",
    "text": "Bifurcaciones\nA menudo interesa que, dependiendo de determinadas condiciones, se ejecute una parte de código o bien otra diferente. Para ello se utilizan las bifurcaciones. La sintaxis es:\n\n\nR\nPython\nMATLAB/Octave\n\n\n\n\nif (condición1){\n    # Comandos cuando la condición1 es cierta\n} else if (condición2){\n    # Comandos cuando la condición2 es cierta\n...\n} else if (condiciónN){\n    # Comandos cuando la condiciónN es cierta\n} else{\n    # Comandos si ninguna de las anteriores es cierta\n}\n\n\n\n\nif condición1:\n    # Comandos cuando la condición1 es cierta\nelif condición2:\n    # Comandos cuando la condición2 es cierta\n...\nelif condiciónN:\n    # Comandos cuando la condiciónN es cierta\nelse:\n    # Comandos si ninguna de las anteriores es cierta\n\n\n\n\nif condición1\n    # Comandos cuando la condición1 es cierta\nelseif condición2\n    # Comandos cuando la condición2 es cierta\n...\nelseif condiciónN\n    # Comandos cuando la condiciónN es cierta\nelse\n    # Comandos si ninguna de las anteriores es cierta\nend\n\n\n\n\nEjemplo:\n\n\nR\nPython\nMATLAB/Octave\n\n\n\n\ni = 1\nif (i == 1) print(paste(\"i vale\", i))\ni = 0\nif (i == 1){\n    print(\"i vale 1\")\n} else {\n    print(\"i no vale 1\")\n}\n\n\n\n\ni = 1\nif i == 1:\n    print('i vale' + i)\ni = 0\nif i == 1:\n    print('i vale 1')\nelse:\n    print('i no vale 1')\n\n\n\n\ni = 1;\nif i == 1\n    ['i vale ', num2str(i)]\ni = 0;\nif i == 1\n    ['i vale 1']\nelse\n    ['i no vale 1']\nend"
  },
  {
    "objectID": "06-introR.html#bucles-for-y-while",
    "href": "06-introR.html#bucles-for-y-while",
    "title": "Apéndice A: Introducción a los lenguajes de programación",
    "section": "Bucles for y while\n",
    "text": "Bucles for y while\n\nSe utilizan para repetir una tarea un número de veces. Los bucles se pueden anidar. La sintaxis es:\n\n\nR\nPython\nMATLAB/Octave\n\n\n\n\nfor (variable in conjunto){\n    # Comandos que se repiten\n}\n\n\n\n\nfor variable in conjunto:\n    # Comandos que se repiten\n\n\n\n\nfor variable = conjunto\n    % Comandos que se repiten\nend\n\n\n\n\nEjemplo:\n\n\nR\nPython\nMATLAB/Octave\n\n\n\n\nfor (year in 2009 : 2012){\n    print(paste(\"El año es:\", year))\n}\n\n\n\n\nfor year in range(2009, 2013):\n    print(\"El año es: \" + str(year))\n\n\n\n\nfor year = 2009 : 2013\n    ['El año es: ' + num2str(year)]\nend\n\n\n\n\nEn los bucles tipo while debe asegurarse que la condición se incumple en algún caso para evitar caer en un bucle infinito. Sintaxis:\n::: panel-tabset ## R\n\nwhile (condición){\n    # Comandos cuando la condición es cierta\n    # Cambio para que no se cumpla la condición y se acabe bucle\n}"
  },
  {
    "objectID": "06-introR.html#python-26",
    "href": "06-introR.html#python-26",
    "title": "Apéndice A: Introducción a los lenguajes de programación",
    "section": "Python",
    "text": "Python\n\nwhile condición:\n    # Comandos cuando la condición es cierta\n    # Cambio para que no se cumpla la condición y se acabe bucle"
  },
  {
    "objectID": "06-introR.html#matlaboctave-26",
    "href": "06-introR.html#matlaboctave-26",
    "title": "Apéndice A: Introducción a los lenguajes de programación",
    "section": "MATLAB/Octave",
    "text": "MATLAB/Octave\n\nwhile condición\n    % Comandos cuando la condición es cierta\n    % Cambio para que no se cumpla la condición y se acabe bucle\nend\n\n\n\nResuelve\nSolución\n\n\n\n¿Cuál es la diferencia entre estos dos comandos?\n\n# R\ni = 1\nwhile (i == 1) \n    print(\"Esta condición es cierta\")\nif (i == 1) \n    print(\"Esta condición es cierta\")\n\n\n# Python\ni = 1\nwhile i == 1:\n    print(\"Esta condición es cierta\")\nif i == 1:\n    print(\"Esta condición es cierta\")\n\n\n# MATLAB/Octave\ni = 1;\nwhile i == 1\n    ['Esta condición es cierta']\nif (i == 1)\n    ['Esta condición es cierta']\n\n\n\nLa primera condición (con while) produce un bucle infinito, puesto que la condición siempre es cierta.\nLa segunda condición (if) simplemente mostrará por pantalla el la cadena Esta condición es cierta, puesto que se cumple la condición, pero lo hará una sola vez.\n\n\n\n\n\n\nResuelve\nR\nPython\nMATLAB/Octave\n\n\n\nEscribe una función generar2 basada en la creada anteriormente a la que añades un segundo argumento de entrada que indique el tipo de distribución que quiera el usuario. Este argumento de entrada se llamará dist y será una cadena de caracteres que tomará dos valores, unif para distribuciones uniformes o norm para una distribución gaussiana. Elimina todas las salidas por pantalla.\nPruébala generando vectores aleatorios de dimensión 5.000 de las dos distribuciones.\n\n\n\ngenerar2 &lt;- function(n = 3000, dist = \"unif\"){\n    if (dist == \"unif\"){\n        a &lt;- runif(n)\n    } else {\n        a &lt;- rnorm(n)\n    }\n    return(a)\n}\na &lt;- generar2(5000, \"unif\")\nhist(a)\na &lt;- generar2(5000, \"pp\")\nhist(a)\n\n\n\n\ndef generar2(n = 3000, dist = \"unif\"):\n    if dist == \"unif\":\n        a = np.random.uniform(0, 1, (n))\n    } else {\n        a = np.random.normal(0, 1, (n))\n    }\n    return a\n}\na = generar2(5000, \"unif\")\nhist(a)\na = generar2(5000, \"pp\")\nhist(a)\n\n\n\n\nfunction a = generar2(n, dist)\n    if nargin &lt; 1\n        n = 3000;\n    end\n    if nargin &lt; 2\n        dist = 'unif';\n    end\n    if (dist == 'unif'){\n        a = rand(n, 1);\n    else\n        a = randn(n, 1);\n    end\nend\na = generar2(5000, 'unif')\nhist(a)\na = generar2(5000, 'pp')\nhist(a)\n\n\n\n\n\n\n\nResuelve\nSolución\nR\nPython\nMATLAB/Octave\n\n\n\nEXPERIMENTO DE MONTECARLO: Calcula 10.000 medias y varianzas de muestras aleatorias de longitud 5.000 de una distribución uniforme. Almacena las medias en el vector media y las varianzas en el vector varianza.\nTEOREMA CENTRAL DEL LÍMITE: Muestra que la distribución empírica de la media muestral es normal. Muestra además la distribución de la varianza muestral.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nN &lt;- 10000\nmedia &lt;- rep(0, N)\nvarianza &lt;- media\nfor (i in 1 : N){\n    y &lt;- generar2(5000, \"unif\")\n    media[i] &lt;- mean(y)\n    varianza[i] &lt;- var(y)\n}\nhist(media)\nhist(varianza)\n\n\n\n\nN = 10000\nmedia = np.full(0, (N))\nvarianza &lt;- media\nfor i in range(N):\n    y = generar2(5000, \"unif\")\n    media[i] = mean(y)\n    varianza[i] = var(y)\nhist(media)\nhist(varianza)\n\n\n\n\nN = 10000;\nmedia = repmat(0, N, 1);\nvarianza = media;\nfor i = 1 : N\n    y = generar2(5000, \"unif\");\n    media(i) = mean(y);\n    varianza(i) = var(y);\nend\nhist(media)\nhist(varianza)\n\n\n\n\n\n\n\nResuelve\nR\nPython\nMATLAB/Octave\n\n\n\nEjercicio avanzado\nEscribe una función fact que calcule el factorial de un número y comprueba que funciona con varios ejemplos.\nEscribe otra función fact2 en la que se llame a sí misma, teniendo en cuenta que \\(n! = n (n - 1)!\\). Recuerda que para evitar llamadas infinitas hay que poner una condición de parada.\n\n\n\nfact &lt;- function(n){\n    resultado = 1\n    for (i in 2 : n)\n        resultado = resultado * i\n    return(resultado)\n}\n\nfact2 &lt;- function(n){\n    if (n == 2)\n        resultado &lt;- 2\n    else\n        resultado &lt;- n * fact(n - 1)\n    return(resultado)\n}\nfact(5)\nfact2(5)\n\n\n\n\ndef fact(n):\n    resultado = 1\n    for i in range(2, n):\n        resultado = resultado * i\n    return resultado\n\ndef fact2(n):\n    if n == 2:\n        resultado = 2\n    else:\n        resultado = n * fact2(n - 1)\n    return resultado\nfact(5)\nfact2(5)\n\n\n\n\nfunction resultado = fact(n)\n    resultado = 1;\n    for i = 2 : n\n        resultado = resultado * i;\nend\n\nfunction resultado = fact2(n)\n    if n == 2\n        resultado = 2;\n    else\n        resultado = n * fact2(n - 1);\nend\nfact(5)\nfact2(5)"
  },
  {
    "objectID": "08-tutorialUComp.html#objetos-de-series-temporales",
    "href": "08-tutorialUComp.html#objetos-de-series-temporales",
    "title": "Apéndice B: Tutorial de UComp",
    "section": "Objetos de series temporales",
    "text": "Objetos de series temporales\nEn este libro y en la librería UComp hemos adoptado siempre una versión lo más sencilla posible de tratar objetos con fechas en cada uno de los lenguajes. Se comenta a continuación cada uno de los casos.\nLa librería se suministra con tres series temporales que sirven de ejemplo para todos los capítulos del libro. Se trata de airpas, las llegadas mensuales a España de viajeros extranjeros por avión; gdp, el PIB trimestral español en términos reales; e ipi, el índice de producción industrial mensual español.\n\n\nR\nPython\nMATLAB/Octave\n\n\n\nEn R UComp prioriza el uso de objetos tipo ts, aunque también está preparada para recibir objetos tipo tsibble, pero nunca devuelve objetos de esta clase. Con los objetos ts se pueden utilizar las funciones estándar, como son head, tail, window, etc. De hecho, se debe evitar cortar objetos ts con el operador de slicing habitual (:) como si fueran vectores o matrices estándar porque lo que se obtiene como respuesta es un vector o matriz, nunca una serie temporal.\nEl siguiente código muestra un ejemplo en el que se genera una serie temporal mensual comenzando en enero de 2000. Se genera una variable yVector que NO es una serie temporal, sino un vector, y otra variable yTs que SÍ se una serie temporal porque se usa el comando window.\n\nlibrary(UComp)\ny = ts(runif(100), start=c(2000, 1), frequency=12)\nyVector = y[10 : 21]\nyTs = window(y, c(2003, 1), c(2003, 12))\n\n\n\nEn Python UComp utiliza objetos de series temporales de pandas, lo que significa que todas las funciones estándar de esa librería se pueden utilizar para su manejo.\nAdicionalmente, y para dar una sensación de uso similar a R, se han añadido dos funciones que pueden ser de gran utilidad. Se trata de ts y window.\nEl siguiente código muestra un ejemplo en el que se genera una serie temporal mensual comenzando en enero de 2000 y se genera una serie yTs que contiene solo el año 2003.\n\nimport pandas as pd\nfrom UComp import *\n\ny = ts(np.random.rand(100), start=\"2000-01-31\", freq=\"m\")    \nyTs = window(y, \"2003-01-31\", \"2003-12-31\")\n\n\n\nEn MATLAB/Octave no se utilizan objetos de series temporales en la libraría UComp, aunque algunas de las salidas de las funciones sí que devuelven objetos tipo table con índices de filas que son fechas. Esto provoca que el usuario tenga que utilizar reiteradamente en muchas de las funciones el período estacional como entrada. Es necesario estar alerta, porque puede provocar errores."
  },
  {
    "objectID": "08-tutorialUComp.html#familias-de-modelos-de-ucomp",
    "href": "08-tutorialUComp.html#familias-de-modelos-de-ucomp",
    "title": "Apéndice B: Tutorial de UComp",
    "section": "Familias de modelos de UComp\n",
    "text": "Familias de modelos de UComp\n\nEl nombre de la librería obedece al proyecto inicial que consistía en implementar modelos de componentes no observables de forma automática (Unobserved Components en inglés, ver Pedregal, 2022). Más tarde el proyecto se amplió incluyendo los modelos de suavizado exponencial con una sola fuente de error (Single Source Of Error, SSOE) y los mismos con múltiples fuentes de error (Multiple Sources Of Error, MSOE).\nLos nombres de las funciones y la sintaxis de todas ellas es muy similar, como se puede ver en la Tabla 1, en la que se indican los nombres de las funciones principales que se encargan de la modelización para cada uno de las familias de modelos. Los nombres de las funciones indican la acción y van precedidos por un asterisco que puede ser sustituido por ETS, PTS o UC para el suavizado exponencial SSOE, MSOE y componentes no observables, respectivamente.\n\n\nTabla 1: Funciones principales de UComp.\n\n\n\n\n\nFunciones\nDescripción\n\n\n\n\nETS, PTS, UC\n\nCorre todas las demás funciones para realizar una modelización completa\n\n\n*model\nIgual que el anterior, pero solo corre la estimación y predicción para conseguir una ejecución más rápida cuando solo se requiere predicción\n\n\n*components\nEstima los componentes óptimos de cada familia de modelos\n\n\n*validate\nCalcula y muestra la tabla de estimación con algunos tests de diagnóstico\n\n\n\n\nAlgunas aclaraciones:\n\nEn todas aquellas situaciones en las que la velocidad de ejecución no sea una restricción, lo más cómodo es ejecutar directamente ETS, PTS o UC.\nLa sintaxis de todas las funciones es muy parecida, aunque no idéntica. Los argumentos de entrada comunes a las funciones principales se encuentran en la Tabla 2.\nPara los modelos de componentes no observables existen funciones adicionales, UCfilter, UCsmooth y UCdisturb que corren el filtro de Kalman, el algoritmo de suavizado o el algoritmo conocido como disturbance para obtener los valores óptimos de los estados y/o perturbaciones del modelo (ver Pedregal, 2022).\n\n*components y *validate son funciones que admiten de entrada un modelo y devuelven otro modelo, lo habitual será queq el modelo de entrada y salida sean el mismo. En Python las funciones components y validate son en realidad métodos de los objetos correspondientes (ver ejemplos más abajo).\n\n\n\nTabla 2: Argumentos de entrada comunes a ETS, PTS, UC, ETSmodel, PTSmodel y UCmodel.\n\n\n\n\n\nNombre\nDescripción\n\n\n\ny\nSerie temporal univariante. Es el único argumento de entrada obligatorio. Si es un vector sin fechas, el argumento s también será obligatorio\n\n\ns\nPeriodo estacional, número de observaciones por año. En MATLAB/Octave es una entrada obligatoria, puesto que en ese caso y es un vector\n\n\nu\nMatriz de variables exógenas de entrada.\n\n\nmodel\nModelo que se desea estimar. Si no se especifica ninguno concreto se identifica el mejor de entre todos los modelos disponibles dentro de cada familia. Ver la ayuda para familiarizarse con las posibilidades en cada caso\n\n\nh\nHorizonte de predicción\n\n\ncriterion\nCriterio de información para identificación automática. Puede ser aic, bic o aicc\n\n\n\nlambda\nParámetro \\(\\lambda\\) de la transformación Box-Cox. NULL or NaN para estimarlo\n\n\narmaIdent\nIdentificación de modelos con ruido ARMA\n\n\nverbose\nMuestra resultados intermedios a medida que se identifica o se estiman los modelos\n\n\n\n\nEl término lambda es una palabra reservada en Python, por lo que dicho argumento se llama lambdaBoxCox.\nLos modelos ETS cuentan además con dos parámetros adicionales para la estimación de intervalos de confianza mediante simulación bootstrap, se trata de bootstrap y nSimul.\nLos modelos UC permiten la identificación automática de atípicos mediante la entrada outlier que indica el número de desviaciones típicas a partir del cual se analiza si los residuos que lo sobrepasan son o no atípicos.\nLa salida de las funciones es un objeto (lista o estructura) con una serie de campos. Los más habituales se muestran en la Tabla 3.\n\n\nTabla 3: Argumentos de salida comunes a ETS, PTS, UC.\n\n\n\n\n\nNombre\nDescripción\n\n\n\nyFor\nVector de predicciones óptimas\n\n\nyForV\nVarianza de predicciones para intervalos de confianza\n\n\ncomp\nComponentes estimados\n\n\n\n\nA continuación se muestran algunos ejemplos sencillos de las funciones anteriores.\n\n\nEjemplo\nR\nPython\nMATLAB/Octave\n\n\n\nEstimación de un modelo ETS y PTS para la serie de pasajeros de avión españoles con los datos transformados en logaritmos desde 2008 hasta diciembre de 2018. Las predicciones se estiman para el año 2019 (h) y se muestra la tabla de estimación del modelo ETS, así como los resultados intermedios (verbose).\n\n\n--------------------------------------------------------\n    Model            AIC           BIC          AICc\n--------------------------------------------------------\n   (A,N,N):        0.9397        0.9834        0.9397\n   (A,N,A):       -1.6453       -1.3177       -1.6150\n   (A,A,N):        0.9697        1.0570        0.9697\n   (A,A,A):       -1.6208       -1.2495       -1.5829\n  (A,Ad,N):        0.9835        1.0927        0.9835\n  (A,Ad,A):       -1.6027       -1.2096       -1.5573\n   (M,N,N):        0.9400        0.9837        0.9400\n   (M,N,A):       -1.6297       -1.3021       -1.5994\n   (M,N,M):       -1.6182       -1.2906       -1.5879\n   (M,A,N):        0.9691        1.0565        0.9691\n   (M,A,A):       -1.6051       -1.2339       -1.5673\n   (M,A,M):       -1.5937       -1.2225       -1.5558\n  (M,Ad,N):        0.9824        1.0915        0.9824\n  (M,Ad,A):       -1.6224       -1.2293       -1.5770\n  (M,Ad,M):       -1.6153       -1.2222       -1.5698\n--------------------------------------------------------\n  Identification time:    0.08300 seconds\n--------------------------------------------------------\n -------------------------------------------------------------\n  Model: ETS(A,N,A)\n  Box-Cox lambda: 1.00\n  Q-Newton: Gradient convergence.\n -------------------------------------------------------------\n                   Param        S.E.          |T|     |Grad|\n -------------------------------------------------------------\n      Alpha:       0.8060     0.0827       9.7450  1.382e-10\n      Gamma:    1.000e-08  1.285e-02       0.0000  6.848e-13\n -------------------------------------------------------------\n   AIC:      -1.6453   BIC:      -1.3177   AICc:      -1.6150\n            Log-Likelihood:     123.5892\n -------------------------------------------------------------\n    Summary statistics:\n -------------------------------------------------------------\n         Missing data:      \n         Q( 1):       0.0008         Q( 4):       5.1667\n         Q( 8):      21.5931         Q(12):      33.3406\n   Bera-Jarque:      90.3046       P-value:       0.0000\n       H(  46):       0.5186       P-value:       0.0281\n   Outliers (&gt;2.7 ES):     4\n         Q( 1):       0.4329         Q( 4):       1.0481\n         Q( 8):      14.7442         Q(12):      37.5224\n   Bera-Jarque:       4.8024       P-value:       0.0906\n       H(  44):       0.4644       P-value:       0.0124\n -------------------------------------------------------------\n\n\n------------------------------------------------------------\n Identification of PTS models:\n------------------------------------------------------------\n    Model            AIC           BIC          AICc\n------------------------------------------------------------\n  (N,N,N):       -1.0470       -1.0033       -1.0470\n  (A,N,N):       -0.1044       -0.0389       -0.1044\n  (N,N,L):       -2.7776       -2.4500       -2.7473\n  (A,N,L):       -2.7854       -2.4360       -2.7551\n  (N,N,D):       -2.6872       -2.2504       -2.6341\n  (A,N,D):       -2.7559       -2.2972       -2.6953\n  (N,A,N):       -0.8857       -0.7983       -0.8857\n  (A,A,N):       -0.1134       -0.0042       -0.1134\n  (N,A,L):       -2.6685       -2.2972       -2.6306\n  (A,A,L):       -2.6834       -2.2903       -2.6380\n  (N,A,D):       -2.5964       -2.1160       -2.5283\n  (A,A,D):       -2.7185       -2.2162       -2.6427\n (N,Ad,N):       -0.9647       -0.8773       -0.9647\n (A,Ad,N):       -0.1642       -0.0550       -0.1642\n (N,Ad,L):       -2.7473       -2.3760       -2.7094\n (A,Ad,L):       -2.7588       -2.3657       -2.7134\n (N,Ad,D):       -2.6737       -2.1932       -2.6055\n (A,Ad,D):       -2.7955       -2.2932       -2.7198\n------------------------------------------------------------\n  Identification time:    1.52861 seconds\n------------------------------------------------------------\n-------------------------------------------------------------\n  Box-Cox lambda: 1.00\n  Model: (A,N,L)\n  Periods: \n  Q-Newton: Function convergence\n  (*)  concentrated out parameters\n -------------------------------------------------------------\n                      Param   asymp.s.e.        |T|     |Grad| \n -------------------------------------------------------------\n        Level:     8.30e-04*  \n         Seas:     1.23e-05     2.03e-05     0.6034   5.37e-05\n    Irregular:     1.81e-04     8.33e-05     2.1778   6.90e-05\n -------------------------------------------------------------\n   AIC:      -2.7854   BIC:      -2.4360   AICc:      -2.7551\n            Log-Likelihood:     199.8394\n -------------------------------------------------------------\n    Summary statistics:\n -------------------------------------------------------------\n         Missing data:      \n         Q( 1):       0.2597         Q( 4):       8.0362\n         Q( 8):      28.0654         Q(12):      47.5452\n   Bera-Jarque:      10.7759       P-value:       0.0046\n       H(  40):       0.6375       P-value:       0.1588\n   Outliers (&gt;2.7 ES):     4\n         Q( 1):       0.3479         Q( 4):       7.8717\n         Q( 8):      26.9195         Q(12):      52.1301\n   Bera-Jarque:       5.0253       P-value:       0.0811\n       H(  40):       0.3787       P-value:       0.0027\n -------------------------------------------------------------\n\n\nLos componentes estimados del modelo ETS se muestran a continuación, así como los valores reales junto con ambas predicciones y los intervalos de confianza al 95% del modelo PTS.\n\n\n\n\n\n\n\n\n\n\n\n# Borrando memoria\nrm(list = ls())\n# Cargando librerías\nlibrary(UComp)\nlibrary(ggplot2)\n# Cortando serie temporal\ny = log(window(airpas, c(2008, 1), c(2018, 12)))\n# Estimando modelos\nmETS = ETS(y, h=12, verbose=TRUE)\nmPTS = PTS(y, h=12, verbose=TRUE)\n# Gráficos de componentes\nplot(mETS)\nplot(mPTS)\n# Gráficos de predicciones\npredicciones = cbind(exp(mETS$yFor),\n                     exp(mPTS$yFor),\n                     exp(mPTS$yFor - 2 * sqrt(mPTS$yForV)), \n                     exp(mPTS$yFor + 2 * sqrt(mPTS$yForV)))\nautoplot(window(airpas, c(2018, 1), c(2019, 12))) + \n         autolayer(predicciones)\n\n\n\n\n# Cargando librerías\nfrom UComp import *\n# Cortando serie temporal\ny = np.log(window(airpas, \"2008-01-31\", \"2018-12-31\"))\n# Estimando modelos\nmETS = ETS(y, h=12, verbose=True)\nmPTS = PTS(y, h=12, verbose=True)\n# Gráficos de componentes\nmETS.plot()\nmPTS.plot()\n# Gráficos de predicciones\npredicciones = pd.concat((np.exp(mETS.yFor),\n                     np.exp(mPTS.yFor),\n                     np.exp(mPTS.yFor - 2 * np.sqrt(mPTS.yForV)), \n                     np.exp(mPTS.yFor + 2 * np.sqrt(mPTS.yForV))),\n                     axis=1)\nplt.plot(window(airpas, \"2018-01-31\", \"2019-12-31\"))\nplt.plot(predicciones)\n\n\n\n\n# Borrando memoria\nclear all\nload data\n# Cortando serie temporal\ny = log(airpas(39 * 12 + 1 : 50 * 12));\n# Estimando modelos\nmETS = ETS(y, 12, h=12, verbose=true);\nmPTS = PTS(y, 12, h=12, verbose=true);\n# Gráficos de componentes\nstackedplot(mETS.comp)\nstackedplot(mPTS.comp)\n# Gráficos de predicciones\nt = (49 * 12 + 1 : 51 * 12);\ntf = (50 * 12 + 1 : 51 * 12);\npredicciones = [exp(mETS.yFor) exp(mPTS.yFor) ...\n                exp(mPTS.yFor - 2 * sqrt(mPTS.yForV)) ...\n                exp(mPTS.yFor + 2 * sqrt(mPTS.yForV))];\nplot(t, airpas(t), tf, predicciones)"
  },
  {
    "objectID": "08-tutorialUComp.html#funciones-de-apoyo-a-la-modelización",
    "href": "08-tutorialUComp.html#funciones-de-apoyo-a-la-modelización",
    "title": "Apéndice B: Tutorial de UComp",
    "section": "Funciones de apoyo a la modelización",
    "text": "Funciones de apoyo a la modelización\nLa Tabla 5 muestra una serie de funciones que ayudan en el diagnóstico de los modelos. Estas funciones se encuentran disponibles en R y Python. En MATLAB es recomendable utilizar la función toolTEST de la toolbox ECOTOOL, ver Pedregal (2019).\n\n\nTabla 4: Funciones de apoyo a la modelización disponibles en R y Python.\n\n\n\n\n\nNombre\nDescripción\n\n\n\ntests\nCorre sumStats, gaussTest, ident, cusum y varTest, y muestra un resumen gráfico\n\n\nsumStats\nEstadísticos descriptivos de un conjunto de series temporales\n\n\ngaussTest\nTests gráficos y formales de normalidad. Shapiro-Wilks, QQ-plot e histograma con distribución normal teórica además de estimación no paramétrica de la distribución empírica\n\n\nident\nAutocorrelograma simple y parcial\n\n\ncusum\nTests CUSUM de constancia de la media y CUSUM squared para constancia de la varianza\n\n\nvarTest\nTest de igualdad de varianzas\n\n\ntsDisplay\nMuestra un gráfico de la serie junto con los autocorrelogramas\n\n\n\n\nEn R se han implementado algunas funciones adicionales complementarias: size para calcula el tamaño de un objeto, residuals para extraer los residuos de un modelo, colMedians y rowMedians para calcular la mediana de una matriz por columnas o filas, y plus_one para calcular la fecha siguiente a la última observación de un objeto ts.\nAdicionalmente, existen otras tres funciones que proporcionan comparaciones predictivas de modelos:\n\n\nslide: produce h predicciones recursivas con un conjunto de modelos implementados en una función definida por el usuario (entrada forecFun). Las predicciones se calculan comenzando en un origen inicial (orig), que se va incrementando en cada iteración en un número de observaciones dadas por step. Es posible utilizar ventanas de dimensión fija mediante la entrada window y es posible utilizar computación en paralelo (parallel). La función devuelve una hiper-matriz de dimensiones \\(h \\times \\text{nOrigen} \\times \\text{nModelos} \\times \\text{nSeries}\\), donde nOrigen, nModelos y nSeries son el número de orígenes, de modelos y de series que intervienen en la estimación.\n\nplotSlide: representa gráficamente los errores de predicción estimados con slide.\n\nAccuracy: Estima determinadas métricas de error para un conjunto de series temporales y varios modelos alternativos. Las métricas son la media de los errores, la raíz cuadrada del error cuadrático medio (RMSE), la media del error absoluto (MAE) y los equivalentes porcentuales. También se incluye el MAPE simétrico (sMAPE), el error escalado absoluto medio (MASE), el MAE relativo (relMAE) y el estadístico U the Theil.\n\n\n\nEjemplo\nR\nPython\nMATLAB/Octave\n\n\n\nLa tabla siguiente muestra las métricas de error estimadas con Accuracy de las predicciones dos años hacia adelante del PIB español con origen en el último trimestre del año 2018 con los modelos naive y UC.\n\n\n             ME     RMSE      MAE      MPE    PRMSE     MAPE    sMAPE     MASE\nnaive  7.568453 10.91430 8.026720 7.909775 11.84212 8.313688 7.684942 3.022146\nm$yFor 7.624897 11.76952 7.675075 8.021094 12.85615 8.068054 7.335188 2.889748\n          RelMAE Theil's U\nnaive  0.2627953  1.338894\nm$yFor 0.2512825  1.453542\n\n\nLas predicciones se representan a continuación, junto con los valores reales.\n\n\n\n\n\nLas tablas de salida y la evidencia gráfica de todos los tests de diagnóstico se muestran a continuación (tests):\n\n\nSummary statistics:\n==================\n                       Serie 1\nData points:          81.00000\nMissing:               0.00000\nMinimum:              -3.40839\n1st quartile:         -0.36312\nMean:                  0.13931\nP(Mean = 0):           0.22916\nMedian:                0.22881\n3rd quartile:          0.64080\nMaximum:               2.93774\nInterquartile range:   1.00392\nRange:                 6.34613\nSatandard deviation:   1.03468\nVariance:              1.07056\nSkewness:             -0.65851\nKurtosis:              2.28973\nAutocorrelation tests:\n=====================\n     SACF sa     LB p.val  SPACF sp\n1   0.028  .  0.066 0.797  0.028  .\n2  -0.117  .  1.225 0.542 -0.118  .\n3   0.028  .  1.292 0.731  0.035  .\n4   0.038  .  1.419 0.841  0.023  .\n5  -0.196  .  4.818 0.439 -0.194  .\n6   0.031  .  4.905 0.556  0.054  .\n7   0.013  .  4.921 0.670 -0.038  .\n8  -0.083  .  5.550 0.697 -0.069  .\n9  -0.053  .  5.814 0.758 -0.038  .\n10  0.061  .  6.166 0.801  0.007  .\n11 -0.052  .  6.428 0.843 -0.051  .\n12  0.041  .  6.593 0.883  0.057  .\n13  0.054  .  6.876 0.908  0.012  .\n14 -0.068  .  7.340 0.921 -0.080  .\n15  0.104  .  8.439 0.905  0.149  .\n16  0.072  .  8.970 0.915  0.010  .\n17 -0.023  .  9.028 0.939  0.014  .\n18 -0.120  . 10.575 0.912 -0.099  .\n19  0.077  . 11.210 0.917  0.049  .\n20 -0.049  . 11.473 0.933 -0.035  .\nGaussianity tests:\n=================\n\n    Shapiro-Wilk normality test\n\ndata:  x\nW = 0.94979, p-value = 0.003113\n\nRatio of variance tests:\n=======================\n Portion_of_data F_statistic p.value\n         0.33333      0.6538   0.285\n\n\n\n\n\n\n\n\n# Cortando serie temporal\ny = window(gdp, end = c(2018, 4))\n# Predicciones\nnaive = ts(rep(tail(y, 1), 8), start=c(2019, 1), frequency=4)\nm = UC(y, h=8)\n# Métricas de error\npy = cbind(naive, m$yFor)\nAccuracy(py, gdp)\n# Gráfico de predicciones\nautoplot(tail(gdp, 24)) + autolayer(py)\n# Diagnóstico\ntests(m)\n\n\n\n\n# Cortando serie temporal\ny = window(gdp, end = \"2018-12-31\")\n# Predicciones\nnaive = ts(y[-1].repeat(8), start=\"2019-01-31\", freq=\"q\")\nm = UC(y, h=8)\n# Métricas de error\npy = pd.concat((naive, m.yFor), axis=1)\nAccuracy(py, gdp)\n# Gráfico de predicciones\nplt.plot(gdp[-24:])\nplt.plot(py)\n# Diagnóstico\ntests(m)\n\n\n\n\n% Cargando datos\nload data\n% Cortando serie temporal\ny = gdp(1 : 96);\n% Predicciones\nnaive = repmat(y(end), 8, 1);\nm = UC(y, 12, h=8);\n% Métricas de error\npy = [naive m.yFor];\nAccuracy(py, gdp(97 : 104))\n% Gráfico de predicciones\nt = (80 : length(gdp))';\nplot(t, gdp(t), (97 : 104), py)\n% Diagnóstico\ntoolTEST(m.v)\n\n\n\n\n\n\nEjemplo\nR\nPython\nMATLAB/Octave\n\n\n\nLa figura siguiente muestra las métricas de error de la predicción recursiva del PIB español con un origen inicial en el último trimestre de 2014 y acabando en el último cuatrimestre de 2019 con ayuda de las funciones slide y plotSlide. Los modelos utilizados son naive y UC.\nPara poder realizar este gráfico es necesario crear previamente una función predice que compute las predicciones 8 periodos hacia adelante con un modelo naive y un UC. Esta función debe tener dos entradas: una serie temporal y el horizonte de predicción.\nAdemás es necesario otra función MAE que calcule el error medio absoluto en función de dos entradas: la predicción de un modelo h periodos hacia adelante y una serie temporal sobre la que se estimaron dichas predicciones.\n\n\n\n\n\n\n\n\n# función predice\npredice = function(y, h=8){\n  m = UC(y, h=8)\n  naive = ts(rep(tail(y, 1), 8), start=time(head(m$yFor, 1)), freq=frequency(y))\n  return(cbind(naive, m$yFor))\n}\n# Función MAE\nMAE = function(py, y){\n  h = length(py)\n  return(cumsum(abs(py - tail(y, h))) / (1 : h))\n}\n# Cortando serie\ny = window(gdp, end=c(2019, 4))\nh = 8\nstep = 1\n# Predicción recursiva\norig = length(window(y, end=c(2014, 4)))\npredicciones = slide(y, orig, predice, h=h, step=step)\n# Gráfico\nerrores = plotSlide(predicciones, y, orig, step, MAE)\n\n\n\n\n# función predice\ndef predice(y, h=8):\n  m = UC(y, h=8)\n  naive = ts(y[-1].repeat(8), start=m.yFor.index[0], freq=\"q\")\n  return pd.concat((naive, m.yFor), axis=1)\n\n# Función MAE\ndef MAE(py, y):\n  h = py.shape[0]\n  return np.cumsum(abs(py - y[-h:])) / np.arange(1, h + 1)\n\n# Cortando serie\ny = window(gdp, end=\"2019-12-31\")\nh = 8\nstep = 1\n# Predicción recursiva\norig = window(y, end=\"2014-12-31\").shape[0]\npredicciones = slide(y, orig, predice, h=h, step=step)\n# Gráfico\nerrores = plotSlide(predicciones, y, orig, step, MAE)\n\n\n\n\n% función predice\nfunction salida = predice(y, s, h)\n  m = UC(y, 4, h=8);\n  naive = repmat(y(end), 8);\n  salida = [naive m.yFor];\nend\n% Función MAE\nfunction salida = MAE(py, y, s)\n  h = length(py);\n  salida = cumsum(abs(py - y(end - h + 1 : end))) ./ (1 : h)';\nend\n% Cortando serie\ny = gdp(1 : 100);\nh = 8;\nstep = 1;\n% Predicción recursiva\norig = 80;\npredicciones = slide(y, 4, orig, @predice, h=h, step=step);\n% Gráfico\nerrores = plotSlide(predicciones, y, 4, orig, step, @MAE);"
  },
  {
    "objectID": "08-tutorialUComp.html#funciones-accesorias-útiles-para-modelización-arima",
    "href": "08-tutorialUComp.html#funciones-accesorias-útiles-para-modelización-arima",
    "title": "Apéndice B: Tutorial de UComp",
    "section": "Funciones accesorias útiles para modelización ARIMA",
    "text": "Funciones accesorias útiles para modelización ARIMA\nLa libraría incluye además una serie de funciones que son de utilidad en la modlización ARIMA. En este momento UComp no incluye la modelización ARIMA específicamente, pero sí que los ruidos ARMA sí que se premiten en las familias de modelos implementadas.\n\n\nTabla 5: Funciones de apoyo a la modelización ARIMA.\n\n\n\n\n\nNombre\nDescripción\n\n\n\narmaFilter\nFiltra una señal por un filtro digital tipo ARMA. En MATLAB/Octave se usa filter\n\n\n\ndif\nAplica diferencias regulares y estacionales a una serie temporal. La función en MATLAB se llama vdif\n\n\n\nroots\nCalcula las raíces de un polinomio en el operador retardo\n\n\nconv\nCalcula la convolución (producto) de polinomios en el operador retardo\n\n\nacft\nCalcula las funciones de autocorrelación simple y parcial teóricas de procesos ARMA\n\n\nzplane\nRepresenta raíces de modelos ARMA en el plano complejo\n\n\n\n\n\n\nEjemplo\nR\nPython\nMATLAB/Octave\n\n\n\nLa siguiente figura muestra el aspecto de una simulación del proceso ARIMA\\((1, 1, 0) \\times (1, 1, 1)\\) \\((1-B)(1-B^4)(1-0.8B^4)y_t=(1-0.8B)(1+0.8B^4)a_t\\), donde \\(a_t\\) es ruido blanco de longitud 100 y \\(B\\) es el operador retardo de forma que \\(B^ly_t=y_{t-l}\\). La serie tiene estacionalidad de periodo 4 y es no estacionaria en su parte regular y estacional, de tal forma que una diferencia regular y otra estacional producirían un proceso estacionario.\nUn punto interesante es que, para eliminar el efecto de las condiciones iniciales es conveniente hacer una simulación más larga de lo deseada y cortar las últimas observaciones.\n\n\n\n\n\nLa siguiente figura muestra que la primera diferencia regular y estacional es estacionaria, además de mostrar una fuerte estacionalidad.\n\n\n\n\n\nLas funciones empíricas son bastante parecidas a las teóricas calculadas con acft para el proceso ARMA estacionario, como se muestra comparando la figura anterior con la siguiente.\n\n\n\n\n\nDado que el modelo es invertible, las raíces del numerador tienen un módulo inferior a la unidad, mientras que el denominador muestra numerosas raíces fuera del círculo unidad, junto con otras múltiples dentro.\n\n\n[1]  0.6687403+0.6687403i -0.6687403+0.6687403i -0.6687403-0.6687403i\n[4]  0.8000000+0.0000000i  0.6687403-0.6687403i\n\n\n[1]  0.0000000+0.9457416i -0.9457416-0.0000000i  0.0000000-0.9457416i\n[4]  1.0000000-0.0000000i  0.0000000+1.0000000i -1.0000000-0.0000000i\n[7]  0.0000000-1.0000000i  0.9457416+0.0000000i  1.0000000+0.0000000i\n\n\n\n\n\n\n\n\n# Simulación\nset.seed(105)\na = rnorm(200)\nnumerador = conv(c(1, -0.8), c(1, 0, 0, 0, 0.8))\ndenominador = conv(c(1, -1), c(1, 0, 0, 0, -1), c(1, 0, 0, 0, -0.8))\ny = armaFilter(numerador, denominador, a) + 300\ny = ts(window(y, 101, 200), frequency = 4)\nautoplot(y)\n# Diferencias\ndy = dif(y, c(1, 1), c(1, 4))\ntsDisplay(dy)\n# Funciones de autocorrelación simple y parcial teóricas\nsalida = acft(numerador, c(1, 0, 0, 0, -0.8), 25)\n# Raíces de los polinomios y plano imaginario\nprint(roots(numerador))\nprint(roots(denominador))\nzplane(numerador, denominador)\n\n\n\n\n# Simulación\na = np.random.normal(0, 1, 200)\nnumerador = conv([1, -0.8], [1, 0, 0, 0, 0.8]);\ndenominador = conv(conv([1, -1], [1, 0, 0, 0, -1]), [1, 0, 0, 0, -0.8]);\ny = armaFilter(numerador, denominador, a) + 300\ny = ts(y[100:], freq=\"q\")\nplt.plot(y)\n# Diferencias\ndy = dif(y, [1, 1], [1, 4])\ntsDisplay(dy)\n# Funciones de autocorrelación simple y parcial teóricas\nsalida = acft(numerador, [1, 0, 0, 0, -0.8], 25)\n# Raíces de los polinomios y plano imaginario\nprint(roots(numerador))\nprint(roots(denominador))\nzplane(numerador, denominador)\n\n\n\n\n% Simulación\na = randn(100);\nnumerador = conv([1 -0.8], [1 0 0 0 0.8]);\ndenominador = conv(conv([1 -1], [1 0 0 0 -1]), [1 0 0 0 -0.8]);\ny = filter(numerador, denominador, a) + 300;\ntoolTEST(y)\n% Diferencias\ndy = vdif(y, [1 1], [1 4]);\ntoolTEST(dy)\n% Funciones de autocorrelación simple y parcial teóricas\nacft(numerador, [1 0 0 0 -0.8], 25, 4);\n% Raíces de los polinomios y plano imaginario\nroots(numerador)\nroots(denominador)\nclose all\nzplane(numerador, denominador)"
  },
  {
    "objectID": "08-tutorialUComp.html#bibliografía",
    "href": "08-tutorialUComp.html#bibliografía",
    "title": "Apéndice B: Tutorial de UComp",
    "section": "Bibliografía",
    "text": "Bibliografía\nPedregal, D. J. (2022). Automatic Identification and Forecasting of Structural Unobserved Components Models with UComp. Journal of Statistical Software, 103(1), 1–33. https://doi.org/10.18637/jss.v103.i09\nPedregal DJ (2019) Time series analysis and forecasting with ECOTOOL. PLOS ONE 14(10): e0221238. https://doi.org/10.1371/journal.pone.0221238"
  },
  {
    "objectID": "07-basicsts.html#procesos-estocásticos",
    "href": "07-basicsts.html#procesos-estocásticos",
    "title": "\n2  Conceptos y herramientas básicas\n",
    "section": "\n2.1 Procesos estocásticos",
    "text": "2.1 Procesos estocásticos\nPara hacer un tratamiento estadístico más riguroso en este libro necesitamos una definición más técnica de serie temporal. Desde este punto de vista definimos un proceso estocástico como una sucesión de variables aleatorias con una distribución de probabilidad dada indexadas por el tiempo. Cada una de las variables aleatorias del proceso tiene su propia función de distribución de probabilidad y pueden o no estar correlacionadas entre sí. Una serie temporal es una realización de dicho proceso estocástico, es decir, una sucesión de valores que son extraídos aleatoriamente de la distribución correspondiente. En el panel superior de la Figura 2.1 se observa un proceso estocástico en el que la ley de probabilidad se mantiene constante en el tiempo, aunque la realización concreta (puntos) no son una constante. En el panel inferior se puede ver lo complicado que puede ser un proceso estocástico, puesto que en ese caso la función de distribución es distinta para cada momento del tiempo, y ¡además el intervalo de muestro no es fijo!\n\n\nFigura 2.1: Dos procesos estocásticos con diferentes características.\n\n\n\nResuelve\nSolución\nR\nPython\nMATLAB/Octave\n\n\n\nBorra toda la memoria. Carga las herramientas del libro. Simula un ruido blanco normal de dimensión 300 con media 100, represéntala gráficamente. Comprueba que la media es 100.\nEn R puedes usar las funciones rm, library(UComp), library(ggplot2)), rnorm, autoplot y sumStats. autoplot función solo representa objetos de series temporales, que puedes convertir con la función as.ts.\nEn Python tendrás que usar, por ejemplo, from UComp import *, np.random.normal, plt.plot, sumStats. Para convertir matrices o vectores numpy a series termporales de pandas puedes usar la función ts.\nEn MATLAB/OCtave puedes usar randn, plot y sumStats.\n\n\n\n\n\n\n##                         Serie 1\n## Data points:          300.00000\n## Missing:                0.00000\n## Minimum:               97.11108\n## 1st quartile:          99.41095\n## Mean:                 100.03358\n## P(Mean = 0):            0.00000\n## Median:                99.96156\n## 3rd quartile:         100.67273\n## Maximum:              102.64917\n## Interquartile range:    1.26179\n## Range:                  5.53809\n## Satandard deviation:    0.96370\n## Variance:               0.92871\n## Skewness:               0.01617\n## Kurtosis:               0.05266\n\nLa media efectivamente es muy próxima a 100. Si se desea hacer un test formal se puede restar 100 a los datos y hacer el test de que la media es cero.\n\nsumStats(a - 100)\n##                         Serie 1\n## Data points:          300.00000\n## Missing:                0.00000\n## Minimum:               -2.88892\n## 1st quartile:          -0.58905\n## Mean:                   0.03358\n## P(Mean = 0):            0.54656\n## Median:                -0.03844\n## 3rd quartile:           0.67273\n## Maximum:                2.64917\n## Interquartile range:    1.26179\n## Range:                  5.53809\n## Satandard deviation:    0.96370\n## Variance:               0.92871\n## Skewness:               0.01617\n## Kurtosis:               0.05266\n\nComo se ve el p-valor del test de que la media es cero es 54%, lo que indica que con un amplio margen no se rechaza la hipótesis de que la media es cero (la variable original tiene media 100).\n\n\n\n# Borramos memoria\nrm(list = ls())\n# Cargamos librerías\nlibrary(UComp)\nlibrary(ggplot2)\n# Generamos números aleatorios gaussianos\na &lt;- rnorm(300) + 100\n# Representación gráfica\nautoplot(as.ts(a))\n# Estadísticos\nsumStats(a)\n\n\n\n\n# Cargamos librerías\nfrom UComp import *\n# Generamos números aleatorios gaussianos\na = np.random.normal(0, 1, 300) + 100\n# Representación gráfica\nplt.plot(a)\n# Estadísticos\nsumStats(a)\nsumStats(a - 100)\n\n\n\n\n% Borramos memoria\nclear all\n% Generamos números aleatorios gaussianos\na = randn(300, 1) + 100;\n% Representación gráfica\nplot(a)\n% Estadísticos\nsumStats(a)\nsumStats(a - 100)\n\n\n\n\nEn la vida real vamos a tener por lo general una sola realización de cada proceso estocástico, que de hecho son los datos observados. Es decir, si tenemos los datos mensuales de ventas de una empresa, cada dato es una realización de una función de distribución que desconocemos y que puede estar cambiando en el tiempo. Esto limita mucho el análisis, puesto que, si tuviéramos muchas realizaciones del mismo proceso estocástico podríamos estimar las leyes de probabilidad con mucha precisión. Normalmente, al disponer solo de una realización, la falta de información la tendremos que suplir con un cúmulo de supuestos sobre dichas distribuciones de probabilidad.\nCon estas definiciones, la predicción implica continuar la sucesión de variables aleatorias basándonos en las propiedades observadas en las anteriores. De esta forma podemos predecir toda la función de probabilidad de la variable, o solamente algunos momentos de la distribución (media, la mediana, la moda, la varianza, etc.). Así podemos obtener predicciones de toda la densidad, solo predicciones puntuales (normalmente la media o la mediana) o predicciones puntuales con algún intervalo de confianza o predicción.\n\n\n\n\n\n\nPredicción con inteligencia artificial\n\n\n\nAunque es evidente que hay similitudes, la predicción en el contexto de sección cruzada (análisis de imagen, sonido, etc.), tan habitual en nuestros días por la abundancia de aplicaciones de inteligencia artificial, es radicalmente diferente a la predicción de lo que puede suceder en el futuro en un sentido temporal. Efectivamente, alguien puede entrenar un modelo con millones de caras diferentes para reconocimiento facial y luego puede predecir, es decir, reconocer, que un objeto nuevo presentado al modelo es una cara o reconocer a una persona concreta. Para que esto pueda ser operativo se supone que las caras que puedan aparecer nuevas serán como las anteriores con las que se ha entrenado el modelo. Sin embargo, cuando se pretende predecir algo en el dominio del tiempo puede suceder que el futuro de un determinado fenómeno pueda resultar totalmente diferente a lo que ha sucedido hasta ahora, de forma que el modelo no servirá para nada.\nDesde luego, hay determinadas variables para las que el supuesto de que el futuro es semejante al pasado suena plausible, por ejemplo, en el caso de la radiación solar, que es un fenómeno físico. Pero en series de naturaleza económica, por ejemplo, el tema es bastante más incierto, piénsese por ejemplo, cómo influyen las turbulencias financieras, guerras o pandemias en las variables económicas. Desgraciadamente, hasta hace poco estos fenómenos los considerábamos poco probables ne Occidente.\n\n\nSea como fuere, si queremos predecir el futuro no tendremos más remedio que suponer que el futuro será como el pasado. Si hay dudas más que razonables de que esto sea cierto, entonces, lo más prudente será utilizar planteamientos alternativos a los que se presentan en este libro.\n\n2.1.1 Momentos de procesos estocásticos\nSe pueden calcular momentos de procesos estocásticos de forma sencilla, basándonos en los momentos de las funciones de distribución. Suponiendo que \\(Y(t)\\) es un proceso estocástico, los que más nos van a interesar son:\n\n\nMedia: \\(E[Y(t)]=\\mu_y(t)\\). Es la esperanza matemática.\n\nVarianza: \\(Var[Y(t)]=\\sigma^2_y(t)=E[Y(t)^2]-E[Y(t)]^2=E[Y(t)^2]-\\mu_y(t)^2\\). Es la esperanza matemática de las desviaciones respecto a la media.\n\nAutocovarianzas: \\(\\gamma_{i-j}=E[Y(i)-\\mu_y(i)]E[Y(j)-\\mu_y(j)]\\). Mide la covarianza en dos momentos del tiempo. Dada esta definición, las autocovarianzas son simétricas, \\(\\gamma_{i-j}=\\gamma_{j-i}\\). Cuando \\(i=j\\), \\(\\gamma_0=Var[Y(t)]\\), la autocovarianza de orden cero es la varianza. Las autocorrelaciones son las autocovarianzas normalizadas por la varianza, \\(\\rho_{i-j}=\\gamma_{i-j}/\\gamma_0\\), de forma que tomará valores entre -1 y 1, indicando correlación perfecta en los extremos o no autocorrelación en caso de que valga cero.\n\n2.1.2 Algunos procesos estocásticos\nYa estamos en condiciones de analizar algunos procesos estocásticos sencillos que usaremos en el futuro.\n\n\nRuido blanco: se trata de un proceso puramente aleatorio. Es una simple sucesión de números aleatorios extraídos de alguna función de distribución. Se asume que tendrá media y varianza constante. Lo único significativo de este proceso es la media, por lo que la predicción óptima es su media, puesto que todo lo que sucede alrededor de la media es impredecible por definición.\n\nPaseo aleatorio (Radom Walk o RW en inglés): se trata de un proceso discreto en la variable tiempo (\\(t\\)) que obedece a la ecuación \\(y_t=y_{t-1}+a_t\\), siendo \\(a_t\\) un ruido blanco. Es decir, el valor presente es el mismo que el anterior con una perturbación aleatoria que puede ser positiva o negativa. La predicción óptima de este modelo será el valor anterior debido a que la mejor predicción de \\(a_t\\) es su media. Cuando se predicen varios periodos hacia adelante la mejor predicción es el último valor observado.\n\nPaseo aleatorio con deriva: se trata de un proceso que obedece a la ecuación \\(y_t=c+y_{t-1}+a_t\\), siendo \\(c\\) una constante. Es igual que el anterior pero con una constante que le imprime una pendiente lineal ascendente o descendente si el signo de la contante es positivo o negativo, respectivamente. En efecto, si llamamos \\(T\\) al momento de la última observación disponible, la predicción óptima de la variable \\(y_t\\) para el momento \\(T+i, i=1,2,\\dots,l\\) con la información hasta la observación \\(T\\) se denota por \\(\\hat{y}_{T+i|T}\\) y se puede obtener aplicando recursivamente la fórmula del modelo:\n\n\\[\n\\begin{array}{l}\n\\hat{y}_{T+1|T} = c + y_{T} \\\\\n\\hat{y}_{T+2|T} = c + \\hat{y}_{T+1|T} = 2c + y_{T} \\\\\n\\vdots \\\\\n\\hat{y}_{T+l|T} = c + \\hat{y}_{T+l-1|T} = 2c + \\hat{y}_{T+l-2|T} = \\dots = lc + y_{T} \\\\\n\\end{array}\n\\]\n\n\n\n\n\n\nEl paseo aleatorio\n\n\n\nKarl Pearson en una carta a la revista Nature en 1905 asemejó el modelo del paseo aleatorio a la trayectoria de un borracho. Esto se debe a que la posición del pobre hombre es la anterior con una pequeña modificación en su entorno que es puramente aleatoria debido a su estado de embriaguez.\n\n\nLa Figura 2.2 muestra la predicción de estos procesos estocásticos partiendo de distintos orígenes de predicción. Las predicciones se muestran en color rojo y los datos en negro. El panel superior izquierdo muestra el caso de un ruido blanco en el que se ve cómo al añadir datos va cambiando la estimación de la media y por tanto la predicción. El panel superior derecho es un paseo aleatorio y la predicción es el último dato observado. Los paneles inferiores muestran los casos de paseos aleatorios con deriva positiva (izquierda) y negativa (derecha).\n\n\nFigura 2.2: Predicción óptima de procesos estocásticos sencillos.\n\n\n\nResuelve\nSolución\nR\nPython\nMATLAB/Octave\n\n\n\nSimula un ruido blanco Normal con media cero y varianza uno de longitud 100. Represéntala gráficamente y muestra la media y la varianza por pantalla.\nSimula un paseo aleatorio basándote en el ruido blanco con una condición inicial de 100 anterior con un bucle for. Dibújala.\nSimula un paseo aleatorio con deriva de 1 basándote en el ruido blanco anterior. Dibújala.\n\n\n\na = rnorm(100)\nautoplot(as.ts(a))\n\n\n\ncat(paste(\"Media: \", mean(a), \"   /   Varianza: \", var(a)))\n## Media:  0.0516018586359169    /   Varianza:  0.983419578990251\ny = rep(100, 100)\nfor (t in 2 : 100){\n     y[t] = y[t - 1] + a[t]\n}\nautoplot(as.ts(y))\n\n\n\ny = rep(100, 100)\nfor (t in 2 : 100){\n     y[t] = 1 + y[t - 1] + a[t]\n}\nautoplot(as.ts(y))\n\n\n\n\n\n\n\n# Simulación\na = rnorm(100)\n# Representación gráfica\nautoplot(as.ts(a))\n\n\n\n# Mostrando valores \ncat(paste(\"Media: \", mean(a), \"   /   Varianza: \", var(a)))\n## Media:  -0.0391342405607215    /   Varianza:  1.36799275552134\n# Simulando paseo aleatorio con condición inicial 100\ny = rep(100, 100)\nfor (t in 2 : 100){\n     y[t] = y[t - 1] + a[t]\n}\nautoplot(as.ts(y))\n\n\n\n# Simulando paseo aleatorio con deriva\ny = rep(100, 100)\nfor (t in 2 : 100){\n     y[t] = 1 + y[t - 1] + a[t]\n}\nautoplot(as.ts(y))\n\n\n\n\n\n\n\n# Simulación\na = np.random.normal(0, 1, 100)\n# Representación gráfica\nplt.plot(a)\n# Mostrando valores\nprint(\"Media: \" + f\"{np.mean(a):.4f}\"  + \"   /   Varianza: \" + f\"{np.var(a):.4f}\")\n# Simulando paseo aleatorio con condición inicial 100\ny = np.full((100), 100)\nfor t in range(1, 100):\n     y[t] = y[t - 1] + a[t]\nplt.plot(y)\n# Simulando paseo aleatorio con deriva\ny = np.full((100), 100)\nfor t in range(1, 100):\n     y[t] = 1 + y[t - 1] + a[t]\nplt.plot(y)\n\n\n\n\n% Simulación\na = randn(100, 1);\n% Representación gráfica\nplot(a)\n% Mostrando valores \n['Media: ' num2str(mean(a)) '   /   Varianza: ' num2str(var(a))]\n% Simulando paseo aleatorio con condición inicial 100\ny = repmat(100, 100, 1);\nfor t = 2 : 100\n     y(t) = y(t - 1) + a(t)\nend\nplot(y)\n% Simulando paseo aleatorio con deriva\ny = repmat(100, 100, 1);\nfor t = 2 : 100\n     y(t) = 1 + y(t - 1) + a(t)\nend\nplot(y)"
  },
  {
    "objectID": "07-basicsts.html#estacionariedad",
    "href": "07-basicsts.html#estacionariedad",
    "title": "\n2  Conceptos y herramientas básicas\n",
    "section": "\n2.2 Estacionariedad",
    "text": "2.2 Estacionariedad\nDadas las definiciones previas queda clara la definición de otro concepto central en el estudio de series temporales, que es el de estacionariedad. Se dice que un proceso estocástico es estacionario en sentido estricto cuando la función de distribución es independiente del tiempo (eso es lo que sucede en el caso del panel superior de la Figura 2.1). Este concepto es tan restrictivo que a menudo se utiliza otro que es la estacionariedad débil que implica que la media y la varianza del proceso son constantes y las autocovarianzas dependen solo de las diferencias de tiempo en que se miden pero no del momento particular en que se miden. Por ejemplo, la autocovarianza de orden 2 (\\(\\gamma_2\\)) toma siempre el mismo valor se mida al principio del proceso, en medio o al final. Si la distribución es Gaussiana, entonces estacionariedad débil equivale a la estricta, puesto que la distribución normal queda completamente determinada por los dos primeros momentos de la distribución.\nEl concepto de estacionariedad vino para permitir estimaciones cuando solo se tiene una realización del proceso estocástico, que es lo que suele suceder. Si asumimos estacionariedad, entonces podemos estimar los momentos de la distribución (que es constante en el tiempo) con los momentos muestrales. Si suponemos que \\(y_t\\) es una realización del proceso estocástico estacionario \\(Y(t)\\), tenemos:\n\nMedia: \\(E[Y(t)]=\\mu_y=1/n \\sum_1^n y_t\\).\nVarianza: \\(Var[Y(t)]=\\sigma^2_y= 1/n \\sum_1^n (y_t - \\mu_y)^2 =1/n \\sum_1^n y_t^2 - \\mu_y^2\\).\nAutocovarianzas: \\(\\gamma_{i}=1 / n \\sum_1^n (y_t-\\mu_y)(y_{t-i}-\\mu_y)\\). Las autocorrelaciones son \\(\\rho_i=\\gamma_i/\\gamma_0\\).\n\nEn cualquier caso, estos estadísticos se pueden calcular siempre para cualquier serie temporal, incluso para las no estacionarias. Pero no tendrán ninguna interpretación, ningún sentido cuando el proceso estocástico del que proceden sea no estacionario, porque solo se calcula un solo valor para cada uno de ellos cuando en realidad están cambiando en el tiempo.\n\n\nResuelve\nSolución\n\n\n\n¿Será estacionaria en media una serie temporal con tendencia?\n¿Son estacionarias en media y varianza las series de la Figura 1.2?\n\n\n¿Será estacionaria en media una serie temporal con tendencia?\nNo. Precisamente la tendencia lo que implica es que la media cambia en el tiempo.\n¿Son estacionarias en media y varianza las series de la Figura 1.2?\nEl IBEX-35 y los pasajeros de avión no son estacionarios ni en media ni varianza. El pib no es estacionario en media, pero podría serlo en varianza. El caudal del Nilo puede ser estacionario en varianza, y salvo un salto cerca del año 1900, podría ser estacionario en media también.\n\n\n\nA la representación de las autocorrelaciones frente al retardo se llama función de autocorrelación simple y es de suma utilidad en el análisis de series temporales (ACF es el acrónimo en inglés). Por extensión se define la función de autocorrelación parcial a la representación gráfica de los coeficientes de correlación parcial frente al retardo (PACF en inglés). Las autocorrelaciones parciales miden la autocorrelación entre dos momentos del tiempo cuando se han tenido en cuenta simultáneamente todos los retardos intermedios o cuando se han descontado todas las relaciones intermedias. serían los coeficientes (\\(c_{i,i}, i=1,2,\\dots, k\\)) de las siguientes regresiones:\n\\[\n\\begin{array}{l}\ny_t = c_{1,0} + c_{1,1} y_{t-1} + a_{1,t} \\\\\ny_t = c_{2,0} + c_{2,1} y_{t-1} + c_{2,2} y_{t-2} + a_{2,t} \\\\\n\\vdots \\\\\ny_t = c_{k,0} + c_{k,1} y_{t-1} + c_{k,2} y_{t-2} + c_{k,k} y_{t-k} + \\dots  + a_{k,t} \\\\\n\\end{array}\n\\]\nLa Figura 2.3 muestra un ejemplo de funciones de autocorrelación simple y parcial.\n\n\nFigura 2.3: Funciones de autocorrelación simple y parcial.\n\n\n\nResuelve\nSolución\nR\nPython\nMATLAB/Octave\n\n\n\nDemuestra que un paseo aleatorio con deriva no es estacionario en media. Para ello asume que la serie temporal sigue la ecuación \\(y_t=c+y_{t-1}+a_t\\) y calcula la media de forma recursiva empezando por \\(E(y_1)\\) y asumiendo que \\(E(y_0)=0\\).\nRealiza una simulación de longitud 100 con deriva \\(c=1\\) y estima su función de autocorrelación simple y parcial.\n\n\nSi calculamos las medias de forma recursiva tenemos:\n\\[\n\\begin{array}{c}\nE(y_1)=E(c+y_0+a_t)=c+E(y_0)=c \\\\\nE(y_2)=c+E(y_1)=2c \\\\\n\\vdots \\\\\nE(y_t)=tc\n\\end{array}\n\\]\nLa media depende del tiempo, por lo que el proceso no es estacionario.\nUna simulación de longitud 100 con deriva \\(c=1\\) tiene el siguiente aspecto:\n\n\n\n\n\nComo se ve, la serie tienen una tendencia lineal clara que depende de la deriva. La ACF tiende a cero muy lentamente y la PACF muestra un único valor cercano a 1, algo que es síntoma de que la serie no es estacionaria. No obstante, la ACF y PACF no significan nada en este caso, puesto que para su cálculo correcto la serie temporal debe ser estacionaria, hipótesis que sabemos que no es cierta.\n\n\n\n# Simulando paseo aleatorio con deriva\ny = rep(0, 100)\na = rnorm(100)\nfor (t in 2 : 100){\n    y[t] = 1 + y[t - 1] + a[t]\n}\ntsDisplay(y)\n\n\n\n\n# Simulando paseo aleatorio con deriva\ny = np.zeros(100)\nfor t in range(1, 100):\n     y[t] = 1 + y[t - 1] + a[t]\ntsDisplay(y)\n\n\n\n\n% Simulando paseo aleatorio con deriva\ny = zeros(100, 1);\na = randn(100, 1);\nfor t = 2 : 100\n    y(t) = 1 + y(t - 1) + a(t);\nend\ntoolTEST(y)"
  },
  {
    "objectID": "07-basicsts.html#transformaciones-de-estacionariedad",
    "href": "07-basicsts.html#transformaciones-de-estacionariedad",
    "title": "\n2  Conceptos y herramientas básicas\n",
    "section": "\n2.3 Transformaciones de estacionariedad",
    "text": "2.3 Transformaciones de estacionariedad\nHemos visto que la aplicación rigurosa de la estadística al análisis de procesos estocásticos requiere del supuesto de estacionariedad. Sin embargo, ya podemos avanzar que muchos de los casos reales de series temporales no exhiben esta propiedad (ver, por ejemplo, la Figura 1.2). Por ello, es habitual aplicar algunas transformaciones previas con el fin de inducir estacionariedad, antes de proceder al análisis estadístico de las mismas.\nHabitualmente el análisis de la estacionariedad se restringe a su acepción débil, que unido al supuesto de normalidad en realidad es equivalente a utilizar la estacionariedad en sentido estricto.\n\n2.3.1 Transformación de varianza\nEl problema de varianza no constante (o heterocedasticidad) a veces se soluciona con la transformación Box-Cox, que es efectiva siempre que exista alguna relación entre la media y la varianza de la serie. La transformación es\n\\[\nz_t= \\Biggl\\{ \\begin{array}{rl}\n          \\ln(y_t) & \\text{si } \\lambda = 0 \\\\\n          \\frac{(y_t^{\\lambda} - 1)}{\\lambda} & \\text{si } \\lambda \\neq 0\n        \\end{array}\n\\tag{2.1}\\]\nComo se ve, la transformación depende de una parámetro \\(\\lambda\\). Como casos particulares tenemos \\(\\lambda = 1\\) (no hay transformación); \\(\\lambda = 0,5\\) ( raíz cuadrada); \\(\\lambda = 0\\) (logaritmo natural); \\(\\lambda = -0,5\\) (inversa de raíz cuadrada); \\(\\lambda = -1\\) (inversa).\nA menudo los resultados no son muy sensibles a grandes variaciones del coeficiente \\(\\lambda\\), por lo que muchos analistas sencillamente toman sistemáticamente el logaritmo natural u otras trasformaciones reconocibles, como la raíz cuadrada o la inversa. Además, se trata de una transformación que no se puede utilizar si la serie tiene valores negativos, en cuyo caso se puede sumar una constante.\nUna cuestión interesante es que la transformación cambia las unidades de la variable y al deshacer la transformación para recuperar la escala original las predicciones siempre serán positivas. De forma que es una modo eficiente de restringir las predicciones para que sean positivas, al margen del problema de heterocedasticidad.\nLa Figura 2.4 muestra los efectos de la transformación. Como se puede ver a simple vista, en este caso un valor negativo no muy alejado de cero puede estabilizar la varianza.\n\n\nFigura 2.4: La transformación Box-Cox en acción.\n\nExisten procedimientos para estimar el coeficiente \\(\\lambda\\) de forma óptima, que se tratará con más detenimiento más adelante. Lo ideal es estimar este parámetro conjuntamente con los demás parámetros de cualquier modelo, pero la dificultad que esto entraña lleva a estimarlo previamente y comprobar a posteriori que la transformación tiene sentido. Un procedimiento es el de Guerrero, que consiste en minimizar el coeficiente de variación (desviación típica dividido por la media) de segmentos de la serie original, típicamente años enteros. Este procedimiento aplicado a la serie de la Figura 2.4 arroja un valor de \\(\\lambda=-0,078\\), que indica que el logaritmo es una buena aproximación.\nEs bastante habitual que esta transformación se utilice automáticamente, a pesar de que existen casos en los que no puede funcionar, puesto que, como se dijo al principio, se asume que existe una relación directa o inversa entre la media y la varianza. En la Figura 2.5 se muestra un caso simulado en el que hay oscilaciones cíclicas y el valor óptimo de \\(\\lambda\\) tiende a ser negativo y grande. En casos que no funciona la forma de tratar el problema es incorporar términos en el modelo que tratan la heterocedasticidad explícitamente.\n\n\nFigura 2.5: Un caso en el que la transformación Box-Cox no funciona.\n\n\n\n\n\n\n\nImportante\n\n\n\nUna cuestión a tener en cuenta es que normalmente le hipótesis de homocedasticidad (varianza constante) se hace sobre los residuos, por lo que la palabra final sobre la conveniencia de la transformación tiene que recaer en contraste de hipótesis de cambios de varianza sobre los mismos, como veremos más tarde.\n\n\n\n\nResuelve\nSolución\nR\nPython\nMATLAB/Octave\n\n\n\nBorra toda la memoria y carga las librerías que necesites. Genera una variable x de la serie de pasajeros de avión (pasajeros) desde el inicio hasta febrero de 2020 y represéntala gráficamente. ¿Es estacionaria la varianza?\nCrea una función para calcular la transformación Box-Cox (que se llame box.cox) que tenga como entradas una serie temporal y el valor de \\(\\lambda\\) y como salida la variable transformada. Separa el caso particular del logaritmo cuando el valor de lambda sea en valor absoluto menor que \\(1e-4\\). Pruébala con distintos valores de lambda: -1, -0,5, 0, 0,5, 1, 2. Represéntalas todas gráficamente.\nEn R te pueden ser de gran ayuda función grid.arrange de la librería gridExtra. En Python te ayudará la función subplot de matplotlib.pyplot. En MATLAB/Octave puedes utilizar también subplot.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(UComp)\nlibrary(ggplot2)\nlibrary(gridExtra)\nx = window(airpas, end=c(2020, 2))\nautoplot(x)\n\n\n# Función para calcular la transformación Box-Cox\nbox.cox = function(x, lambda){\n    if (abs(lambda) &lt; 1e-4){\n        return(log(x))\n    } else if (lambda &gt; 0.99 && lambda &lt; 1.01) {\n        return(x)\n    } else {\n        return((x ^ lambda - 1) / lambda)\n    }\n}\n# Casos particulares\np1 = autoplot(box.cox(x, -1))\np2 = autoplot(box.cox(x, -0.5))\np3 = autoplot(box.cox(x, 0))\np4 = autoplot(box.cox(x, 0.5))\np5 = autoplot(box.cox(x, 1))\np6 = autoplot(box.cox(x, 2))\ngrid.arrange(p1, p2, p3, p4, p5, p6, nrow = 2)\n\n\n\n\nfrom UComp import *\n    \nx = window(airpas, end=\"2020-02-29\")\nplt.plot(x)\n# Función para calcular la transformación Box-Cox\ndef box_cox(x, lam):\n    if abs(lam) &lt; 1e-4:\n        return np.log(x)\n    elif 0.99 &lt; lam &lt; 1.01:\n        return x\n    else:\n        return (x ** lam - 1) / lam\n# Casos particulares\np1 = box_cox(x, -1)\np2 = box_cox(x, -0.5)\np3 = box_cox(x, 0)\np4 = box_cox(x, 0.5)\np5 = box_cox(x, 1)\np6 = box_cox(x, 2)\nplt.subplot(2, 3, 1)\nplt.plot(p1)\nplt.title('lambda = -1')\nplt.subplot(2, 3, 2)\nplt.plot(p2)\nplt.title('lambda = -0.5')\nplt.subplot(2, 3, 3)\nplt.plot(p3)\nplt.title('lambda = 0')\nplt.subplot(2, 3, 4)\nplt.plot(p4)\nplt.title('lambda = 0.5')\nplt.subplot(2, 3, 5)\nplt.plot(p5)\nplt.title('lambda = 1')\nplt.subplot(2, 3, 6)\nplt.plot(p6)\nplt.title('lambda = 2')\nplt.tight_layout()\nplt.show()\n\n\n\n\nx = airpas(1 : 614);\nplot(x)\n% Función para calcular la transformación Box-Cox\nfunction y = box_cox(x, lambda)\n    if abs(lambda) &lt; 1e-4\n        y = log(x);\n    elseif lambda &gt; 0.99 && lambda &lt; 1.01\n        y = x;\n    else\n        y = (x .^ lambda - 1) / lambda;\n    end\nend\n% Casos particulares\np1 = box_cox(x, -1);\np2 = box_cox(x, -0.5);\np3 = box_cox(x, 0);\np4 = box_cox(x, 0.5);\np5 = box_cox(x, 1);\np6 = box_cox(x, 2);\nsubplot(2, 3, 1);\nplot(p1);\ntitle('lambda = -1');\nsubplot(2, 3, 2);\nplot(p2);\ntitle('lambda = -0.5');\nsubplot(2, 3, 3);\nplot(p3);\ntitle('lambda = 0');\nsubplot(2, 3, 4);\nplot(p4);\ntitle('lambda = 0.5');\nsubplot(2, 3, 5);\nplot(p5);\ntitle('lambda = 1');\nsubplot(2, 3, 6);\nplot(p6);\ntitle('lambda = 2');\nsgtitle('Transformación de Box-Cox');\n\n\n\n\n\n2.3.2 Transformación de media\nEn el caso de la media, la diferenciación produce estacionariedad. Aplicar una diferencia a una serie temporal significa generar una serie en la que a cada observación se resta la anterior. La diferencia de la serie temporal \\(y_t\\) es \\(d_t=y_t-y_{t-1}\\). La Figura 2.6 muestra cómo una serie no estacionaria en media, como es el IBEX-35, resulta en una serie estacionaria al aplicar una diferencia.\n\n\nFigura 2.6: IBEX-35 diario y su primera diferencia.\n\n\n\nResuelve\nSolución\n\n\n\nDemuestra que la diferencia de un paseo aleatorio es ruido blanco.\nDemuestra que la diferencia de un paseo aleatorio con deriva es un ruido blanco con media igual a la constante del proceso.\n\n\nDemuestra que la diferencia de un paseo aleatorio es ruido blanco.\n\\[\n\\begin{array}{c}\ny_t=y_{t-1}+a_t \\\\\ny_t - y_{t-1} = a_t\n\\end{array}\n\\] El lado izquierdo es la diferencia de la serie original.\nDemuestra que la diferencia de un paseo aleatorio con deriva es un ruido blanco con media igual a la constante del proceso.\n\\[\n\\begin{array}{c}\ny_t=c+y_{t-1}+a_t \\\\\ny_t - y_{t-1} = c+a_t\n\\end{array}\n\\]\nComo \\(a_t\\) tiene media cero, \\(c+a_t\\) tendrá media \\(c\\).\n\n\n\nSe pueden aplicar varias diferencias sucesivamente, es decir, aplicar dos diferencias es aplicar una diferencia a la primera diferencia: \\(z_t=d_t-d_{t-1}=y_t-2y_{t-1}+y_{t-2}\\). Se puede aplicar también una diferencia estacional cuando la estacionalidad no sea estacionaria, que consiste en restar a cada dato el dato del mismo periodo del año anterior, para datos mensuales \\(d_t=y_t-y_{t-12}\\). Se pueden mezclar los dos tipos de diferencias, por ejemplo, aplicar una diferencia regular y una estacional implica \\(ddy_t=dy_t-dy_{t-12}=y_t-y_{t-1}-y_{t-12}+y_{t-13}\\). La Figura 2.7 muestra el efecto de cada uno de estos tipos de diferencias.\n\n\n\n\n\n\nEstacionariedad y estacionalidad\n\n\n\nEstos son dos términos que a menudo provocan confusión. Estacionariedad implica que las leyes de probabilidad no cambian en el tiempo, mientras que estacionalidad es la repetición, normalmente no determinista, de un ciclo con una determinada frecuencia a lo largo del tiempo.\nPodemos tener series temporales con estacionalidad estacionaria, pero también las hay con estacionalidad no estacionaria.\n\n\n\n\nFigura 2.7: Varias combinaciones de diferencias.\n\nLa diferencia regular marca de forma clara el problema de la heterocedasticidad. La diferencia estacional diluye la estacionalidad a simple vista, pero deja la serie con oscilaciones importantes de la media. Las dos diferencias producen un planchado efectivo de la serie en cuanto a la estacionariedad en media. Una indicación clara de la no estacionariedad de la estacionalidad es que esta se ve a simple vista en la serie original. Sin embargo, la diferencia estacional no elimina la estacionalidad, solo la convierte en estacionaria.\nLa inducción de estacionariedad en media mediante diferenciación es típica de muchos modelos de predicción, como es la regresión con series temporales y sobre todo los modelos ARIMA. Sin embargo, en este libro, los modelos utilizados son de otra naturaleza tales que en lugar de inducir estacionariedad mediante la diferenciación la van a modelizar en forma de tendencias o de especificaciones con raíces unitarias de los componentes estacionales.\n\n\nResuelve\nSolución\nR\nPython\nMATLAB/Octave\n\n\n\nConsidera la serie temporal pasajeros hasta febrero de 2020. Comprueba si las series con varias diferencias son estacionarias en media. Para ello calcula y representa gráficamente los siguientes casos (\\(d\\) es la diferencia regular y \\(D\\) la estacional: a) \\(d = 0\\), \\(D = 0\\); b) \\(d = 1\\), \\(D = 0\\); c) \\(Sd = 0\\), \\(D = 1\\); d) \\(d = 1\\), \\(D = 1\\); e) \\(d = 2\\), \\(D = 0\\); f) \\(d = 0\\), \\(D = 2\\).\n¿Qué transformación de media y varianza utilizarías para esta serie? Represéntala con tsDisplay. ¿Es ruido blanco?\nPuedes repetir el análisis para el Índice de Producción Industrial (ipi) y el Producto Interior Bruto español (gdp).\n\n\nUtilizando la variable x del ejemplo anterior, las diferencias son\n\n\n\n\n\nLa transformación de varianza más conveniente parece el logaritmo con \\(d=1\\) y \\(D=1\\). La serie no es ruido blanco, puesto que hay correlaciones muy marcadas distintas de cero. El correlograma indica además la presencia de estacionalidad por los coeficientes significativos en los retardos estacionales (múltiplos de 12), aunque la estacionalidad no se ve a simple vista en el gráfico de la serie. Como ya hemos dicho, las diferencias no eliminan la estacionalidad, simplemente la hacen estacionaria.\n\n\n\n\n\nSe puede utilizar el mismo código para analizar la serie gdp y sacar conclusiones.\n\n\n\n# Generando diferencias\np1 = autoplot(dif(x, c(0, 0), c(1, 12)))\np2 = autoplot(dif(x, c(1, 0), c(1, 12)))\np3 = autoplot(dif(x, c(0, 1), c(1, 12)))\np4 = autoplot(dif(x, c(1, 1), c(1, 12)))\np5 = autoplot(dif(x, c(2, 0), c(1, 12)))\np6 = autoplot(dif(x, c(0, 2), c(1, 12)))\ngrid.arrange(p1, p2, p3, p4, p5, p6, nrow = 2)\n# Gráfico de serie diferenciada y funciones *ACF* y *PACF*\ntsDisplay(dif(box.cox(x, 0), c(1, 1), c(1, 12)))\n\n\n\n\n# Generando diferencias\np1 = dif(x, [0, 0], [1, 12])\np2 = dif(x, [1, 0], [1, 12])\np3 = dif(x, [0, 1], [1, 12])\np4 = dif(x, [1, 1], [1, 12])\np5 = dif(x, [2, 0], [1, 12])\np6 = dif(x, [0, 2], [1, 12])\n# Representando gráficamente\nplt.subplot(2, 3, 1)\nplt.plot(p1)\nplt.title('diffs = [0, 0]')\nplt.subplot(2, 3, 2)\nplt.plot(p2)\nplt.title('diffs = [1, 0]')\nplt.subplot(2, 3, 3)\nplt.plot(p3)\nplt.title('diffs = [0, 1]')\nplt.subplot(2, 3, 4)\nplt.plot(p4)\nplt.title('diffs = [1, 1]')\nplt.subplot(2, 3, 5)\nplt.plot(p5)\nplt.title('diffs = [2, 0]')\nplt.subplot(2, 3, 6)\nplt.plot(p6)\nplt.title('diffs = [0, 2]')\nplt.tight_layout()\nplt.show()\n# Gráfico de serie diferenciada y funciones *ACF* y *PACF*\ntsDisplay(dif(box_cox(x, 0), [1, 1], [1, 12]))\n\n\n\n\n% Generando diferencias\np1 = vdif(x, [0 0], [1 12]);\np2 = vdif(x, [1 0], [1 12]);\np3 = vdif(x, [0 1], [1 12]);\np4 = vdif(x, [1 1], [1 12]);\np5 = vdif(x, [2 0], [1 12]);\np6 = vdif(x, [0 2], [1 12]);\n% Representando gráficamente\nsubplot(2, 3, 1)\nplot(p1)\ntitle('diffs = [0, 0]')\nsubplot(2, 3, 2)\nplot(p2)\ntitle('diffs = [1, 0]')\nsubplot(2, 3, 3)\nplot(p3)\ntitle('diffs = [0, 1]')\nsubplot(2, 3, 4)\nplot(p4)\ntitle('diffs = [1, 1]')\nsubplot(2, 3, 5)\nplot(p5)\ntitle('diffs = [2, 0]')\nsubplot(2, 3, 6)\nplot(p6)\ntitle('diffs = [0, 2]')\n% Gráfico de serie diferenciada y funciones *ACF* y *PACF*\ntoolTEST(vdif(box_cox(x, 0), [1, 1], [1, 12]))"
  },
  {
    "objectID": "07-basicsts.html#contraste-de-hipótesis-para-modelos-de-series-temporales",
    "href": "07-basicsts.html#contraste-de-hipótesis-para-modelos-de-series-temporales",
    "title": "\n2  Conceptos y herramientas básicas\n",
    "section": "\n2.4 Contraste de hipótesis para modelos de series temporales",
    "text": "2.4 Contraste de hipótesis para modelos de series temporales\nUna característica común de todos los modelos estadísticos es el diagnóstico que se realiza a posteriori sobre el mismo, con el fin de contrastar empíricamente que las hipótesis sobre las que se basan las buenas propiedades de las estimaciones se cumplen para la muestra en estudio.\nLa mayor parte de estas hipótesis, no todas, descansan en que el residuo, la parte que el modelo no es capaz de explicar, debe ser lo más parecido a ruido blanco, es decir, un proceso puramente aleatorio independiente en el tiempo, con media y varianza constante y procedente de una distribución normal. La Figura 2.8 muestra algunos resultados gráficos de tales contrastes.\n\n\nFigura 2.8: Tests de diagnóstico sobre un ruido blanco simulado.\n\nLa primera fila se corresponde con la función de autocorrelación simple y parcial estimadas en las que cada coeficiente se puede comparar con su intervalo al \\(95\\%\\) de confianza para determinar si es o no significativo (valores que se salen de las bandas serán significativamente distintos de cero).\nEn la segunda fila aparecen tests de normalidad:\n\nA la izquierda se muestra un histograma junto con la función de densidad teórica de una distribución normal con la misma media y varianza que la muestra y una estimación no-paramétrica del histograma. La distribución será normal cuanto más se parezcan el histograma y su estimación no-paramétrica a la teórica.\nA la derecha se muestra el gráfico Q-Q que compara los cuantiles de la muestra con los cuantiles de la distribución normal. En una muestra normal los cuantiles deben ser muy parecidos entre sí y los puntos deben ajustarse a la diagonal.\n\nEn la tercera fila se muestran los tests CUSUM y CUSUMsq para contrastar que la emdia y varianza son constantes en el tiempo, respectivamente. Ambos tests se basan en estimaciones recursivas de la media y varianza de la muestra (estimaciones con número creciente de observaciones). El proceso será estacionario en media y varianza si los valores de los estadísticos (líneas continuas negras) se encuentran dentro de la bandas de confanza (discontinuas rojas).\nLos contrastes gráficos se suelen complementar con otros formales:\n\nEs conveniente calcular los estadísticos básicos de posición, dispersión, asimetría y curtosis, que dan una idea sobre la distribución de la muestra.\nUn test de media cero asumiendo que la muestra es independiente con varianza constante es muy sencillo de llevar a cabo.\nLo mismo sucede con un test de ratio de varianzas de dos segmentos de la muestra distantes entre sí para contrastar que la varianza es constante.\nExiste gran variedad de contrastes de normalidad formales que se pueden utilizar, como el de Shapiro-Wilk.\nEl contraste de Ljung-Box de autocorrelación es también muy utilizado. Se basa en los cuadrados de las autocorrelaciones de retardos acumulados. La Tabla 2.1 muestra una salida que se utilizará en este libro para la serie de la Figura 2.8. Las estimaciones de los coeficientes de autocorrelación simple y parcial aparecen bajo los nombres SACF y SPACF (‘s’ de sample o muestra). La significación individual de cada coeficiente se indica con puntos y signos positivos y negativos para los casos en que no son significativos o significativos por el lado positivo o negativo, respectivamente. El test Ljung-Box se encuentra en las columnas LB y p.val que proporcionan los valores del estadístico y el tamaño en términos de probabilidad de la región de no rechazo del test, de forma que si dicha probabilidad es superior al \\(5\\%\\) no se puede rechazar la hipótesis de no autocorrelación. Hay que tener en cuenta que este test contrasta la autocorrelaciones agregada hasta el retardo correspondiente. Por ejemplo, en la tabla aparece que para el retardo \\(6\\) la probabilidad de no rechazo es del \\(20\\%\\) indicando que no existe autocorrelación hasta el retardo \\(6\\). Se puede comprobar que esa misma probabilidad cuando se añade un retardo es \\(2,5\\%\\) indicando que puede haber correlación hasta el retardo \\(7\\), sin embargo, en el correlograma de la Figura 2.8 se ve que esto se debe exclusivamente a un valor elevado de la autocorrelación de retardo \\(7\\).\n\n\n\n\n\nTabla 2.1: Tests de autocorrelación.\n\n\nSACF\nsa\nLB\np.val\nSPACF\nsp\n\n\n\n1\n-0.015\n.\n0.046\n0.829\n-0.015\n.\n\n\n2\n-0.111\n.\n2.573\n0.276\n-0.112\n.\n\n\n3\n-0.059\n.\n3.276\n0.351\n-0.063\n.\n\n\n4\n0.069\n.\n4.262\n0.372\n0.055\n.\n\n\n5\n-0.141\n.\n8.371\n0.137\n-0.155\n-\n\n\n6\n-0.014\n.\n8.413\n0.209\n-0.009\n.\n\n\n7\n0.191\n+\n16.030\n0.025\n0.173\n+\n\n\n8\n0.065\n.\n16.921\n0.031\n0.046\n.\n\n\n9\n0.038\n.\n17.222\n0.045\n0.100\n.\n\n\n10\n0.067\n.\n18.182\n0.052\n0.097\n.\n\n\n\n\n\n\n\n\nResuelve\nSolución\nR\nPython\nMATLAB/Octave\n\n\n\nSimula un ruido blanco normal de dimensión 300 con media 100 (rnorm), represéntala gráficamente y comprueba que la media es cien. Comprueba la autocorrelación. Realiza los tests de gaussianidad. Realiza los tests de heterocedasticidad. Realiza los tests CUSUM y CUSUMsq.\nSimula un ruido blanco normal de dimensión 300 con media 100 y varianza 5 y estima todos los tests.\nEn todos los lenguajes puedes usar las funciones gaussTest, varTest, cusum y tests. Prueba además con sumStats, ident y/o tsDisplay. En MATLAB/Octave te puede ayudar toolTEST, vident y sumstats.\n\n\nLa primera solución es la siguiente:\n\n\n\n\n##                         Serie 1\n## Data points:          300.00000\n## Missing:                0.00000\n## Minimum:               97.27983\n## 1st quartile:          99.28604\n## Mean:                 100.04715\n## P(Mean = 0):            0.00000\n## Median:               100.00691\n## 3rd quartile:         100.77362\n## Maximum:              102.88842\n## Interquartile range:    1.48758\n## Range:                  5.60859\n## Satandard deviation:    1.06853\n## Variance:               1.14176\n## Skewness:               0.06186\n## Kurtosis:              -0.40482\n\n\n\n##      SACF sa     LB p.val  SPACF sp\n## 1  -0.024  .  0.173 0.677 -0.024  .\n## 2   0.027  .  0.396 0.820  0.027  .\n## 3  -0.056  .  1.342 0.719 -0.054  .\n## 4  -0.033  .  1.669 0.796 -0.036  .\n## 5  -0.069  .  3.151 0.677 -0.069  .\n## 6   0.066  .  4.507 0.608  0.062  .\n## 7  -0.093  .  7.193 0.409 -0.092  .\n## 8  -0.019  .  7.310 0.504 -0.036  .\n## 9   0.021  .  7.445 0.591  0.026  .\n## 10 -0.034  .  7.803 0.648 -0.043  .\n## 11 -0.052  .  8.644 0.655 -0.057  .\n## 12 -0.009  .  8.669 0.731 -0.026  .\n## 13 -0.004  .  8.674 0.797  0.003  .\n## 14  0.104  . 12.071 0.601  0.095  .\n## 15 -0.009  . 12.098 0.672 -0.026  .\n## 16  0.069  . 13.597 0.629  0.066  .\n## 17 -0.020  . 13.722 0.687 -0.006  .\n## 18  0.003  . 13.725 0.747 -0.005  .\n## 19 -0.024  . 13.904 0.789 -0.008  .\n## 20  0.001  . 13.904 0.835 -0.011  .\n## 21 -0.013  . 13.962 0.871  0.013  .\n## 22 -0.053  . 14.869 0.868 -0.069  .\n## 23 -0.141  - 21.402 0.557 -0.147  -\n## 24 -0.045  . 22.058 0.576 -0.043  .\n## 25  0.113  . 26.245 0.395  0.122  +\n## 26 -0.050  . 27.074 0.405 -0.063  .\n## 27  0.040  . 27.594 0.432  0.009  .\n## 28  0.001  . 27.595 0.486 -0.004  .\n## 29  0.037  . 28.055 0.515  0.048  .\n## 30 -0.027  . 28.301 0.555 -0.056  .\n## 31  0.043  . 28.920 0.573  0.009  .\n## 32 -0.024  . 29.108 0.614  0.017  .\n## 33 -0.064  . 30.502 0.592 -0.089  .\n## 34  0.067  . 32.027 0.565  0.044  .\n## 35  0.046  . 32.738 0.578  0.053  .\n## 36 -0.044  . 33.391 0.593 -0.022  .\n## 37 -0.002  . 33.392 0.639  0.012  .\n\n\n\n## \n##  Shapiro-Wilk normality test\n## \n## data:  x\n## W = 0.99469, p-value = 0.3861\n\n\n\n##  Portion_of_data F_statistic p.value\n##          0.33333      1.2065   0.352\n\n\n\n\nLa segunda simulación es:\n\n\nSummary statistics:\n==================\n                        Serie 1\nData points:          300.00000\nMissing:                0.00000\nMinimum:               93.91752\n1st quartile:          98.40353\nMean:                 100.10543\nP(Mean = 0):            0.00000\nMedian:               100.01546\n3rd quartile:         101.72986\nMaximum:              106.45870\nInterquartile range:    3.32633\nRange:                 12.54118\nSatandard deviation:    2.38931\nVariance:               5.70880\nSkewness:               0.06186\nKurtosis:              -0.40482\nAutocorrelation tests:\n=====================\n     SACF sa     LB p.val  SPACF sp\n1  -0.024  .  0.173 0.677 -0.024  .\n2   0.027  .  0.396 0.820  0.027  .\n3  -0.056  .  1.342 0.719 -0.054  .\n4  -0.033  .  1.669 0.796 -0.036  .\n5  -0.069  .  3.151 0.677 -0.069  .\n6   0.066  .  4.507 0.608  0.062  .\n7  -0.093  .  7.193 0.409 -0.092  .\n8  -0.019  .  7.310 0.504 -0.036  .\n9   0.021  .  7.445 0.591  0.026  .\n10 -0.034  .  7.803 0.648 -0.043  .\n11 -0.052  .  8.644 0.655 -0.057  .\n12 -0.009  .  8.669 0.731 -0.026  .\n13 -0.004  .  8.674 0.797  0.003  .\n14  0.104  . 12.071 0.601  0.095  .\n15 -0.009  . 12.098 0.672 -0.026  .\n16  0.069  . 13.597 0.629  0.066  .\n17 -0.020  . 13.722 0.687 -0.006  .\n18  0.003  . 13.725 0.747 -0.005  .\n19 -0.024  . 13.904 0.789 -0.008  .\n20  0.001  . 13.904 0.835 -0.011  .\n21 -0.013  . 13.962 0.871  0.013  .\n22 -0.053  . 14.869 0.868 -0.069  .\n23 -0.141  - 21.402 0.557 -0.147  -\n24 -0.045  . 22.058 0.576 -0.043  .\n25  0.113  . 26.245 0.395  0.122  +\nGaussianity tests:\n=================\n\n    Shapiro-Wilk normality test\n\ndata:  x\nW = 0.99469, p-value = 0.3861\n\nRatio of variance tests:\n=======================\n Portion_of_data F_statistic p.value\n         0.33333      1.2065   0.352\n\n\n\n\n\n\n\n\n# Cargando librerías\nlibrary(UComp)\nlibrary(ggplot2)\nset.seed(2)\n# Simulando números aleatorios\ny = rnorm(300) + 100\n# Gráfico\nautoplot(as.ts(y))\n# Estadísticos\nsumStats(y)\n# Identificación\nident(y)\ntsDisplay(y)\n# Tests de gaussianidad\ngaussTest(y)\n# Tests de heterocedasticidad\nvarTest(y)\n# Tests CUSUM\ncusum(y)\n# Segunda simulación\nset.seed(2)\ny = rnorm(300) * sqrt(5) + 100\ntests(y)\n\n\n\n\nfrom UComp import *\n# Simulando números aleatorios\ny = np.random.normal(100, 1, 300)\n# Gráfico\nplt.plot(y)\n# Estadísticos\nsumStats(y)\n# Identificación\nident(y)\ntsDisplay(y)\n# Tests de gaussianidad\ngaussTest(y)\n# Tests de heterocedasticidad\nvarTest(y)\n# Tests CUSUM\ncusum(y)\n# Segunda simulación\ny = np.random.normal(100, 5, 300)\ntests(y)\n\n\n\nEn MATLAB se recomienda usar la función toolTESTen el que se pueden ver todos los tests y muchos más, en realidad.\n\nclear all\n% Simulando números aleatorios\ny = randn(300, 1) + 100;\n% Tests\ntoolTEST(y)\n% Segunda simulación\ny = rnorm(300) * sqrt(5) + 100\ntoolTEST(y)"
  },
  {
    "objectID": "07-basicsts.html#métodos-y-modelos-de-predicción",
    "href": "07-basicsts.html#métodos-y-modelos-de-predicción",
    "title": "\n2  Conceptos y herramientas básicas\n",
    "section": "\n2.5 Métodos y modelos de predicción",
    "text": "2.5 Métodos y modelos de predicción\nCuando se analizan datos cuantitativos (también cualitativos, pero nos centraremos en los cuantitativos) lo usual es utilizar métodos de predicción o modelos estadísticos. Los métodos son reglas heurísticas de predicción, basándonos en propiedades de las series a analizar y el sentido común. Los modelos estadísticos, por el contrario, son formulaciones matemáticas del problema en las que existe (necesariamente para que sea estadístico) algún componente aleatorio.\nAlgunos métodos sencillos de predicción puntuales pueden ser los siguientes:\n\n\nMedia: la predicción será sencillamente la media de la serie. Es evidente que esta predicción será prudente cuando se observe un nivel medio constante en la serie temporal, pero en general será poco recomendable.\n\nNaive o ingenuo: consiste en predecir el futuro utilizando la última observación y prolongándola hacia adelante.\n\nNaive con estacionalidad: en aquellas series que tienen estacionalidad consistiría en proyectar hacia el futuro el último año completo observado.\n\nMedia anual (media del último año): predice el futuro con la media del último año.\n\nLa Figura 2.9 ilustra la predicción de estos métodos en una serie real. Es evidente que la media predice muy por debajo de lo que pueden ser las observaciones futuras reales. Lo mismo sucede con el naive. El método con estacionalidad parece mucho más apropiado, aunque, dada la inercia ascendente de la serie, con toda seguridad predice por debajo de los valores reales.\n\n\nFigura 2.9: Ejemplos de predicción con métodos sencillos.\n\n\n\n\n\n\n\nImportante\n\n\n\nEstos métodos son tan sencillos de aplicar que cualquier método o modelo más sofisticado debería ser capaz de mejorar sus predicciones por un amplio margen. Es una buena práctica utilizar estos modelos como opciones mínimas con qué comparar cualquier otro.\n\n\nAunque no todos los métodos de predicción que se puedan pensar son formalizables como modelos estadísticos, muchos, como los expuestos más arriba, sí se corresponden con modelos estadísticos que le dan un soporte más riguroso. La ventaja de utilizar modelos es que se puede comprobar empíricamente si una serie se corresponde con un modelo utilizado, y por tanto, si la predicción es óptima. Si el modelo al que corresponde el método utilizado no es adecuado, las predicciones podrán ser mejoradas cuando se utilice otro método/modelo alternativo. A continuación se citan dos ejemplos, que se completarán en el ejercicio propuesto al final del capítulo.\n\nEl método de la media será correcto cuando la serie temporal es ruido blanco más una constante, que será la media y habrá que estimar en cada caso:\n\nFormulación: \\(y_t=c+a_t\\).\nEstimación: el único parámetro a estimar sería la media (\\(c\\)), cuya estimación óptima es la media de la serie, es decir, \\(\\hat{c}=1/n\\sum_{t=1}^n{y_t}\\).\nDiagnóstico: la serie \\(\\hat{a}_t=y_t-\\hat{c}\\) debe ser ruido blanco con media cero y varianza constante.\nPredicción: la predicción óptima es la media estimada, \\(\\hat{y}_{T+l|T}=\\hat{c}\\).\n\n\nEl método de predicción naive se corresponde con un paseo aleatorio:\n\nFormulación: \\(y_t=y_{t-1}+a_t\\).\nEstimación: no hay ningún parámetro que estimar.\nDiagnóstico: si se estima la diferencia de la serie tenemos \\(\\hat{a}_t=y_t-y_{t-1}\\), que es ruido blanco. Es decir, la forma de comprobar que una serie sigue un paseo aleatorio es examinando que la primera diferencia es ruido blanco con media cero y varianza constante.\nPredicción: última observación disponible \\(\\hat{y}_{T+l|T}=y_T\\).\n\n\n\n\n\nResuelve\nSolución\nR\nPython\nMATLAB/Octave\n\n\n\nEscribe una función con las siguientes características:\n\nNombre: simpleMethods.\nEntradas: una serie temporal y el horizonte de predicción.\nSalidas: predicciones y residuos, que pueden ser salidas individuales o en una lista, dependiendo del lenguaje que uses. En predicciones se devuelven las predicciones del método de la media (ruido blanco) y naive (paseo aleatorio). En residuos se devuelven los residuos de cada modelo, respectivamente. Para que sea más estético puedes poner nombres a las columnas que ayuden a identificar los modelos.\n\nSimula un paseo aleatorio con una condición inicial de 100 y representa gráficamente la serie con las predicciones de los dos métodos, y en otro gráfico los residuos de los dos métodos. ¿Son adecuados los modelos?\n\n\nUna vez construida la función (ver las pestañas de cada lenguaje), la solución es\n\n\n\n\n\nLa adecuación de los modelos se puede ver con la función tests. Los primeros residuos indican que el primer modelo no es correcto y el segundo sí.\n\n\nSummary statistics:\n==================\n                        Serie 1\nData points:          150.00000\nMissing:                0.00000\nMinimum:              -19.60714\n1st quartile:          -7.75692\nMean:                   0.00000\nP(Mean = 0):            1.00000\nMedian:                 4.46768\n3rd quartile:           7.44709\nMaximum:               13.43764\nInterquartile range:   15.20401\nRange:                 33.04478\nSatandard deviation:    9.97561\nVariance:              99.51273\nSkewness:              -0.74110\nKurtosis:              -0.90106\nAutocorrelation tests:\n=====================\n    SACF sa       LB p.val  SPACF sp\n1  0.983  +  147.988     0  0.983  +\n2  0.966  +  291.729     0 -0.036  .\n3  0.947  +  430.847     0 -0.051  .\n4  0.927  +  564.999     0 -0.049  .\n5  0.906  +  694.006     0 -0.032  .\n6  0.886  +  818.292     0  0.032  .\n7  0.864  +  937.313     0 -0.076  .\n8  0.843  + 1051.505     0  0.030  .\n9  0.824  + 1161.254     0  0.026  .\n10 0.804  + 1266.589     0 -0.016  .\n11 0.784  + 1367.326     0 -0.041  .\n12 0.761  + 1463.106     0 -0.073  .\n13 0.739  + 1554.033     0  0.000  .\n14 0.715  + 1639.662     0 -0.078  .\n15 0.690  + 1720.003     0 -0.025  .\n16 0.666  + 1795.502     0  0.037  .\n17 0.644  + 1866.499     0  0.021  .\n18 0.621  + 1933.010     0 -0.023  .\n19 0.598  + 1995.191     0 -0.027  .\n20 0.574  + 2053.031     0 -0.033  .\n21 0.549  + 2106.243     0 -0.081  .\n22 0.524  + 2155.094     0 -0.001  .\n23 0.499  + 2199.863     0  0.012  .\n24 0.475  + 2240.713     0  0.001  .\n25 0.452  + 2277.982     0  0.028  .\nGaussianity tests:\n=================\n\n    Shapiro-Wilk normality test\n\ndata:  x\nW = 0.85966, p-value = 1.212e-10\n\nRatio of variance tests:\n=======================\n Portion_of_data F_statistic p.value\n         0.33333      4.9442       0\n\n\n\n\n\nSummary statistics:\n==================\n                        Serie 1\nData points:          150.00000\nMissing:                1.00000\nMinimum:               -2.72182\n1st quartile:          -0.44052\nMean:                   0.14530\nP(Mean = 0):            0.07289\nMedian:                 0.17498\n3rd quartile:           0.87184\nMaximum:                2.53311\nInterquartile range:    1.31236\nRange:                  5.25493\nSatandard deviation:    0.98189\nVariance:               0.96410\nSkewness:              -0.33320\nKurtosis:               0.05596\nAutocorrelation tests:\n=====================\n     SACF sa     LB p.val  SPACF sp\n1   0.042  .  0.272 0.602  0.042  .\n2   0.188  +  5.696 0.058  0.187  +\n3  -0.101  .  7.253 0.064 -0.119  .\n4   0.140  . 10.277 0.036  0.120  .\n5  -0.057  . 10.784 0.056 -0.033  .\n6   0.234  + 19.383 0.004  0.194  +\n7  -0.033  . 19.557 0.007 -0.023  .\n8  -0.025  . 19.656 0.012 -0.125  .\n9  -0.105  . 21.438 0.011 -0.036  .\n10  0.090  . 22.755 0.012  0.077  .\n11  0.081  . 23.813 0.014  0.120  .\n12 -0.097  . 25.357 0.013 -0.211  -\n13 -0.044  . 25.678 0.019 -0.030  .\n14 -0.063  . 26.342 0.023  0.026  .\n15 -0.115  . 28.563 0.018 -0.117  .\n16  0.023  . 28.649 0.026  0.047  .\n17  0.063  . 29.322 0.032  0.026  .\n18 -0.062  . 29.981 0.038 -0.025  .\n19  0.002  . 29.982 0.052  0.062  .\n20  0.099  . 31.695 0.047  0.122  .\n21  0.058  . 32.280 0.055  0.023  .\n22 -0.012  . 32.304 0.072 -0.073  .\n23  0.026  . 32.426 0.092  0.036  .\n24 -0.096  . 34.079 0.083 -0.121  .\n25 -0.013  . 34.109 0.106  0.013  .\nGaussianity tests:\n=================\n\n    Shapiro-Wilk normality test\n\ndata:  x\nW = 0.98775, p-value = 0.2149\n\nRatio of variance tests:\n=======================\n Portion_of_data F_statistic p.value\n         0.33333        0.59  0.0706\n\n\n\n\n\n\n\n\n# Función\nsimpleMethods = function(y, h=frequency(y)){\n  n = length(y)\n  # Inicializando salidas\n  predicciones = ts(matrix(NA, h, 2), frequency=frequency(y), start=plus_one(y))\n  residuos = ts(matrix(NA, n, 2), frequency=frequency(y), start=start(y))\n  colnames(predicciones) = c(\"media\", \"naive\")\n  colnames(residuos) = c(\"media\", \"naive\")\n  # Estimando predicciones\n  predicciones[, 1] = mean(y, na.rm = TRUE)\n  predicciones[, 2] = tail(y[!is.na(y)], 1)\n  # Estimando residuos\n  residuos[, 1] = y - predicciones[1, 1]\n  residuos[2 : n, 2] = y[2 : n] - y[1 : (n - 1)]\n  return(list(predicciones=predicciones, residuos=residuos))\n}\n# Simulación de paseo aleatorio\nn = 150\na = rnorm(n)\nx = ts(rep(100, n), frequency = 12, start=2000)\nfor (t in 2 : n){\n  x[t] = x[t - 1] + a[t]\n}\n# Llamada a la función simpleMethods\nsalida = simpleMethods(x)\n# Representando predicciones\np1 = autoplot(x) + autolayer(salida$predicciones)\n# Representando residuos\np2 = autoplot(salida$residuos)\ngrid.arrange(grobs = list(p1, p2),\n            layout_matrix = rbind(1, 2))\n# Comprobando si los modelos son adecuados\ntests(salida$residuos[, 1])\ntests(salida$residuos[, 2])\n\n\n\n\n# Función\ndef simpleMethods(y, h=4):\n    n = len(y)\n    # Inicializando salidas\n    predicciones = pd.DataFrame(np.full((h, 2), np.nan))\n    predicciones.index = range(len(y), len(y) + h)\n    residuos = pd.DataFrame(np.full((n, 2), np.nan))\n    predicciones.columns = [\"media\", \"naive\"]\n    residuos.columns = [\"media\", \"naive\"]\n    # Estimando predicciones\n    predicciones.iloc[:, 0] = np.mean(y)\n    predicciones.iloc[:, 1] = y[~np.isnan(y)].values[-1]\n    # Estimando residuos\n    residuos.iloc[:, 0] = y - predicciones.iloc[0, 0]\n    residuos.iloc[1 : n, 1] = y[1 : n].values - y[0 : (n - 1)].values\n    return {\"predicciones\": predicciones, \"residuos\": residuos}\n# Simulación de paseo aleatorio\nn = 150\na = np.random.normal(0, 1, n)\nx = pd.Series(np.full(n, 0.0))\nfor t in range(1, n):\n  x[t] = x[t - 1] + a[t]\n# Llamada a la función simpleMethods\nsalida = simpleMethods(x)\n# Representando predicciones\nplt.plot(x[-30:])\nplt.plot(salida[\"predicciones\"])\n# Representando residuos\nplt.plot(salida[\"residuos\"])\n# Comprobando si los modelos son adecuados\ntests(np.array(salida[\"residuos\"][\"media\"]))\ntests(np.array(salida[\"residuos\"][\"naive\"]))\n\n\n\n\n% Función\nfunction salida = simpleMethods(y, h)\n    if nargin &lt; 2\n        h = 4;\n    end\n    n = length(y);\n    % Inicializando salidas\n    predicciones = nan(h, 2);\n    residuos = nan(n, 2);\n    % Estimando predicciones\n    predicciones(:, 1) = mean(y);\n    predicciones(:, 2) = y(~isnan(y))(end);\n    % Estimando residuos\n    residuos(:, 1) = y - predicciones(end, 1);\n    residuos(2 : n, 2) = y(2 : n) - y(1 : (n - 1));\n    salida = struct('predicciones', predicciones, 'residuos', residuos);\nend\n% Simulación de paseo aleatorio\nn = 150;\na = randn(n, 1);\nx = zeros(n, 1);\nfor t = 2 : n\n    x(t) = x(t - 1) + a(t);\nend\n% Llamada a la función simpleMethods\nsalida = simpleMethods(x);\n% Representando predicciones\nt = (n - 50 : n)';\nplot(t, x(t), (n + 1 : n + 4), salida.predicciones)\n% Representando residuos y comprobando adecuación de modelos\ntoolTEST(salida.residuos)"
  },
  {
    "objectID": "07-basicsts.html#esquema-general-de-análisis-de-series-temporales",
    "href": "07-basicsts.html#esquema-general-de-análisis-de-series-temporales",
    "title": "\n2  Conceptos y herramientas básicas\n",
    "section": "\n2.6 Esquema general de análisis de series temporales",
    "text": "2.6 Esquema general de análisis de series temporales\nCon todo lo visto hasta el momento, lo único que resta es plantear el esquema general de análisis de series temporales, que se muestra en la Figura 2.10. Lo primero es que hay que tener claro el problema que se quiere resolver y disponer de datos cuantitativos.\n\n\nFigura 2.10: Esquema general de análisis de series temporales.\n\nEs conveniente siempre realizar un análisis exploratorio de los datos para tener una evidencia inicial sobre la calidad y la escala de los mismos, periodo de muestreo, la longitud en el tiempo, anomalías que se pueden detectar a simple vista, estacionariedad en media y varianza, tendencias, estacionalidades, nivel de ruido, etc. Es una fase de la que ya se puede tener una buena intuición de los resultados que se pueden esperar y los problemas con que el analizador se va a encontrar. Dentro de esta fase también se suele dividir la muestra en un training set y un test set. El training set es la parte de la muestra inicial con la que se realiza todo el análisis. El test set son los datos que se apartan de la muestra y sirven de evaluación final de nuestros modelos. Estos últimos no se utilizan en el proceso de modelización, sino solo para evaluación. Aunque se suele recomendar reservar un \\(25\\%\\) de la muestra como test set, la proporción puede variar en función de muchas circunstancias. Lo normal es que las predicciones en tiempo real se lleven a cabo con el mejor modelo re-estimado para toda la muestra.\nEl siguiente paso será explorar si son necesarias transformaciones que hagan los datos más conformables con los métodos estadísticos que se utilizarán más tarde. Como ya hemos visto, una de las cuestiones a comprobar es la estacionariedad en varianza mediante la transformación Box-Cox, que nunca está de más explorar. Dependiendo de los modelos que se vayan a utilizar, la estacionariedad en media mediante diferencias también será un tema a tratar. Los métodos que se utilizarán en este libro, sin embargo, no necesitan diferenciación.\nLa fase de identificación consiste en buscar el modelo concreto apropiado a cada serie temporal dentro de una familia que se quiere investigar. Por ejemplo, más tarde se introducirán los modelos de suavizado exponencial, pero hay muchos tipos dentro de esta familia, dependiendo de la versión concreta de tendencia y estacionalidad que se elijan. La identificación consiste en elegir una versión concreta de todos esos componentes que sea adecuada para la serie a tratar.\nLa estimación consiste en buscar los parámetros óptimos de los que depende el modelo identificado en la fase anterior por algún método estadístico, habitualmente Máxima Verosimilitud. Es importante escalar los datos para evitar magnitudes muy grandes o muy pequeñas que pueden llevar a errores numéricos en el proceso de optimización. El mismo modelo identificado puede generar comportamientos dinámicos muy diferentes en función de los valores de los parámetros que se asignen.\nLa fase de diagnóstico consiste en analizar si el modelo estimado es coherente con los datos que se están analizando. En particular, interesará ver que los parámetros estimados son significativos, que no se ha producido ninguna anomalía en la estimación como errores numéricos, etc. Si el modelo conlleva estimación de componentes también conviene asegurarse de que dichos componentes son lo que se espera de ellos. Finalmente, habrá que comprobar que los residuos son ruido blanco con la batería de contrastes propuestos en la sección anterior.\nCuando algún modelo pasa los tests de diagnóstico se pasa a utilizar el o los modelos en predicción y compararlos entre sí o con algún estándar establecido. Dependiendo del contexto, esta fase se complementa con la ayuda de expertos (judgemental forecasting) que corrigen las predicciones producidas por los modelos estadísticos basándose en su experiencia personal en el sector.\nLa prueba definitiva es probar los modelos en el test set o fuera de la muestra. Si los errores de predicción son aceptables se pasará a usar el modelo en modo producción. Pero si hay alguna deficiencia habrá que volver a alguna de las fases anteriores. Hay que tener en cuenta que los tipos de errores, así como el resultado de los tests de diagnóstico, nos pueden dar indicaciones de si tenemos que volver al principio del análisis o basta con hacer una identificación más fina o utilizar otro método de estimación.\n\n\n\n\n\n\nImportante\n\n\n\nTodo este proceso se puede realizar manualmente, aunque algunas fases se pueden automatizar. De hecho hay un interés creciente en la automatización como consecuencia de las ingentes cantidades de información que hoy día se quiere procesar en tiempo récord. Las fases automatizables hasta el momento son las de la transformación, identificación y estimación. Hay que avisar que con excesiva frecuencia se tratan los datos de modo completamente automático, sin ni siquiera realizar un mínimo de análisis exploratorio. El riesgo de obtener resultados absurdos o mediocres es muy alto en esos casos."
  }
]